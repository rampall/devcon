{
	"systeminfo": "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | ",
	"model": {
		"type": "base",
		"multilingual": false,
		"vocab": 51864,
		"audio": {
			"ctx": 1500,
			"state": 512,
			"head": 8,
			"layer": 6
		},
		"text": {
			"ctx": 448,
			"state": 512,
			"head": 8,
			"layer": 6
		},
		"mels": 80,
		"f16": 1
	},
	"params": {
		"model": "models/ggml-base.en.bin",
		"language": "en",
		"translate": false
	},
	"result": {
		"language": "en"
	},
	"transcription": [
		{
			"timestamps": {
				"from": "00:00:00,000",
				"to": "00:00:11,000"
			},
			"offsets": {
				"from": 0,
				"to": 11000
			},
			"text": " [Music]"
		},
		{
			"timestamps": {
				"from": "00:00:11,000",
				"to": "00:00:14,000"
			},
			"offsets": {
				"from": 11000,
				"to": 14000
			},
			"text": " We don't have any display."
		},
		{
			"timestamps": {
				"from": "00:00:14,000",
				"to": "00:00:18,000"
			},
			"offsets": {
				"from": 14000,
				"to": 18000
			},
			"text": " So, we have a lot of beautiful slides."
		},
		{
			"timestamps": {
				"from": "00:00:18,000",
				"to": "00:00:22,000"
			},
			"offsets": {
				"from": 18000,
				"to": 22000
			},
			"text": " I've got a lot of that started on the ten hour flight over here."
		},
		{
			"timestamps": {
				"from": "00:00:22,000",
				"to": "00:00:28,000"
			},
			"offsets": {
				"from": 22000,
				"to": 28000
			},
			"text": " And maybe I'll post them online or something like that, so it doesn't make any better waste."
		},
		{
			"timestamps": {
				"from": "00:00:28,000",
				"to": "00:00:39,000"
			},
			"offsets": {
				"from": 28000,
				"to": 39000
			},
			"text": " But after we'll do this more verbally and maybe I'll end up this whiteboard if I have the courage to draw and fly."
		},
		{
			"timestamps": {
				"from": "00:00:39,000",
				"to": "00:00:44,000"
			},
			"offsets": {
				"from": 39000,
				"to": 44000
			},
			"text": " But anyway, I'm Johnny and also I think, excuse me."
		},
		{
			"timestamps": {
				"from": "00:00:44,000",
				"to": "00:00:47,000"
			},
			"offsets": {
				"from": 44000,
				"to": 47000
			},
			"text": " Is Tech Guy coming? Yeah, fine."
		},
		{
			"timestamps": {
				"from": "00:00:47,000",
				"to": "00:00:51,000"
			},
			"offsets": {
				"from": 47000,
				"to": 51000
			},
			"text": " Okay, so we'll see what happens there."
		},
		{
			"timestamps": {
				"from": "00:00:51,000",
				"to": "00:00:58,000"
			},
			"offsets": {
				"from": 51000,
				"to": 58000
			},
			"text": " So, I'm Johnny, I work at Uport with these lovely people here."
		},
		{
			"timestamps": {
				"from": "00:00:58,000",
				"to": "00:01:19,000"
			},
			"offsets": {
				"from": 58000,
				"to": 79000
			},
			"text": " And today I wanted to talk about the US of privacy and informed consent and how exploiting users of rationality and things like that can cause a situation where we end up opting in to a surveillance state."
		},
		{
			"timestamps": {
				"from": "00:01:19,000",
				"to": "00:01:31,000"
			},
			"offsets": {
				"from": 79000,
				"to": 91000
			},
			"text": " I'll say we don't want, but we get voluntarily. Something different from a state-run surveillance apparatus like the Chinese social credit system."
		},
		{
			"timestamps": {
				"from": "00:01:31,000",
				"to": "00:01:44,000"
			},
			"offsets": {
				"from": 91000,
				"to": 104000
			},
			"text": " And why this is more of a threat for most of our, most countries, Europe and America, things that are run by, you know, I don't know if you've heard of this term, \"surveillance capitalism\""
		},
		{
			"timestamps": {
				"from": "00:01:44,000",
				"to": "00:02:01,000"
			},
			"offsets": {
				"from": 104000,
				"to": 121000
			},
			"text": " The idea that, you know, the underlying principles of the advertising model of the internet cause us to get this, you know, worst of all outcomes situation where everybody is surveilled and they do it voluntarily, even though we say we don't want it."
		},
		{
			"timestamps": {
				"from": "00:02:01,000",
				"to": "00:02:05,000"
			},
			"offsets": {
				"from": 121000,
				"to": 125000
			},
			"text": " I actually don't look at these slides just so I follow the plot."
		},
		{
			"timestamps": {
				"from": "00:02:05,000",
				"to": "00:02:27,000"
			},
			"offsets": {
				"from": 125000,
				"to": 147000
			},
			"text": " So I have a very clever intro to this where I showed you a fake application about a D5 credit score and you connect your Facebook to it and then you, then I go on to tell you how this is the exact flow in a different context that was exploited for Cambridge Analytica."
		},
		{
			"timestamps": {
				"from": "00:02:27,000",
				"to": "00:02:50,000"
			},
			"offsets": {
				"from": 147000,
				"to": 170000
			},
			"text": " And what they did was not a failure of finny privacy technology. It was a failure on the part of product designers to, or maybe not a failure, maybe they got exactly what they were trying to design for, which is trying to get users to give up more data than they were comfortable with, using kind of what we would call a UX dark patterns."
		},
		{
			"timestamps": {
				"from": "00:02:50,000",
				"to": "00:02:55,000"
			},
			"offsets": {
				"from": 170000,
				"to": 175000
			},
			"text": " So, let's see here."
		},
		{
			"timestamps": {
				"from": "00:02:55,000",
				"to": "00:03:10,000"
			},
			"offsets": {
				"from": 175000,
				"to": 190000
			},
			"text": " So yeah, and if you, you know, there's also a slide here around what would it have looked like to try and convince a user to not opt into this, to this surveillance."
		},
		{
			"timestamps": {
				"from": "00:03:10,000",
				"to": "00:03:38,000"
			},
			"offsets": {
				"from": 190000,
				"to": 218000
			},
			"text": " And in the Cambridge Analytica example, if you're not familiar with it, users were taking an online personality quiz and they, you know, you get that Facebook login screen where you say, connect with Facebook and then it says something like, Facebook will get your profile information, and it also said you, Facebook will get access to your friends profile information, but nobody, nobody reads this."
		},
		{
			"timestamps": {
				"from": "00:03:38,000",
				"to": "00:03:57,000"
			},
			"offsets": {
				"from": 218000,
				"to": 237000
			},
			"text": " It's kind of a fine print, and it has done nothing to stem the flow of data into Facebook's system, even though it's kind of more of a cover your ass type of UX, right? It's, it's implied consent rather than informed consent."
		},
		{
			"timestamps": {
				"from": "00:03:57,000",
				"to": "00:04:18,000"
			},
			"offsets": {
				"from": 237000,
				"to": 258000
			},
			"text": " So there's this, one of the interesting properties of decentralized identity and self sovereign identity that kind of raises the stakes in terms of product design around these consent and disclosure choices is the fact that we've inverted the model of our relationship with the control of our identity."
		},
		{
			"timestamps": {
				"from": "00:04:18,000",
				"to": "00:04:30,000"
			},
			"offsets": {
				"from": 258000,
				"to": 270000
			},
			"text": " So, you know, in the Facebook example, Google example, they own your identity and they take on a lot of responsibility for stewarding that identity and they don't always act in our best interest."
		},
		{
			"timestamps": {
				"from": "00:04:30,000",
				"to": "00:04:43,000"
			},
			"offsets": {
				"from": 270000,
				"to": 283000
			},
			"text": " However, when we move to decentralized identity, we invert this model and your identity really in a paradoxical oxymoron kind of way, your identity becomes centralized on you, right?"
		},
		{
			"timestamps": {
				"from": "00:04:43,000",
				"to": "00:04:56,000"
			},
			"offsets": {
				"from": 283000,
				"to": 296000
			},
			"text": " You are in control of all of the data. You are a single point of failure in IT security circles. There's a common phrase that the most vulnerable part of any security system is the human, right?"
		},
		{
			"timestamps": {
				"from": "00:04:56,000",
				"to": "00:05:12,000"
			},
			"offsets": {
				"from": 296000,
				"to": 312000
			},
			"text": " They're able to be exploited in various ways, and this, this becomes an even bigger risk once we centralized that point of failure on the human by introducing decentralized identity."
		},
		{
			"timestamps": {
				"from": "00:05:12,000",
				"to": "00:05:25,000"
			},
			"offsets": {
				"from": 312000,
				"to": 325000
			},
			"text": " So, this puts product designers in a precarious position. Well, I guess more of a, or in a heightened position or more important position in these, in these flows."
		},
		{
			"timestamps": {
				"from": "00:05:25,000",
				"to": "00:05:37,000"
			},
			"offsets": {
				"from": 325000,
				"to": 337000
			},
			"text": " So, a lot of damage now can be done by people to disclose things, especially if you convince a user to disclose something and they don't realize that it's going on chain, what that even means."
		},
		{
			"timestamps": {
				"from": "00:05:37,000",
				"to": "00:06:02,000"
			},
			"offsets": {
				"from": 337000,
				"to": 362000
			},
			"text": " So, we really have to avoid that. So, there is this common term that we probably all heard about, and it is called the privacy paradox, and it's that users often say they want privacy, they're really pissed when you violate their privacy"
		},
		{
			"timestamps": {
				"from": "00:06:02,000",
				"to": "00:06:16,000"
			},
			"offsets": {
				"from": 362000,
				"to": 376000
			},
			"text": " and, you know, an Equifax or, you know, Cambridge Analytica or any of these other high profile situations, and then you watch their behavior and then they go on and they keep doing the exact same thing over and over again, which is, you know, giving up a lot of their data."
		},
		{
			"timestamps": {
				"from": "00:06:16,000",
				"to": "00:06:38,000"
			},
			"offsets": {
				"from": 376000,
				"to": 398000
			},
			"text": " There's a couple psychological things that cause this to happen. One has been pretty studied extensively in behavioral economics, it's the risk perception gap, and it's this idea that users are really bad, or not users humans are really bad, you know, anticipating risk and measuring risk."
		},
		{
			"timestamps": {
				"from": "00:06:38,000",
				"to": "00:06:57,000"
			},
			"offsets": {
				"from": 398000,
				"to": 417000
			},
			"text": " And one thing users do is they say, \"Oh, it'll never happen to me,\" or even if there is a huge data leak, the idea that the chance that I am targeted isn't going to be, it is pretty low, I'm just one small drop in a sea of data."
		},
		{
			"timestamps": {
				"from": "00:06:57,000",
				"to": "00:07:18,000"
			},
			"offsets": {
				"from": 417000,
				"to": 438000
			},
			"text": " And it's also, there's these compound-decing effects that happen when you do disclosures over and over again, and lots of studies have shown that you can denomize data with as few as like four or five data points in a whole sea of anonymized data, correlate those things, and it's trivially easy to denomize these types of things."
		},
		{
			"timestamps": {
				"from": "00:07:18,000",
				"to": "00:07:30,000"
			},
			"offsets": {
				"from": 438000,
				"to": 450000
			},
			"text": " And it's really hard to centralize that for most people, so that's something we have to be aware of. And I'm sorry, you don't get to see all my fancy animations, Keto."
		},
		{
			"timestamps": {
				"from": "00:07:30,000",
				"to": "00:07:45,000"
			},
			"offsets": {
				"from": 450000,
				"to": 465000
			},
			"text": " So the other thing we hear a lot from our research at Uboard, and anybody that has done anything really hit a privacy and talk to people about it, is another thing which is, \"I've got nothing to hide.\" Right, so lots of people say this."
		},
		{
			"timestamps": {
				"from": "00:07:45,000",
				"to": "00:08:04,000"
			},
			"offsets": {
				"from": 465000,
				"to": 484000
			},
			"text": " I don't know if this has been out of word. Okay, so there's this idea of \"I've got nothing to hide\" that people say this all the time, and that comes from an improper framing and an improper conceptualization of what privacy is."
		},
		{
			"timestamps": {
				"from": "00:08:04,000",
				"to": "00:08:20,000"
			},
			"offsets": {
				"from": 484000,
				"to": 500000
			},
			"text": " Privacy is often looked at as hiding bad things, and it's also looked at at an individual level. But I think one of the things we have to start thinking about, oh, and it also comes from people thinking of privacy as this binary, and the media as this binary."
		},
		{
			"timestamps": {
				"from": "00:08:20,000",
				"to": "00:08:43,000"
			},
			"offsets": {
				"from": 500000,
				"to": 523000
			},
			"text": " But something we've learned is that privacy and privacy and everybody are these spectral things, right? And movement along these spectrums, which you have a whole team of people."
		},
		{
			"timestamps": {
				"from": "00:08:43,000",
				"to": "00:08:58,000"
			},
			"offsets": {
				"from": 523000,
				"to": 538000
			},
			"text": " Privacy and anonymity are these spectral things, and really what privacy is about is about some transparency, control, and the ability to move along this spectrum in giving context with as little friction as possible."
		},
		{
			"timestamps": {
				"from": "00:08:58,000",
				"to": "00:09:05,000"
			},
			"offsets": {
				"from": 538000,
				"to": 545000
			},
			"text": " And that's the type of thing we want to enable. So people don't feel like they're either anonymous, or they're either private or not. Right?"
		},
		{
			"timestamps": {
				"from": "00:09:05,000",
				"to": "00:09:15,000"
			},
			"offsets": {
				"from": 545000,
				"to": 555000
			},
			"text": " And we want to contextualize privacy as much as possible. So, I don't, okay, I need to get to this slide."
		},
		{
			"timestamps": {
				"from": "00:09:15,000",
				"to": "00:09:18,000"
			},
			"offsets": {
				"from": 555000,
				"to": 558000
			},
			"text": " Let's see here."
		},
		{
			"timestamps": {
				"from": "00:09:18,000",
				"to": "00:09:22,000"
			},
			"offsets": {
				"from": 558000,
				"to": 562000
			},
			"text": " [applause]"
		},
		{
			"timestamps": {
				"from": "00:09:22,000",
				"to": "00:09:36,000"
			},
			"offsets": {
				"from": 562000,
				"to": 576000
			},
			"text": " [indistinct chatter]"
		},
		{
			"timestamps": {
				"from": "00:09:36,000",
				"to": "00:09:39,000"
			},
			"offsets": {
				"from": 576000,
				"to": 579000
			},
			"text": " Okay, you guys want to see me? I'll zoom through from the top."
		},
		{
			"timestamps": {
				"from": "00:09:39,000",
				"to": "00:09:42,000"
			},
			"offsets": {
				"from": 579000,
				"to": 582000
			},
			"text": " [applause]"
		},
		{
			"timestamps": {
				"from": "00:09:42,000",
				"to": "00:09:48,000"
			},
			"offsets": {
				"from": 582000,
				"to": 588000
			},
			"text": " This is one of the more unique presentations."
		},
		{
			"timestamps": {
				"from": "00:09:48,000",
				"to": "00:10:02,000"
			},
			"offsets": {
				"from": 588000,
				"to": 602000
			},
			"text": " So, anyway, this is what I was going to show you up top. An application, DeFi credit, we've all seen this login flow, login with Facebook, get a credit score that we can use for DeFi lending."
		},
		{
			"timestamps": {
				"from": "00:10:02,000",
				"to": "00:10:07,000"
			},
			"offsets": {
				"from": 602000,
				"to": 607000
			},
			"text": " And then I revealed that this is a joke, and you all go, \"Oh, wow, that was fun.\""
		},
		{
			"timestamps": {
				"from": "00:10:07,000",
				"to": "00:10:08,000"
			},
			"offsets": {
				"from": 607000,
				"to": 608000
			},
			"text": " [laughter]"
		},
		{
			"timestamps": {
				"from": "00:10:08,000",
				"to": "00:10:17,000"
			},
			"offsets": {
				"from": 608000,
				"to": 617000
			},
			"text": " Okay, your channel, Erika, and blah, blah, is this what we would have to do to get a user to stop from sharing? I don't know."
		},
		{
			"timestamps": {
				"from": "00:10:17,000",
				"to": "00:10:19,000"
			},
			"offsets": {
				"from": 617000,
				"to": 619000
			},
			"text": " [laughter]"
		},
		{
			"timestamps": {
				"from": "00:10:19,000",
				"to": "00:10:23,000"
			},
			"offsets": {
				"from": 619000,
				"to": 623000
			},
			"text": " Dark pout scenes with Aylids, inverted model."
		},
		{
			"timestamps": {
				"from": "00:10:23,000",
				"to": "00:10:28,000"
			},
			"offsets": {
				"from": 623000,
				"to": 628000
			},
			"text": " [laughter]"
		},
		{
			"timestamps": {
				"from": "00:10:28,000",
				"to": "00:10:32,000"
			},
			"offsets": {
				"from": 628000,
				"to": 632000
			},
			"text": " This is a fancy word, and I skipped over, because I didn't want to explain it."
		},
		{
			"timestamps": {
				"from": "00:10:32,000",
				"to": "00:10:34,000"
			},
			"offsets": {
				"from": 632000,
				"to": 634000
			},
			"text": " Cool, no?"
		},
		{
			"timestamps": {
				"from": "00:10:34,000",
				"to": "00:10:39,000"
			},
			"offsets": {
				"from": 634000,
				"to": 639000
			},
			"text": " [laughter]"
		},
		{
			"timestamps": {
				"from": "00:10:39,000",
				"to": "00:10:41,000"
			},
			"offsets": {
				"from": 639000,
				"to": 641000
			},
			"text": " Okay, now we're back."
		},
		{
			"timestamps": {
				"from": "00:10:41,000",
				"to": "00:10:42,000"
			},
			"offsets": {
				"from": 641000,
				"to": 642000
			},
			"text": " [laughter]"
		},
		{
			"timestamps": {
				"from": "00:10:42,000",
				"to": "00:10:48,000"
			},
			"offsets": {
				"from": 642000,
				"to": 648000
			},
			"text": " [applause]"
		},
		{
			"timestamps": {
				"from": "00:10:48,000",
				"to": "00:10:58,000"
			},
			"offsets": {
				"from": 648000,
				"to": 658000
			},
			"text": " So, this is a quote from our research most recently that me and Aylid, the other designer on the team, did."
		},
		{
			"timestamps": {
				"from": "00:10:58,000",
				"to": "00:11:08,000"
			},
			"offsets": {
				"from": 658000,
				"to": 668000
			},
			"text": " And we heard from one of the users that, you know, if Facebook has my data, I'm okay with it, but if Facebook is selling it to someone else or some other third party, right?"
		},
		{
			"timestamps": {
				"from": "00:11:08,000",
				"to": "00:11:11,000"
			},
			"offsets": {
				"from": 668000,
				"to": 671000
			},
			"text": " I'm uncomfortable because I don't have control of it."
		},
		{
			"timestamps": {
				"from": "00:11:11,000",
				"to": "00:11:24,000"
			},
			"offsets": {
				"from": 671000,
				"to": 684000
			},
			"text": " And this gets to this idea of this sense of control that we have to, you know, at the UX level, try and give users, and that's kind of what really throws them off."
		},
		{
			"timestamps": {
				"from": "00:11:24,000",
				"to": "00:11:29,000"
			},
			"offsets": {
				"from": 684000,
				"to": 689000
			},
			"text": " And this is not necessarily some objective level of privacy exposure or risk."
		},
		{
			"timestamps": {
				"from": "00:11:29,000",
				"to": "00:11:35,000"
			},
			"offsets": {
				"from": 689000,
				"to": 695000
			},
			"text": " It's about feeling helpless in terms of managing it and controlling it."
		},
		{
			"timestamps": {
				"from": "00:11:35,000",
				"to": "00:11:41,000"
			},
			"offsets": {
				"from": 695000,
				"to": 701000
			},
			"text": " And that brings me to, there's this idea of an UX called dark patterns."
		},
		{
			"timestamps": {
				"from": "00:11:41,000",
				"to": "00:11:52,000"
			},
			"offsets": {
				"from": 701000,
				"to": 712000
			},
			"text": " And these are UX patterns that designers use, you know, in an unethical way to try and get users to do things that aren't in their best interest."
		},
		{
			"timestamps": {
				"from": "00:11:52,000",
				"to": "00:12:07,000"
			},
			"offsets": {
				"from": 712000,
				"to": 727000
			},
			"text": " One of the ones that's is relevant to this whole topic is something called privacy suckering, which is kind of what I showed you at the beginning, which is a flow where a user doesn't realize how much privacy they're giving up, how much data they're giving up."
		},
		{
			"timestamps": {
				"from": "00:12:07,000",
				"to": "00:12:12,000"
			},
			"offsets": {
				"from": 727000,
				"to": 732000
			},
			"text": " And this is done, you know, not in their best interest."
		},
		{
			"timestamps": {
				"from": "00:12:12,000",
				"to": "00:12:21,000"
			},
			"offsets": {
				"from": 732000,
				"to": 741000
			},
			"text": " And it's because of these dark patterns, because you have this decision of how to design these flows, that design is an inherently moral act."
		},
		{
			"timestamps": {
				"from": "00:12:21,000",
				"to": "00:12:27,000"
			},
			"offsets": {
				"from": 741000,
				"to": 747000
			},
			"text": " So there's no offloading the responsibility in just doing your job or whatever."
		},
		{
			"timestamps": {
				"from": "00:12:27,000",
				"to": "00:12:35,000"
			},
			"offsets": {
				"from": 747000,
				"to": 755000
			},
			"text": " So this has been incumbent on designers at the forefront and the front lines of this battle for privacy and against surveillance."
		},
		{
			"timestamps": {
				"from": "00:12:35,000",
				"to": "00:12:39,000"
			},
			"offsets": {
				"from": 755000,
				"to": 759000
			},
			"text": " For us to have some ethics about how we design these things."
		},
		{
			"timestamps": {
				"from": "00:12:39,000",
				"to": "00:12:49,000"
			},
			"offsets": {
				"from": 759000,
				"to": 769000
			},
			"text": " And to think carefully, not about just using the norms of surveillance capitalism, not just relying on terms of service and privacy policies that nobody reads,"
		},
		{
			"timestamps": {
				"from": "00:12:49,000",
				"to": "00:12:56,000"
			},
			"offsets": {
				"from": 769000,
				"to": 776000
			},
			"text": " very obfuscated disclosure requests like the Facebook example, and we have to come up with a better model."
		},
		{
			"timestamps": {
				"from": "00:12:56,000",
				"to": "00:13:00,000"
			},
			"offsets": {
				"from": 776000,
				"to": 780000
			},
			"text": " So, again, this is about moving through that spectrum."
		},
		{
			"timestamps": {
				"from": "00:13:00,000",
				"to": "00:13:03,000"
			},
			"offsets": {
				"from": 780000,
				"to": 783000
			},
			"text": " It's one way we need to think about privacy."
		},
		{
			"timestamps": {
				"from": "00:13:03,000",
				"to": "00:13:13,000"
			},
			"offsets": {
				"from": 783000,
				"to": 793000
			},
			"text": " The other is about something called contextual integrity, which is some of the Allison research by an academic named Helen Nissenbaum."
		},
		{
			"timestamps": {
				"from": "00:13:13,000",
				"to": "00:13:22,000"
			},
			"offsets": {
				"from": 793000,
				"to": 802000
			},
			"text": " And what she frames contextual integrity is, for privacy's sake, is these five principles."
		},
		{
			"timestamps": {
				"from": "00:13:22,000",
				"to": "00:13:29,000"
			},
			"offsets": {
				"from": 802000,
				"to": 809000
			},
			"text": " You have a data subject, you have a sender of data, you have the recipient of data information type and transmission principle."
		},
		{
			"timestamps": {
				"from": "00:13:29,000",
				"to": "00:13:34,000"
			},
			"offsets": {
				"from": 809000,
				"to": 814000
			},
			"text": " And it's when one of these five things changes, that's when a violation occurs."
		},
		{
			"timestamps": {
				"from": "00:13:34,000",
				"to": "00:13:36,000"
			},
			"offsets": {
				"from": 814000,
				"to": 816000
			},
			"text": " So it's about flows of information."
		},
		{
			"timestamps": {
				"from": "00:13:36,000",
				"to": "00:13:41,000"
			},
			"offsets": {
				"from": 816000,
				"to": 821000
			},
			"text": " And that is kind of what our user was getting to when they say, \"I'm okay with Facebook having my data.\""
		},
		{
			"timestamps": {
				"from": "00:13:41,000",
				"to": "00:13:45,000"
			},
			"offsets": {
				"from": 821000,
				"to": 825000
			},
			"text": " Even though we might argue and try and say, \"Even that's bad.\""
		},
		{
			"timestamps": {
				"from": "00:13:45,000",
				"to": "00:13:52,000"
			},
			"offsets": {
				"from": 825000,
				"to": 832000
			},
			"text": " But they're concerned more about when one of these things changes, but it starts going to some party that they didn't know about."
		},
		{
			"timestamps": {
				"from": "00:13:52,000",
				"to": "00:14:08,000"
			},
			"offsets": {
				"from": 832000,
				"to": 848000
			},
			"text": " So there's also some interesting research around kind of the difference between hypothetical and actual disclosures when you ask users about privacy and about their information."
		},
		{
			"timestamps": {
				"from": "00:14:08,000",
				"to": "00:14:15,000"
			},
			"offsets": {
				"from": 848000,
				"to": 855000
			},
			"text": " And the difference is in framing privacy as relative or objective."
		},
		{
			"timestamps": {
				"from": "00:14:15,000",
				"to": "00:14:31,000"
			},
			"offsets": {
				"from": 855000,
				"to": 871000
			},
			"text": " So the research showed that, and the privacy paradox is this right here, this conflict, which is in hypothetical situations you ask users, you tell them how much what their privacy risk is for a given disclosure."
		},
		{
			"timestamps": {
				"from": "00:14:31,000",
				"to": "00:14:34,000"
			},
			"offsets": {
				"from": 871000,
				"to": 874000
			},
			"text": " And they say, \"I'm not going to do it. I would never give up all that data.\""
		},
		{
			"timestamps": {
				"from": "00:14:34,000",
				"to": "00:14:40,000"
			},
			"offsets": {
				"from": 874000,
				"to": 880000
			},
			"text": " And then you have them, you know, they say, \"Give up all this data and we'll let you do this personality.\""
		},
		{
			"timestamps": {
				"from": "00:14:40,000",
				"to": "00:14:42,000"
			},
			"offsets": {
				"from": 880000,
				"to": 882000
			},
			"text": " And then they do."
		},
		{
			"timestamps": {
				"from": "00:14:42,000",
				"to": "00:14:58,000"
			},
			"offsets": {
				"from": 882000,
				"to": 898000
			},
			"text": " But what we could do is we can frame, or what the research shows is that users behave in a more privacy preserving way when the risks associated with their privacy are framed relatively."
		},
		{
			"timestamps": {
				"from": "00:14:58,000",
				"to": "00:15:06,000"
			},
			"offsets": {
				"from": 898000,
				"to": 906000
			},
			"text": " And that means that when you frame the risk of a disclosure, what you're getting up relative to what the norms are."
		},
		{
			"timestamps": {
				"from": "00:15:06,000",
				"to": "00:15:21,000"
			},
			"offsets": {
				"from": 906000,
				"to": 921000
			},
			"text": " So if you say, \"If you tell a user doing this, instead of telling them doing this, it has this objective risk, like you have, I don't know, some 85% chance of getting hacked or an information-cating hacked.\""
		},
		{
			"timestamps": {
				"from": "00:15:21,000",
				"to": "00:15:30,000"
			},
			"offsets": {
				"from": 921000,
				"to": 930000
			},
			"text": " You say, \"This thing you're about to do is going to make you more or less private based on your previous behavior or more or less private relative to your peers.\""
		},
		{
			"timestamps": {
				"from": "00:15:30,000",
				"to": "00:15:35,000"
			},
			"offsets": {
				"from": 930000,
				"to": 935000
			},
			"text": " Then users typically will engage in more privacy preserving behavior."
		},
		{
			"timestamps": {
				"from": "00:15:35,000",
				"to": "00:15:39,000"
			},
			"offsets": {
				"from": 935000,
				"to": 939000
			},
			"text": " So that's one thing that we can implement at the UX level."
		},
		{
			"timestamps": {
				"from": "00:15:43,000",
				"to": "00:15:52,000"
			},
			"offsets": {
				"from": 943000,
				"to": 952000
			},
			"text": " So there's this model of consent that we have now."
		},
		{
			"timestamps": {
				"from": "00:15:52,000",
				"to": "00:15:55,000"
			},
			"offsets": {
				"from": 952000,
				"to": 955000
			},
			"text": " And a lot of it is implied consent."
		},
		{
			"timestamps": {
				"from": "00:15:55,000",
				"to": "00:16:07,000"
			},
			"offsets": {
				"from": 955000,
				"to": 967000
			},
			"text": " This is the terms of service and all of that, where nobody actually really consents to any of it because nobody reads it, but it's implied that you have consent into whatever they want to do with your data because it's buried in the terms of service."
		},
		{
			"timestamps": {
				"from": "00:16:07,000",
				"to": "00:16:16,000"
			},
			"offsets": {
				"from": 967000,
				"to": 976000
			},
			"text": " And the privacy policy. We've talked kind of like about moving to informed consent, but informing people is hard."
		},
		{
			"timestamps": {
				"from": "00:16:16,000",
				"to": "00:16:22,000"
			},
			"offsets": {
				"from": 976000,
				"to": 982000
			},
			"text": " It's hard to get them, give them all the information in the world, and they won't read it, they'll ignore it."
		},
		{
			"timestamps": {
				"from": "00:16:22,000",
				"to": "00:16:26,000"
			},
			"offsets": {
				"from": 982000,
				"to": 986000
			},
			"text": " You can put the fine print to log in with Facebook button, and it's not going to help."
		},
		{
			"timestamps": {
				"from": "00:16:26,000",
				"to": "00:16:35,000"
			},
			"offsets": {
				"from": 986000,
				"to": 995000
			},
			"text": " So at Uport, kind of operating on a more progressive consent model."
		},
		{
			"timestamps": {
				"from": "00:16:35,000",
				"to": "00:16:42,000"
			},
			"offsets": {
				"from": 995000,
				"to": 1002000
			},
			"text": " I guess this is like a twist on progressive disclosure, which is a common UX pattern, which is just in time notices."
		},
		{
			"timestamps": {
				"from": "00:16:42,000",
				"to": "00:16:52,000"
			},
			"offsets": {
				"from": 1002000,
				"to": 1012000
			},
			"text": " You want to contextualize all of your disclosures rather than giving them a wholesale disclosure around what, around your data."
		},
		{
			"timestamps": {
				"from": "00:16:52,000",
				"to": "00:16:58,000"
			},
			"offsets": {
				"from": 1012000,
				"to": 1018000
			},
			"text": " You want to do it bite-sized chunks and in the moment, what it makes sense."
		},
		{
			"timestamps": {
				"from": "00:16:58,000",
				"to": "00:17:03,000"
			},
			"offsets": {
				"from": 1018000,
				"to": 1023000
			},
			"text": " So I'll show you a couple of things we're thinking about at Uport at the interface level."
		},
		{
			"timestamps": {
				"from": "00:17:03,000",
				"to": "00:17:21,000"
			},
			"offsets": {
				"from": 1023000,
				"to": 1041000
			},
			"text": " Just if you've used Uport before, you know that one of the key interactions in the Uport app is the selective disclosure, and this is getting users to affirmatively disclose things just in time in a progressive way."
		},
		{
			"timestamps": {
				"from": "00:17:21,000",
				"to": "00:17:24,000"
			},
			"offsets": {
				"from": 1041000,
				"to": 1044000
			},
			"text": " It by app and interaction, by interaction."
		},
		{
			"timestamps": {
				"from": "00:17:24,000",
				"to": "00:17:35,000"
			},
			"offsets": {
				"from": 1044000,
				"to": 1055000
			},
			"text": " De-faulting information not to be shared, having users opt in actively in the moment, giving users a sense of control over what they're doing."
		},
		{
			"timestamps": {
				"from": "00:17:35,000",
				"to": "00:17:37,000"
			},
			"offsets": {
				"from": 1055000,
				"to": 1057000
			},
			"text": " It addresses some of the things."
		},
		{
			"timestamps": {
				"from": "00:17:37,000",
				"to": "00:17:46,000"
			},
			"offsets": {
				"from": 1057000,
				"to": 1066000
			},
			"text": " So you're able to, this is a, you know, maybe you come to DEFCON and you would be able to share your name and your ticket, right?"
		},
		{
			"timestamps": {
				"from": "00:17:46,000",
				"to": "00:17:47,000"
			},
			"offsets": {
				"from": 1066000,
				"to": 1067000
			},
			"text": " This is something that you could do."
		},
		{
			"timestamps": {
				"from": "00:17:47,000",
				"to": "00:17:51,000"
			},
			"offsets": {
				"from": 1067000,
				"to": 1071000
			},
			"text": " You did something similar in East Denver this past year in February."
		},
		{
			"timestamps": {
				"from": "00:17:51,000",
				"to": "00:18:13,000"
			},
			"offsets": {
				"from": 1071000,
				"to": 1093000
			},
			"text": " This is an idea of something we're working on and thinking about around, after you share things, in the moment, allowing you to layer preferences and defaults and build a robust and complex set of privacy preferences over time, rather than having a user try and set this all in one go, right up front."
		},
		{
			"timestamps": {
				"from": "00:18:13,000",
				"to": "00:18:32,000"
			},
			"offsets": {
				"from": 1093000,
				"to": 1112000
			},
			"text": " That gives users a sense that they're usually on their way to try and do something else, and this type of thing, putting all of that up front, most apps like Facebook and Google, whatnot, they know that users are just going to opt into everything, because they're trying to get something else done."
		},
		{
			"timestamps": {
				"from": "00:18:32,000",
				"to": "00:18:42,000"
			},
			"offsets": {
				"from": 1112000,
				"to": 1122000
			},
			"text": " So we need to have users set up these privacy preserving preferences in smaller chunks where they're easier to digest."
		},
		{
			"timestamps": {
				"from": "00:18:42,000",
				"to": "00:18:47,000"
			},
			"offsets": {
				"from": 1122000,
				"to": 1127000
			},
			"text": " This is kind of how I'm thinking about displaying data inside the app."
		},
		{
			"timestamps": {
				"from": "00:18:47,000",
				"to": "00:18:58,000"
			},
			"offsets": {
				"from": 1127000,
				"to": 1138000
			},
			"text": " So one thing you'd be able to do is see who you've shared and you get a piece of data with, building to revoke data under GDPR log."
		},
		{
			"timestamps": {
				"from": "00:18:58,000",
				"to": "00:19:09,000"
			},
			"offsets": {
				"from": 1138000,
				"to": 1149000
			},
			"text": " You can automate these types of requests to get help go to details, but, you know, using signed messages, we can log the requests."
		},
		{
			"timestamps": {
				"from": "00:19:09,000",
				"to": "00:19:17,000"
			},
			"offsets": {
				"from": 1149000,
				"to": 1157000
			},
			"text": " If you have a data request and then whoever you made the request to, needs to respond and fulfill it, our time's up."
		},
		{
			"timestamps": {
				"from": "00:19:17,000",
				"to": "00:19:20,000"
			},
			"offsets": {
				"from": 1157000,
				"to": 1160000
			},
			"text": " Okay, so I'll go through the rest of this really quickly."
		},
		{
			"timestamps": {
				"from": "00:19:20,000",
				"to": "00:19:26,000"
			},
			"offsets": {
				"from": 1160000,
				"to": 1166000
			},
			"text": " So, give users control, allow you to set some of these preferences."
		},
		{
			"timestamps": {
				"from": "00:19:26,000",
				"to": "00:19:38,000"
			},
			"offsets": {
				"from": 1166000,
				"to": 1178000
			},
			"text": " So to recap, aggressive consent, give users a sense of control, frame privacy decisions relatively, privacy preserving defaults, allow, oh, I missed it, they'll be there."
		},
		{
			"timestamps": {
				"from": "00:19:38,000",
				"to": "00:19:42,000"
			},
			"offsets": {
				"from": 1178000,
				"to": 1182000
			},
			"text": " Allow users to build preferences over time."
		},
		{
			"timestamps": {
				"from": "00:19:42,000",
				"to": "00:19:47,000"
			},
			"offsets": {
				"from": 1182000,
				"to": 1187000
			},
			"text": " Finally, we just launched a new demo today, so you can go try out some of this stuff."
		},
		{
			"timestamps": {
				"from": "00:19:47,000",
				"to": "00:19:49,000"
			},
			"offsets": {
				"from": 1187000,
				"to": 1189000
			},
			"text": " Not all of those screens are live in the app right now."
		},
		{
			"timestamps": {
				"from": "00:19:49,000",
				"to": "00:19:54,000"
			},
			"offsets": {
				"from": 1189000,
				"to": 1194000
			},
			"text": " They're still part of our ongoing research, but this is live right now."
		},
		{
			"timestamps": {
				"from": "00:19:54,000",
				"to": "00:20:05,000"
			},
			"offsets": {
				"from": 1194000,
				"to": 1205000
			},
			"text": " If you go to ecosystems.uport.me, you can try that out and see how you work and enable a lot of these new sharing use cases and new applications."
		},
		{
			"timestamps": {
				"from": "00:20:05,000",
				"to": "00:20:12,000"
			},
			"offsets": {
				"from": 1205000,
				"to": 1212000
			},
			"text": " And we'll be with our shirts and our swag here in a bit out of DevCon Park."
		},
		{
			"timestamps": {
				"from": "00:20:12,000",
				"to": "00:20:14,000"
			},
			"offsets": {
				"from": 1212000,
				"to": 1214000
			},
			"text": " So, thank you all for that."
		},
		{
			"timestamps": {
				"from": "00:20:14,000",
				"to": "00:20:17,000"
			},
			"offsets": {
				"from": 1214000,
				"to": 1217000
			},
			"text": " [Applause]"
		},
		{
			"timestamps": {
				"from": "00:20:17,000",
				"to": "00:20:32,000"
			},
			"offsets": {
				"from": 1217000,
				"to": 1232000
			},
			"text": " [Music]"
		}
	]
}
