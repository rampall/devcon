{
	"systeminfo": "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | ",
	"model": {
		"type": "base",
		"multilingual": false,
		"vocab": 51864,
		"audio": {
			"ctx": 1500,
			"state": 512,
			"head": 8,
			"layer": 6
		},
		"text": {
			"ctx": 448,
			"state": 512,
			"head": 8,
			"layer": 6
		},
		"mels": 80,
		"f16": 1
	},
	"params": {
		"model": "models/ggml-base.en.bin",
		"language": "en",
		"translate": false
	},
	"result": {
		"language": "en"
	},
	"transcription": [
		{
			"timestamps": {
				"from": "00:00:00,000",
				"to": "00:00:13,000"
			},
			"offsets": {
				"from": 0,
				"to": 13000
			},
			"text": " [Music]"
		},
		{
			"timestamps": {
				"from": "00:00:13,000",
				"to": "00:00:18,300"
			},
			"offsets": {
				"from": 13000,
				"to": 18300
			},
			"text": " So it's the latest Ethereum-sharding design that proposed by Dan Krak,"
		},
		{
			"timestamps": {
				"from": "00:00:18,300",
				"to": "00:00:21,500"
			},
			"offsets": {
				"from": 18300,
				"to": 21500
			},
			"text": " he's here today in 2021."
		},
		{
			"timestamps": {
				"from": "00:00:21,500",
				"to": "00:00:27,000"
			},
			"offsets": {
				"from": 21500,
				"to": 27000
			},
			"text": " And so in this new design that it unlocked so many of scaling,"
		},
		{
			"timestamps": {
				"from": "00:00:27,000",
				"to": "00:00:32,000"
			},
			"offsets": {
				"from": 27000,
				"to": 32000
			},
			"text": " I mean, the challenge."
		},
		{
			"timestamps": {
				"from": "00:00:32,000",
				"to": "00:00:36,000"
			},
			"offsets": {
				"from": 32000,
				"to": 36000
			},
			"text": " And it's not the sharded data."
		},
		{
			"timestamps": {
				"from": "00:00:36,000",
				"to": "00:00:42,000"
			},
			"offsets": {
				"from": 36000,
				"to": 42000
			},
			"text": " We don't we shot it at the data blocks rather than having many VPN shots."
		},
		{
			"timestamps": {
				"from": "00:00:42,000",
				"to": "00:00:52,000"
			},
			"offsets": {
				"from": 42000,
				"to": 52000
			},
			"text": " But the dog sharding protocol is there are still some technical challenge that we need to fix."
		},
		{
			"timestamps": {
				"from": "00:00:52,000",
				"to": "00:00:58,000"
			},
			"offsets": {
				"from": 52000,
				"to": 58000
			},
			"text": " So right before we have the full time-sharding protocol is also here."
		},
		{
			"timestamps": {
				"from": "00:00:58,000",
				"to": "00:01:08,000"
			},
			"offsets": {
				"from": 58000,
				"to": 68000
			},
			"text": " And many of us, they collaborated and figured out that we can have a more feasible solution"
		},
		{
			"timestamps": {
				"from": "00:01:08,000",
				"to": "00:01:15,000"
			},
			"offsets": {
				"from": 68000,
				"to": 75000
			},
			"text": " that in the short term to address these scaling."
		},
		{
			"timestamps": {
				"from": "00:01:15,000",
				"to": "00:01:23,000"
			},
			"offsets": {
				"from": 75000,
				"to": 83000
			},
			"text": " And it can greatly scale the Ethereum with their two robots in the very near future."
		},
		{
			"timestamps": {
				"from": "00:01:23,000",
				"to": "00:01:29,000"
			},
			"offsets": {
				"from": 83000,
				"to": 89000
			},
			"text": " So that's the protocol and the protocol and protocol sharding."
		},
		{
			"timestamps": {
				"from": "00:01:29,000",
				"to": "00:01:39,000"
			},
			"offsets": {
				"from": 89000,
				"to": 99000
			},
			"text": " Okay, so there are some many common features in both EIP flavorful and the full time-sharding."
		},
		{
			"timestamps": {
				"from": "00:01:39,000",
				"to": "00:01:50,000"
			},
			"offsets": {
				"from": 99000,
				"to": 110000
			},
			"text": " So today we will break down these topics and you can see that they both have the KZG commitment."
		},
		{
			"timestamps": {
				"from": "00:01:50,000",
				"to": "00:01:55,000"
			},
			"offsets": {
				"from": 110000,
				"to": 115000
			},
			"text": " So Dan Krak will introduce the cryptography part first."
		},
		{
			"timestamps": {
				"from": "00:01:55,000",
				"to": "00:01:58,000"
			},
			"offsets": {
				"from": 115000,
				"to": 118000
			},
			"text": " And they also have the blocks transactions."
		},
		{
			"timestamps": {
				"from": "00:01:58,000",
				"to": "00:02:02,000"
			},
			"offsets": {
				"from": 118000,
				"to": 122000
			},
			"text": " So we will also introduce like what is blocked today."
		},
		{
			"timestamps": {
				"from": "00:02:02,000",
				"to": "00:02:07,000"
			},
			"offsets": {
				"from": 122000,
				"to": 127000
			},
			"text": " And FEMA kit is also the shared common features here."
		},
		{
			"timestamps": {
				"from": "00:02:07,000",
				"to": "00:02:16,000"
			},
			"offsets": {
				"from": 127000,
				"to": 136000
			},
			"text": " And so the challenging part of dog sharding is the PBS and the DS."
		},
		{
			"timestamps": {
				"from": "00:02:16,000",
				"to": "00:02:19,000"
			},
			"offsets": {
				"from": 136000,
				"to": 139000
			},
			"text": " We will also talk about it later."
		},
		{
			"timestamps": {
				"from": "00:02:19,000",
				"to": "00:02:22,000"
			},
			"offsets": {
				"from": 139000,
				"to": 142000
			},
			"text": " Okay, it's the agenda today."
		},
		{
			"timestamps": {
				"from": "00:02:22,000",
				"to": "00:02:27,000"
			},
			"offsets": {
				"from": 142000,
				"to": 147000
			},
			"text": " We have a very rich agenda in the next two hours."
		},
		{
			"timestamps": {
				"from": "00:02:27,000",
				"to": "00:02:36,000"
			},
			"offsets": {
				"from": 147000,
				"to": 156000
			},
			"text": " So yeah, and that will I will hand it to Dan Krak next to introduce the cryptography in dog sharding."
		},
		{
			"timestamps": {
				"from": "00:02:36,000",
				"to": "00:02:43,000"
			},
			"offsets": {
				"from": 156000,
				"to": 163000
			},
			"text": " So if you can go through this, you will know everything."
		},
		{
			"timestamps": {
				"from": "00:02:43,000",
				"to": "00:02:45,000"
			},
			"offsets": {
				"from": 163000,
				"to": 165000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:02:45,000",
				"to": "00:02:49,000"
			},
			"offsets": {
				"from": 165000,
				"to": 169000
			},
			"text": " That's the key."
		},
		{
			"timestamps": {
				"from": "00:02:49,000",
				"to": "00:02:50,000"
			},
			"offsets": {
				"from": 169000,
				"to": 170000
			},
			"text": " Thank you."
		},
		{
			"timestamps": {
				"from": "00:02:50,000",
				"to": "00:02:55,000"
			},
			"offsets": {
				"from": 170000,
				"to": 175000
			},
			"text": " Thank you, Sarah."
		},
		{
			"timestamps": {
				"from": "00:02:55,000",
				"to": "00:02:56,000"
			},
			"offsets": {
				"from": 175000,
				"to": 176000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:02:56,000",
				"to": "00:03:02,000"
			},
			"offsets": {
				"from": 176000,
				"to": 182000
			},
			"text": " So I will be giving an introduction to KZG commitments."
		},
		{
			"timestamps": {
				"from": "00:03:02,000",
				"to": "00:03:12,000"
			},
			"offsets": {
				"from": 182000,
				"to": 192000
			},
			"text": " I will be starting with giving a motivation to understand why we need these advanced polynomial commitments"
		},
		{
			"timestamps": {
				"from": "00:03:12,000",
				"to": "00:03:19,000"
			},
			"offsets": {
				"from": 192000,
				"to": 199000
			},
			"text": " and why we can't do all this simply with Merkle routes, which we are all quite familiar with."
		},
		{
			"timestamps": {
				"from": "00:03:19,000",
				"to": "00:03:23,000"
			},
			"offsets": {
				"from": 199000,
				"to": 203000
			},
			"text": " So I will be going through the motivation."
		},
		{
			"timestamps": {
				"from": "00:03:23,000",
				"to": "00:03:31,000"
			},
			"offsets": {
				"from": 203000,
				"to": 211000
			},
			"text": " I will quickly go give an overview over the finite fields that we use."
		},
		{
			"timestamps": {
				"from": "00:03:31,000",
				"to": "00:03:41,000"
			},
			"offsets": {
				"from": 211000,
				"to": 221000
			},
			"text": " In order to commit to polynomials, I will kind of motivate KZG commitments as like hashes of polynomials"
		},
		{
			"timestamps": {
				"from": "00:03:41,000",
				"to": "00:03:46,000"
			},
			"offsets": {
				"from": 221000,
				"to": 226000
			},
			"text": " then go to the actual meat, which is how do KZG commitments work."
		},
		{
			"timestamps": {
				"from": "00:03:46,000",
				"to": "00:03:51,000"
			},
			"offsets": {
				"from": 226000,
				"to": 231000
			},
			"text": " And finally, because we also use these a lot in our construction now,"
		},
		{
			"timestamps": {
				"from": "00:03:51,000",
				"to": "00:03:56,000"
			},
			"offsets": {
				"from": 231000,
				"to": 236000
			},
			"text": " I will be going through the technique, which I call random evaluation,"
		},
		{
			"timestamps": {
				"from": "00:03:56,000",
				"to": "00:04:01,000"
			},
			"offsets": {
				"from": 236000,
				"to": 241000
			},
			"text": " a nice trick that you can often use to work with polynomials"
		},
		{
			"timestamps": {
				"from": "00:04:01,000",
				"to": "00:04:11,000"
			},
			"offsets": {
				"from": 241000,
				"to": 251000
			},
			"text": " and that makes a lot of things that you want to do with polynomials a lot more efficient."
		},
		{
			"timestamps": {
				"from": "00:04:11,000",
				"to": "00:04:12,000"
			},
			"offsets": {
				"from": 251000,
				"to": 252000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:04:12,000",
				"to": "00:04:17,000"
			},
			"offsets": {
				"from": 252000,
				"to": 257000
			},
			"text": " So let's talk about data available in sampling and erasure coding."
		},
		{
			"timestamps": {
				"from": "00:04:17,000",
				"to": "00:04:23,000"
			},
			"offsets": {
				"from": 257000,
				"to": 263000
			},
			"text": " So what's data available in sampling?"
		},
		{
			"timestamps": {
				"from": "00:04:23,000",
				"to": "00:04:29,000"
			},
			"offsets": {
				"from": 263000,
				"to": 269000
			},
			"text": " So the idea is we somehow have a large blob of data"
		},
		{
			"timestamps": {
				"from": "00:04:29,000",
				"to": "00:04:32,000"
			},
			"offsets": {
				"from": 269000,
				"to": 272000
			},
			"text": " and what we're working on is scalability."
		},
		{
			"timestamps": {
				"from": "00:04:32,000",
				"to": "00:04:40,000"
			},
			"offsets": {
				"from": 272000,
				"to": 280000
			},
			"text": " So scalability means that somehow we have to make it so that a node has to do less work"
		},
		{
			"timestamps": {
				"from": "00:04:40,000",
				"to": "00:04:42,000"
			},
			"offsets": {
				"from": 280000,
				"to": 282000
			},
			"text": " to achieve the same thing that we do today."
		},
		{
			"timestamps": {
				"from": "00:04:42,000",
				"to": "00:04:49,000"
			},
			"offsets": {
				"from": 282000,
				"to": 289000
			},
			"text": " So right now every node ensures that all Ethereum blocks are available by downloading all the blocks."
		},
		{
			"timestamps": {
				"from": "00:04:49,000",
				"to": "00:04:55,000"
			},
			"offsets": {
				"from": 289000,
				"to": 295000
			},
			"text": " So that's just an implicit part of it, like it seems obvious because right now you also execute the full blocks"
		},
		{
			"timestamps": {
				"from": "00:04:55,000",
				"to": "00:04:59,000"
			},
			"offsets": {
				"from": 295000,
				"to": 299000
			},
			"text": " but it's one of the things that don't scale in the currently theorem system."
		},
		{
			"timestamps": {
				"from": "00:04:59,000",
				"to": "00:05:08,000"
			},
			"offsets": {
				"from": 299000,
				"to": 308000
			},
			"text": " So we need a way to reduce this workload, but we want to do it in such a way that we don't lose any of the security that this provides."
		},
		{
			"timestamps": {
				"from": "00:05:08,000",
				"to": "00:05:11,000"
			},
			"offsets": {
				"from": 308000,
				"to": 311000
			},
			"text": " And that's what makes us tricky."
		},
		{
			"timestamps": {
				"from": "00:05:11,000",
				"to": "00:05:28,000"
			},
			"offsets": {
				"from": 311000,
				"to": 328000
			},
			"text": " So the basic idea is, okay, what if we take our data blob and we just take that random samples of the data are available."
		},
		{
			"timestamps": {
				"from": "00:05:28,000",
				"to": "00:05:36,000"
			},
			"offsets": {
				"from": 328000,
				"to": 336000
			},
			"text": " So if we do this naively, if we just take the data as it is, then this does not really work."
		},
		{
			"timestamps": {
				"from": "00:05:36,000",
				"to": "00:05:45,000"
			},
			"offsets": {
				"from": 336000,
				"to": 345000
			},
			"text": " Because even missing a tiny amount of data is catastrophic potentially for a blockchain."
		},
		{
			"timestamps": {
				"from": "00:05:45,000",
				"to": "00:05:53,000"
			},
			"offsets": {
				"from": 345000,
				"to": 353000
			},
			"text": " But by doing random sampling, you can never find out whether a tiny bit is missing."
		},
		{
			"timestamps": {
				"from": "00:05:53,000",
				"to": "00:05:56,000"
			},
			"offsets": {
				"from": 353000,
				"to": 356000
			},
			"text": " You can only see whether major part of the data are missing."
		},
		{
			"timestamps": {
				"from": "00:05:56,000",
				"to": "00:06:03,000"
			},
			"offsets": {
				"from": 356000,
				"to": 363000
			},
			"text": " So what we'll need to do is, in order for this technique to work, is we need to encode the data in such a way"
		},
		{
			"timestamps": {
				"from": "00:06:03,000",
				"to": "00:06:16,000"
			},
			"offsets": {
				"from": 363000,
				"to": 376000
			},
			"text": " that even having some part of the data, say 50%, is enough to guarantee that all the data is available."
		},
		{
			"timestamps": {
				"from": "00:06:16,000",
				"to": "00:06:23,000"
			},
			"offsets": {
				"from": 376000,
				"to": 383000
			},
			"text": " And so the way we do this is we extend the data using a so-called read Solomon code."
		},
		{
			"timestamps": {
				"from": "00:06:23,000",
				"to": "00:06:34,000"
			},
			"offsets": {
				"from": 383000,
				"to": 394000
			},
			"text": " And if you know a little bit about polynomials, like read Solomon code is nothing else but extending the data using polynomials."
		},
		{
			"timestamps": {
				"from": "00:06:34,000",
				"to": "00:06:41,000"
			},
			"offsets": {
				"from": 394000,
				"to": 401000
			},
			"text": " So what you do is, let's say, in the simple example, we have four blocks of original data."
		},
		{
			"timestamps": {
				"from": "00:06:41,000",
				"to": "00:06:48,000"
			},
			"offsets": {
				"from": 401000,
				"to": 408000
			},
			"text": " And what we'll do is we will take these four as evaluations of a polynomial."
		},
		{
			"timestamps": {
				"from": "00:06:48,000",
				"to": "00:06:53,000"
			},
			"offsets": {
				"from": 408000,
				"to": 413000
			},
			"text": " There will always be a polynomial of degree three that goes through these four points."
		},
		{
			"timestamps": {
				"from": "00:06:53,000",
				"to": "00:06:58,000"
			},
			"offsets": {
				"from": 413000,
				"to": 418000
			},
			"text": " And then we can evaluate this polynomial for more points."
		},
		{
			"timestamps": {
				"from": "00:06:58,000",
				"to": "00:07:05,000"
			},
			"offsets": {
				"from": 418000,
				"to": 425000
			},
			"text": " And what this means, because four points always determine a polynomial of degree three, that any of these four points are enough"
		},
		{
			"timestamps": {
				"from": "00:07:05,000",
				"to": "00:07:07,000"
			},
			"offsets": {
				"from": 425000,
				"to": 427000
			},
			"text": " to reconstruct exactly the same polynomial."
		},
		{
			"timestamps": {
				"from": "00:07:07,000",
				"to": "00:07:09,000"
			},
			"offsets": {
				"from": 427000,
				"to": 429000
			},
			"text": " It does not matter which four points you have."
		},
		{
			"timestamps": {
				"from": "00:07:09,000",
				"to": "00:07:13,000"
			},
			"offsets": {
				"from": 429000,
				"to": 433000
			},
			"text": " And so this is the basis of re-major coding."
		},
		{
			"timestamps": {
				"from": "00:07:13,000",
				"to": "00:07:25,000"
			},
			"offsets": {
				"from": 433000,
				"to": 445000
			},
			"text": " And now, because of this, the data availability sampling idea that we had here actually works, because now I don't need to ensure that every single bit of the data is available."
		},
		{
			"timestamps": {
				"from": "00:07:25,000",
				"to": "00:07:32,000"
			},
			"offsets": {
				"from": 445000,
				"to": 452000
			},
			"text": " Now I only need to know that at least 50% of the data available, and then I can always reconstruct everything."
		},
		{
			"timestamps": {
				"from": "00:07:32,000",
				"to": "00:07:33,000"
			},
			"offsets": {
				"from": 452000,
				"to": 453000
			},
			"text": " Yeah, sure."
		},
		{
			"timestamps": {
				"from": "00:07:33,000",
				"to": "00:07:35,000"
			},
			"offsets": {
				"from": 453000,
				"to": 455000
			},
			"text": " Can you tell now double the data?"
		},
		{
			"timestamps": {
				"from": "00:07:35,000",
				"to": "00:07:37,000"
			},
			"offsets": {
				"from": 455000,
				"to": 457000
			},
			"text": " Yes, yeah, we have double the data."
		},
		{
			"timestamps": {
				"from": "00:07:37,000",
				"to": "00:07:39,000"
			},
			"offsets": {
				"from": 457000,
				"to": 459000
			},
			"text": " Actually, because I'm about it."
		},
		{
			"timestamps": {
				"from": "00:07:39,000",
				"to": "00:07:40,000"
			},
			"offsets": {
				"from": 459000,
				"to": 460000
			},
			"text": " No."
		},
		{
			"timestamps": {
				"from": "00:07:40,000",
				"to": "00:07:42,000"
			},
			"offsets": {
				"from": 460000,
				"to": 462000
			},
			"text": " So that's the trick."
		},
		{
			"timestamps": {
				"from": "00:07:42,000",
				"to": "00:07:44,000"
			},
			"offsets": {
				"from": 462000,
				"to": 464000
			},
			"text": " We do random sampling."
		},
		{
			"timestamps": {
				"from": "00:07:44,000",
				"to": "00:07:48,000"
			},
			"offsets": {
				"from": 464000,
				"to": 468000
			},
			"text": " So as an example, we vary 30 random blocks."
		},
		{
			"timestamps": {
				"from": "00:07:48,000",
				"to": "00:07:59,000"
			},
			"offsets": {
				"from": 468000,
				"to": 479000
			},
			"text": " So if the data is not available, that means the attacker needs to have withheld 50%, because if they submitted more than 50% to the network, it's all there."
		},
		{
			"timestamps": {
				"from": "00:07:59,000",
				"to": "00:08:10,000"
			},
			"offsets": {
				"from": 479000,
				"to": 490000
			},
			"text": " Okay, so if it's not available, then each of these samples, because we used local randomness to query them, has only a 50% chance of succeeding."
		},
		{
			"timestamps": {
				"from": "00:08:10,000",
				"to": "00:08:16,000"
			},
			"offsets": {
				"from": 490000,
				"to": 496000
			},
			"text": " So that means in aggregate, the probability that all of them succeed is now 2 to the minus 30, which is 1 in a billion."
		},
		{
			"timestamps": {
				"from": "00:08:16,000",
				"to": "00:08:17,000"
			},
			"offsets": {
				"from": 496000,
				"to": 497000
			},
			"text": " So this is why it scales."
		},
		{
			"timestamps": {
				"from": "00:08:17,000",
				"to": "00:08:21,000"
			},
			"offsets": {
				"from": 497000,
				"to": 501000
			},
			"text": " You don't need to ensure, you don't need to query 50% of the data."
		},
		{
			"timestamps": {
				"from": "00:08:21,000",
				"to": "00:08:31,000"
			},
			"offsets": {
				"from": 501000,
				"to": 511000
			},
			"text": " You only need to do like a tiny number of random samples, and this number is constant, so it does not depend on the amount of data."
		},
		{
			"timestamps": {
				"from": "00:08:31,000",
				"to": "00:08:36,000"
			},
			"offsets": {
				"from": 511000,
				"to": 516000
			},
			"text": " Okay, so what if we, now we have the polynomials, polynomials."
		},
		{
			"timestamps": {
				"from": "00:08:36,000",
				"to": "00:08:54,000"
			},
			"offsets": {
				"from": 516000,
				"to": 534000
			},
			"text": " So we have these evaluations, this was the original data, d0 to d3, and then we have these extensions that we computed, and we just compute a normal Merkle tree on top of that and use this root as our data availability root."
		},
		{
			"timestamps": {
				"from": "00:08:54,000",
				"to": "00:09:03,000"
			},
			"offsets": {
				"from": 534000,
				"to": 543000
			},
			"text": " So the problem with this is that Merkle roots do not tell you anything about the content of the data, so like it could be anything."
		},
		{
			"timestamps": {
				"from": "00:09:03,000",
				"to": "00:09:09,000"
			},
			"offsets": {
				"from": 543000,
				"to": 549000
			},
			"text": " So in this case, let's say an attacker wants to trick our data availability system."
		},
		{
			"timestamps": {
				"from": "00:09:09,000",
				"to": "00:09:16,000"
			},
			"offsets": {
				"from": 549000,
				"to": 556000
			},
			"text": " They could just not use this polynomial extension, but they could just put random data."
		},
		{
			"timestamps": {
				"from": "00:09:16,000",
				"to": "00:09:20,000"
			},
			"offsets": {
				"from": 556000,
				"to": 560000
			},
			"text": " So basically in coding terms, they have provided an invalid code."
		},
		{
			"timestamps": {
				"from": "00:09:20,000",
				"to": "00:09:30,000"
			},
			"offsets": {
				"from": 560000,
				"to": 570000
			},
			"text": " What that means is that any, if you get four different chunks of this data, you would always get a different polynomial."
		},
		{
			"timestamps": {
				"from": "00:09:30,000",
				"to": "00:09:46,000"
			},
			"offsets": {
				"from": 570000,
				"to": 586000
			},
			"text": " So you would, so content is all about agreeing on something, and in this case, we would not actually have agreed on something because the data is different depending on which of these samples we've got."
		},
		{
			"timestamps": {
				"from": "00:09:46,000",
				"to": "00:10:00,000"
			},
			"offsets": {
				"from": 586000,
				"to": 600000
			},
			"text": " So the only way to make this work is if we add fraud proofs to the system where you basically prove that someone has provided this invalid code, but that isn't great."
		},
		{
			"timestamps": {
				"from": "00:10:00,000",
				"to": "00:10:02,000"
			},
			"offsets": {
				"from": 600000,
				"to": 602000
			},
			"text": " So that has some problems."
		},
		{
			"timestamps": {
				"from": "00:10:02,000",
				"to": "00:10:15,000"
			},
			"offsets": {
				"from": 602000,
				"to": 615000
			},
			"text": " They add a lot of complexity, and particularly in this case, because this is about the layer one itself, it would make our system very, very difficult to design because now validators would need to basically wait for the fraud proof in order to know what's going on."
		},
		{
			"timestamps": {
				"from": "00:10:15,000",
				"to": "00:10:20,000"
			},
			"offsets": {
				"from": 615000,
				"to": 620000
			},
			"text": " So that would be kind of very effective to design this."
		},
		{
			"timestamps": {
				"from": "00:10:20,000",
				"to": "00:10:28,000"
			},
			"offsets": {
				"from": 620000,
				"to": 628000
			},
			"text": " So the interesting question is what if we could find some kind of commitment that instead always commits to a polynomial."
		},
		{
			"timestamps": {
				"from": "00:10:28,000",
				"to": "00:10:33,000"
			},
			"offsets": {
				"from": 628000,
				"to": 633000
			},
			"text": " So we always know that the encoding is valid."
		},
		{
			"timestamps": {
				"from": "00:10:33,000",
				"to": "00:10:34,000"
			},
			"offsets": {
				"from": 633000,
				"to": 634000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:10:34,000",
				"to": "00:10:40,000"
			},
			"offsets": {
				"from": 634000,
				"to": 640000
			},
			"text": " And that is why we will introduce KZG commitments, and we need to start a little bit earlier."
		},
		{
			"timestamps": {
				"from": "00:10:40,000",
				"to": "00:10:47,000"
			},
			"offsets": {
				"from": 640000,
				"to": 647000
			},
			"text": " So I will start by introducing finite fields a little bit for those who are not familiar with it."
		},
		{
			"timestamps": {
				"from": "00:10:47,000",
				"to": "00:10:50,000"
			},
			"offsets": {
				"from": 647000,
				"to": 650000
			},
			"text": " Okay, so I can do it for you."
		},
		{
			"timestamps": {
				"from": "00:10:50,000",
				"to": "00:10:52,000"
			},
			"offsets": {
				"from": 650000,
				"to": 652000
			},
			"text": " Okay, so what's the finite field?"
		},
		{
			"timestamps": {
				"from": "00:10:52,000",
				"to": "00:11:08,000"
			},
			"offsets": {
				"from": 652000,
				"to": 668000
			},
			"text": " Okay, so to understand what a field is, it's basically think about rational real or complex numbers, which you've already learned about, and just remind yourself like we have basic operations that we can do in them."
		},
		{
			"timestamps": {
				"from": "00:11:08,000",
				"to": "00:11:26,000"
			},
			"offsets": {
				"from": 668000,
				"to": 686000
			},
			"text": " We can add, subtract, multiply and divide, and we can do that to all of these except division by zero, so that you're always able to do these operations, and you have some laws like their associative commutative, distributive."
		},
		{
			"timestamps": {
				"from": "00:11:26,000",
				"to": "00:11:38,000"
			},
			"offsets": {
				"from": 686000,
				"to": 698000
			},
			"text": " Basically, just think of, I mean, I could give the formal laws here, but I don't think that would be the best illustration because you're already very familiar with these rules when you work with rational or real numbers."
		},
		{
			"timestamps": {
				"from": "00:11:38,000",
				"to": "00:11:51,000"
			},
			"offsets": {
				"from": 698000,
				"to": 711000
			},
			"text": " And the big difference for finite fields is that unlike these fields that we're very familiar with, which all have an infinite number of elements, they have a finite number of elements."
		},
		{
			"timestamps": {
				"from": "00:11:51,000",
				"to": "00:12:00,000"
			},
			"offsets": {
				"from": 711000,
				"to": 720000
			},
			"text": " That's quite important because otherwise we can't encode them with a finite number of bits, which is kind of like something that we need to be able to do."
		},
		{
			"timestamps": {
				"from": "00:12:00,000",
				"to": "00:12:06,000"
			},
			"offsets": {
				"from": 720000,
				"to": 726000
			},
			"text": " So yeah, so that means that each element can be represented using the same number of bits."
		},
		{
			"timestamps": {
				"from": "00:12:06,000",
				"to": "00:12:13,000"
			},
			"offsets": {
				"from": 726000,
				"to": 733000
			},
			"text": " And as an example, of showing how this works, he has a very small finite field, this is F5."
		},
		{
			"timestamps": {
				"from": "00:12:13,000",
				"to": "00:12:30,000"
			},
			"offsets": {
				"from": 733000,
				"to": 750000
			},
			"text": " And basically, the way it works is you take the five numbers, zero, one, two, three, four, and you use your normal integer operations to compute like the addition, subtraction and multiplication."
		},
		{
			"timestamps": {
				"from": "00:12:30,000",
				"to": "00:12:39,000"
			},
			"offsets": {
				"from": 750000,
				"to": 759000
			},
			"text": " But whenever you've done that, you take the result and do, take the remainder after division by five, so you take it modular five."
		},
		{
			"timestamps": {
				"from": "00:12:39,000",
				"to": "00:12:49,000"
			},
			"offsets": {
				"from": 759000,
				"to": 769000
			},
			"text": " And then when you write it out, basically you'll find that for each element, so we haven't yet defined like how do we do division."
		},
		{
			"timestamps": {
				"from": "00:12:49,000",
				"to": "00:12:54,000"
			},
			"offsets": {
				"from": 769000,
				"to": 774000
			},
			"text": " So if you write down the multiplication table, you'll find that each element has actually an inverse."
		},
		{
			"timestamps": {
				"from": "00:12:54,000",
				"to": "00:13:03,000"
			},
			"offsets": {
				"from": 774000,
				"to": 783000
			},
			"text": " So basically, if you take, for example, here like two, then you can see that two times three is six, but modular five, that's one."
		},
		{
			"timestamps": {
				"from": "00:13:03,000",
				"to": "00:13:04,000"
			},
			"offsets": {
				"from": 783000,
				"to": 784000
			},
			"text": " So it has an inverse."
		},
		{
			"timestamps": {
				"from": "00:13:04,000",
				"to": "00:13:17,000"
			},
			"offsets": {
				"from": 784000,
				"to": 797000
			},
			"text": " Okay, that's nice. And like, then the other way around three, the inverse is two, and for four, if you take four times four, it's 16, and that's again modular five, that's one."
		},
		{
			"timestamps": {
				"from": "00:13:17,000",
				"to": "00:13:23,000"
			},
			"offsets": {
				"from": 797000,
				"to": 803000
			},
			"text": " And so we've just found like by just listing these numbers that every element has an inverse."
		},
		{
			"timestamps": {
				"from": "00:13:23,000",
				"to": "00:13:26,000"
			},
			"offsets": {
				"from": 803000,
				"to": 806000
			},
			"text": " And the reason for that, that is that five is a prime number."
		},
		{
			"timestamps": {
				"from": "00:13:26,000",
				"to": "00:13:36,000"
			},
			"offsets": {
				"from": 806000,
				"to": 816000
			},
			"text": " So whenever we could take these modular operations, modular prime number, then we'll find that we actually have a finite field."
		},
		{
			"timestamps": {
				"from": "00:13:36,000",
				"to": "00:13:45,000"
			},
			"offsets": {
				"from": 816000,
				"to": 825000
			},
			"text": " And so that's our finite fields, except that the fields we're going to be working with in practice will have a lot more elements."
		},
		{
			"timestamps": {
				"from": "00:13:45,000",
				"to": "00:13:55,000"
			},
			"offsets": {
				"from": 825000,
				"to": 835000
			},
			"text": " So the prime that we're going to be using will have 255 bits, so it's like a very, very big number because, yeah, we want to be able to present a lot of numbers in this field."
		},
		{
			"timestamps": {
				"from": "00:13:55,000",
				"to": "00:14:03,000"
			},
			"offsets": {
				"from": 835000,
				"to": 843000
			},
			"text": " Cool. Okay, so let's think about hashing polynomials."
		},
		{
			"timestamps": {
				"from": "00:14:03,000",
				"to": "00:14:07,000"
			},
			"offsets": {
				"from": 843000,
				"to": 847000
			},
			"text": " Okay, so a quick reminder, what's a polynomial?"
		},
		{
			"timestamps": {
				"from": "00:14:07,000",
				"to": "00:14:11,000"
			},
			"offsets": {
				"from": 847000,
				"to": 851000
			},
			"text": " So a polynomial is an expression of this form."
		},
		{
			"timestamps": {
				"from": "00:14:11,000",
				"to": "00:14:23,000"
			},
			"offsets": {
				"from": 851000,
				"to": 863000
			},
			"text": " So it's like a sum over some coefficients, Fi, and terms of the term of the form x to the power of i."
		},
		{
			"timestamps": {
				"from": "00:14:23,000",
				"to": "00:14:31,000"
			},
			"offsets": {
				"from": 863000,
				"to": 871000
			},
			"text": " So the property is that this, it has to be finite sum, so it says sum from zero to n."
		},
		{
			"timestamps": {
				"from": "00:14:31,000",
				"to": "00:14:34,000"
			},
			"offsets": {
				"from": 871000,
				"to": 874000
			},
			"text": " And we is the degree of the polynomial."
		},
		{
			"timestamps": {
				"from": "00:14:34,000",
				"to": "00:14:41,000"
			},
			"offsets": {
				"from": 874000,
				"to": 881000
			},
			"text": " And basically the other important thing that you have to always remind yourself there can never be any negative terms."
		},
		{
			"timestamps": {
				"from": "00:14:41,000",
				"to": "00:14:43,000"
			},
			"offsets": {
				"from": 881000,
				"to": 883000
			},
			"text": " So you cannot have x to the minus one."
		},
		{
			"timestamps": {
				"from": "00:14:43,000",
				"to": "00:14:49,000"
			},
			"offsets": {
				"from": 883000,
				"to": 889000
			},
			"text": " It's only terms of the form x to the power of zero, one, two, three, and so on."
		},
		{
			"timestamps": {
				"from": "00:14:49,000",
				"to": "00:14:56,000"
			},
			"offsets": {
				"from": 889000,
				"to": 896000
			},
			"text": " Yeah, and each polynomial defines a polynomial function."
		},
		{
			"timestamps": {
				"from": "00:14:56,000",
				"to": "00:14:58,000"
			},
			"offsets": {
				"from": 896000,
				"to": 898000
			},
			"text": " So it's important to distinguish between the two."
		},
		{
			"timestamps": {
				"from": "00:14:58,000",
				"to": "00:15:01,000"
			},
			"offsets": {
				"from": 898000,
				"to": 901000
			},
			"text": " So polynomial is just an expression of this type."
		},
		{
			"timestamps": {
				"from": "00:15:01,000",
				"to": "00:15:06,000"
			},
			"offsets": {
				"from": 901000,
				"to": 906000
			},
			"text": " So it's just, you could think of it even as a list of coefficients."
		},
		{
			"timestamps": {
				"from": "00:15:06,000",
				"to": "00:15:08,000"
			},
			"offsets": {
				"from": 906000,
				"to": 908000
			},
			"text": " And then it defines a function."
		},
		{
			"timestamps": {
				"from": "00:15:08,000",
				"to": "00:15:18,000"
			},
			"offsets": {
				"from": 908000,
				"to": 918000
			},
			"text": " But like, for example, in some fields, like in finite fields, you will have the property that the same polynomial function can have many polynomials corresponding to it."
		},
		{
			"timestamps": {
				"from": "00:15:18,000",
				"to": "00:15:26,000"
			},
			"offsets": {
				"from": 918000,
				"to": 926000
			},
			"text": " Because there's only a finite number of functions, but there's an infinite number of polynomials."
		},
		{
			"timestamps": {
				"from": "00:15:26,000",
				"to": "00:15:31,000"
			},
			"offsets": {
				"from": 926000,
				"to": 931000
			},
			"text": " This property you don't have in infinite fields."
		},
		{
			"timestamps": {
				"from": "00:15:31,000",
				"to": "00:15:43,000"
			},
			"offsets": {
				"from": 931000,
				"to": 943000
			},
			"text": " And so the cool property that polynomials have is that for any k points, there will always be a polynomial of k minus one or lower that goes through all of these points."
		},
		{
			"timestamps": {
				"from": "00:15:43,000",
				"to": "00:15:49,000"
			},
			"offsets": {
				"from": 943000,
				"to": 949000
			},
			"text": " And that's polynomial is unique."
		},
		{
			"timestamps": {
				"from": "00:15:49,000",
				"to": "00:16:02,000"
			},
			"offsets": {
				"from": 949000,
				"to": 962000
			},
			"text": " And the other property is that a polynomial of degree n that is not constant has at most n zeros."
		},
		{
			"timestamps": {
				"from": "00:16:02,000",
				"to": "00:16:14,000"
			},
			"offsets": {
				"from": 962000,
				"to": 974000
			},
			"text": " So what would be cool if we could imagine a hash function for polynomials?"
		},
		{
			"timestamps": {
				"from": "00:16:14,000",
				"to": "00:16:23,000"
			},
			"offsets": {
				"from": 974000,
				"to": 983000
			},
			"text": " So let's imagine that we could have a hash function that takes a polynomial and hashes it."
		},
		{
			"timestamps": {
				"from": "00:16:23,000",
				"to": "00:16:24,000"
			},
			"offsets": {
				"from": 983000,
				"to": 984000
			},
			"text": " Okay, that's easy."
		},
		{
			"timestamps": {
				"from": "00:16:24,000",
				"to": "00:16:30,000"
			},
			"offsets": {
				"from": 984000,
				"to": 990000
			},
			"text": " But it should have an extra property which is that we can construct proofs of evaluation."
		},
		{
			"timestamps": {
				"from": "00:16:30,000",
				"to": "00:16:43,000"
			},
			"offsets": {
				"from": 990000,
				"to": 1003000
			},
			"text": " So basically what we want is that for any z, so any point, we can evaluate this polynomials, compute y equals f of z."
		},
		{
			"timestamps": {
				"from": "00:16:43,000",
				"to": "00:16:48,000"
			},
			"offsets": {
				"from": 1003000,
				"to": 1008000
			},
			"text": " And we want some proof that this is correct."
		},
		{
			"timestamps": {
				"from": "00:16:48,000",
				"to": "00:16:56,000"
			},
			"offsets": {
				"from": 1008000,
				"to": 1016000
			},
			"text": " So that would be an interesting hash of polynomials that gives us something new."
		},
		{
			"timestamps": {
				"from": "00:16:56,000",
				"to": "00:17:05,000"
			},
			"offsets": {
				"from": 1016000,
				"to": 1025000
			},
			"text": " And this hash and the proof should be small in some sense."
		},
		{
			"timestamps": {
				"from": "00:17:05,000",
				"to": "00:17:09,000"
			},
			"offsets": {
				"from": 1025000,
				"to": 1029000
			},
			"text": " So here's some idea."
		},
		{
			"timestamps": {
				"from": "00:17:09,000",
				"to": "00:17:13,000"
			},
			"offsets": {
				"from": 1029000,
				"to": 1033000
			},
			"text": " Okay, what if we just choose a random number?"
		},
		{
			"timestamps": {
				"from": "00:17:13,000",
				"to": "00:17:18,000"
			},
			"offsets": {
				"from": 1033000,
				"to": 1038000
			},
			"text": " For example, let's say we choose the number three."
		},
		{
			"timestamps": {
				"from": "00:17:18,000",
				"to": "00:17:25,000"
			},
			"offsets": {
				"from": 1038000,
				"to": 1045000
			},
			"text": " If we want to hash a polynomial, we just evaluate it at this random number three."
		},
		{
			"timestamps": {
				"from": "00:17:25,000",
				"to": "00:17:29,000"
			},
			"offsets": {
				"from": 1045000,
				"to": 1049000
			},
			"text": " So we put x equals three."
		},
		{
			"timestamps": {
				"from": "00:17:29,000",
				"to": "00:17:33,000"
			},
			"offsets": {
				"from": 1049000,
				"to": 1053000
			},
			"text": " Here's a couple of examples how that works."
		},
		{
			"timestamps": {
				"from": "00:17:33,000",
				"to": "00:17:39,000"
			},
			"offsets": {
				"from": 1053000,
				"to": 1059000
			},
			"text": " If we stay in our small field f5 between f and b4 with just those five numbers."
		},
		{
			"timestamps": {
				"from": "00:17:39,000",
				"to": "00:17:50,000"
			},
			"offsets": {
				"from": 1059000,
				"to": 1070000
			},
			"text": " If our polynomials x squared plus two x plus four, then the hash is like x squared nine plus two x six plus four, modulo five is four."
		},
		{
			"timestamps": {
				"from": "00:17:50,000",
				"to": "00:17:57,000"
			},
			"offsets": {
				"from": 1070000,
				"to": 1077000
			},
			"text": " And then here's a second example, so a bit of a bigger polynomial, module five, and in this case it's zero."
		},
		{
			"timestamps": {
				"from": "00:17:57,000",
				"to": "00:18:01,000"
			},
			"offsets": {
				"from": 1077000,
				"to": 1081000
			},
			"text": " Okay, that seems a bit stupid to just do it at one point."
		},
		{
			"timestamps": {
				"from": "00:18:01,000",
				"to": "00:18:08,000"
			},
			"offsets": {
				"from": 1081000,
				"to": 1088000
			},
			"text": " But the interesting thing is, if our modulo has 256 bits, which is what we're going to work with in practice,"
		},
		{
			"timestamps": {
				"from": "00:18:08,000",
				"to": "00:18:17,000"
			},
			"offsets": {
				"from": 1088000,
				"to": 1097000
			},
			"text": " it's actually extremely unlikely that two randomly chosen polynomials have the same hash and quotation marks,"
		},
		{
			"timestamps": {
				"from": "00:18:17,000",
				"to": "00:18:20,000"
			},
			"offsets": {
				"from": 1097000,
				"to": 1100000
			},
			"text": " just like it is for a normal hash function."
		},
		{
			"timestamps": {
				"from": "00:18:20,000",
				"to": "00:18:23,000"
			},
			"offsets": {
				"from": 1100000,
				"to": 1103000
			},
			"text": " So that's an interesting property."
		},
		{
			"timestamps": {
				"from": "00:18:23,000",
				"to": "00:18:32,000"
			},
			"offsets": {
				"from": 1103000,
				"to": 1112000
			},
			"text": " It seems like a very stupid and simple operation, but in some ways, in one way, it already has a property like a hash."
		},
		{
			"timestamps": {
				"from": "00:18:32,000",
				"to": "00:18:35,000"
			},
			"offsets": {
				"from": 1112000,
				"to": 1115000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:18:35,000",
				"to": "00:18:42,000"
			},
			"offsets": {
				"from": 1115000,
				"to": 1122000
			},
			"text": " So if we accept this for now, then let's have a look at some of the things we could do with it."
		},
		{
			"timestamps": {
				"from": "00:18:42,000",
				"to": "00:18:48,000"
			},
			"offsets": {
				"from": 1122000,
				"to": 1128000
			},
			"text": " So for example, we can actually add two hashes of polynomials."
		},
		{
			"timestamps": {
				"from": "00:18:48,000",
				"to": "00:18:59,000"
			},
			"offsets": {
				"from": 1128000,
				"to": 1139000
			},
			"text": " So like if we have the hash of, oops, if the tag of the hash of two functions, hash of f and f hash of g,"
		},
		{
			"timestamps": {
				"from": "00:18:59,000",
				"to": "00:19:09,000"
			},
			"offsets": {
				"from": 1139000,
				"to": 1149000
			},
			"text": " then the hash will just be the sum, like the hash of the, some of the functions will just be the hash of f and plus the hash of g."
		},
		{
			"timestamps": {
				"from": "00:19:09,000",
				"to": "00:19:16,000"
			},
			"offsets": {
				"from": 1149000,
				"to": 1156000
			},
			"text": " And that's because of this homomorphic property, which is trivial if you write it out in polynomials."
		},
		{
			"timestamps": {
				"from": "00:19:16,000",
				"to": "00:19:23,000"
			},
			"offsets": {
				"from": 1156000,
				"to": 1163000
			},
			"text": " And the same is true if you multiply two of these polynomials."
		},
		{
			"timestamps": {
				"from": "00:19:23,000",
				"to": "00:19:28,000"
			},
			"offsets": {
				"from": 1163000,
				"to": 1168000
			},
			"text": " And that's just because polynomial evaluation itself is a homomorphic property."
		},
		{
			"timestamps": {
				"from": "00:19:28,000",
				"to": "00:19:33,000"
			},
			"offsets": {
				"from": 1168000,
				"to": 1173000
			},
			"text": " Like if you, you can either first add two polynomials, any value item, or you can like,"
		},
		{
			"timestamps": {
				"from": "00:19:33,000",
				"to": "00:19:38,000"
			},
			"offsets": {
				"from": 1173000,
				"to": 1178000
			},
			"text": " evaluate them and then add the result and the same for multiplication."
		},
		{
			"timestamps": {
				"from": "00:19:38,000",
				"to": "00:19:43,000"
			},
			"offsets": {
				"from": 1178000,
				"to": 1183000
			},
			"text": " So it has some really cool properties if we could use this hash function."
		},
		{
			"timestamps": {
				"from": "00:19:43,000",
				"to": "00:19:46,000"
			},
			"offsets": {
				"from": 1183000,
				"to": 1186000
			},
			"text": " But there's one problem."
		},
		{
			"timestamps": {
				"from": "00:19:46,000",
				"to": "00:19:58,000"
			},
			"offsets": {
				"from": 1186000,
				"to": 1198000
			},
			"text": " If, if you use this, then if someone knows this random number, right, then they could easily create a collision of this polynomial hash function."
		},
		{
			"timestamps": {
				"from": "00:19:58,000",
				"to": "00:20:04,000"
			},
			"offsets": {
				"from": 1198000,
				"to": 1204000
			},
			"text": " Because while for random polynomials, it's very unlikely that they evaluate to the same point."
		},
		{
			"timestamps": {
				"from": "00:20:04,000",
				"to": "00:20:13,000"
			},
			"offsets": {
				"from": 1204000,
				"to": 1213000
			},
			"text": " It is very easy to create like manually two polynomials that evaluate to the same value at this random number."
		},
		{
			"timestamps": {
				"from": "00:20:13,000",
				"to": "00:20:18,000"
			},
			"offsets": {
				"from": 1213000,
				"to": 1218000
			},
			"text": " So it doesn't quite work as a hash function as we know it."
		},
		{
			"timestamps": {
				"from": "00:20:18,000",
				"to": "00:20:29,000"
			},
			"offsets": {
				"from": 1218000,
				"to": 1229000
			},
			"text": " But what, it would be different if somehow instead we could put this random number into a black box."
		},
		{
			"timestamps": {
				"from": "00:20:29,000",
				"to": "00:20:36,000"
			},
			"offsets": {
				"from": 1229000,
				"to": 1236000
			},
			"text": " So if we could, if we could find a way of computing with these finite field elements."
		},
		{
			"timestamps": {
				"from": "00:20:36,000",
				"to": "00:20:47,000"
			},
			"offsets": {
				"from": 1236000,
				"to": 1247000
			},
			"text": " But instead of giving everyone who wants to evaluate this hash function, giving them the actual number, you give them a black box."
		},
		{
			"timestamps": {
				"from": "00:20:47,000",
				"to": "00:20:53,000"
			},
			"offsets": {
				"from": 1247000,
				"to": 1253000
			},
			"text": " So like we, we said, assume we have a cryptographic way of putting a number into a black box."
		},
		{
			"timestamps": {
				"from": "00:20:53,000",
				"to": "00:20:58,000"
			},
			"offsets": {
				"from": 1253000,
				"to": 1258000
			},
			"text": " And then we give them our random number S."
		},
		{
			"timestamps": {
				"from": "00:20:58,000",
				"to": "00:21:03,000"
			},
			"offsets": {
				"from": 1258000,
				"to": 1263000
			},
			"text": " And we give them also like the random number S squared and S to the power of three."
		},
		{
			"timestamps": {
				"from": "00:21:03,000",
				"to": "00:21:06,000"
			},
			"offsets": {
				"from": 1263000,
				"to": 1266000
			},
			"text": " But all of them own in the black box."
		},
		{
			"timestamps": {
				"from": "00:21:06,000",
				"to": "00:21:14,000"
			},
			"offsets": {
				"from": 1266000,
				"to": 1274000
			},
			"text": " And we do it in such a way like this black box needs to have the property that you can multiply it with another number."
		},
		{
			"timestamps": {
				"from": "00:21:14,000",
				"to": "00:21:22,000"
			},
			"offsets": {
				"from": 1274000,
				"to": 1282000
			},
			"text": " And you can add two of these, but you cannot multiply two numbers in a black box."
		},
		{
			"timestamps": {
				"from": "00:21:22,000",
				"to": "00:21:27,000"
			},
			"offsets": {
				"from": 1282000,
				"to": 1287000
			},
			"text": " So if that, if you could do that, then this would actually work."
		},
		{
			"timestamps": {
				"from": "00:21:27,000",
				"to": "00:21:36,000"
			},
			"offsets": {
				"from": 1287000,
				"to": 1296000
			},
			"text": " Because now the attacker would not be able to like create these two polynomials because they don't know, they don't know this number."
		},
		{
			"timestamps": {
				"from": "00:21:36,000",
				"to": "00:21:49,000"
			},
			"offsets": {
				"from": 1296000,
				"to": 1309000
			},
			"text": " And so they cannot craft, hand craft the polynomials so that they evaluate to the same number at that point."
		},
		{
			"timestamps": {
				"from": "00:21:49,000",
				"to": "00:21:56,000"
			},
			"offsets": {
				"from": 1309000,
				"to": 1316000
			},
			"text": " And basically the cool thing is that elliptic curves actually give you, give you exactly that."
		},
		{
			"timestamps": {
				"from": "00:21:56,000",
				"to": "00:22:08,000"
			},
			"offsets": {
				"from": 1316000,
				"to": 1328000
			},
			"text": " So elliptic curves are basically, you can think of them as a way of creating black box finite field elements."
		},
		{
			"timestamps": {
				"from": "00:22:08,000",
				"to": "00:22:13,000"
			},
			"offsets": {
				"from": 1328000,
				"to": 1333000
			},
			"text": " And the finite field that you have to use is the curve order of that elliptic curve."
		},
		{
			"timestamps": {
				"from": "00:22:13,000",
				"to": "00:22:21,000"
			},
			"offsets": {
				"from": 1333000,
				"to": 1341000
			},
			"text": " So if we have an elliptic curve, which we call G1, why we need this in XG1, we'll come to later."
		},
		{
			"timestamps": {
				"from": "00:22:21,000",
				"to": "00:22:32,000"
			},
			"offsets": {
				"from": 1341000,
				"to": 1352000
			},
			"text": " It's just elliptic curve that has a generator, which means that that's a point so that the, if you add that point again and again,"
		},
		{
			"timestamps": {
				"from": "00:22:32,000",
				"to": "00:22:35,000"
			},
			"offsets": {
				"from": 1352000,
				"to": 1355000
			},
			"text": " it will generate your whole curve."
		},
		{
			"timestamps": {
				"from": "00:22:35,000",
				"to": "00:22:39,000"
			},
			"offsets": {
				"from": 1355000,
				"to": 1359000
			},
			"text": " And the order of the curve is P, so that's the number of points."
		},
		{
			"timestamps": {
				"from": "00:22:39,000",
				"to": "00:22:49,000"
			},
			"offsets": {
				"from": 1359000,
				"to": 1369000
			},
			"text": " And then basically you have the property that X times G, where X is a finite field element, X times G1,"
		},
		{
			"timestamps": {
				"from": "00:22:49,000",
				"to": "00:22:51,000"
			},
			"offsets": {
				"from": 1369000,
				"to": 1371000
			},
			"text": " is basically this black box."
		},
		{
			"timestamps": {
				"from": "00:22:51,000",
				"to": "00:22:56,000"
			},
			"offsets": {
				"from": 1371000,
				"to": 1376000
			},
			"text": " And the reason for that is that it is hard to compute so-called discrete logarithms."
		},
		{
			"timestamps": {
				"from": "00:22:56,000",
				"to": "00:23:10,000"
			},
			"offsets": {
				"from": 1376000,
				"to": 1390000
			},
			"text": " So it's difficult, like when you have computed this X times G1, it is difficult to compute X from that point."
		},
		{
			"timestamps": {
				"from": "00:23:10,000",
				"to": "00:23:13,000"
			},
			"offsets": {
				"from": 1390000,
				"to": 1393000
			},
			"text": " So that's a cryptographic assumption."
		},
		{
			"timestamps": {
				"from": "00:23:13,000",
				"to": "00:23:27,000"
			},
			"offsets": {
				"from": 1393000,
				"to": 1407000
			},
			"text": " So if we have that, then if we take two elliptic curve elements, G and H, then we can multiply them with field elements,"
		},
		{
			"timestamps": {
				"from": "00:23:27,000",
				"to": "00:23:36,000"
			},
			"offsets": {
				"from": 1407000,
				"to": 1416000
			},
			"text": " like we can compute X times G, we can add the two G plus H, and we can compute linear combinations like X times G plus Y times H."
		},
		{
			"timestamps": {
				"from": "00:23:36,000",
				"to": "00:23:42,000"
			},
			"offsets": {
				"from": 1416000,
				"to": 1422000
			},
			"text": " But we can't compute this, we can't, without computing the discrete logarithm with each heart,"
		},
		{
			"timestamps": {
				"from": "00:23:42,000",
				"to": "00:23:45,000"
			},
			"offsets": {
				"from": 1422000,
				"to": 1425000
			},
			"text": " we can't compute something like G times H."
		},
		{
			"timestamps": {
				"from": "00:23:45,000",
				"to": "00:23:54,000"
			},
			"offsets": {
				"from": 1425000,
				"to": 1434000
			},
			"text": " And so just like we said before, like we want this black box, so we will introduce the notation,"
		},
		{
			"timestamps": {
				"from": "00:23:54,000",
				"to": "00:24:01,000"
			},
			"offsets": {
				"from": 1434000,
				"to": 1441000
			},
			"text": " like X and squared brackets, one for saying that it's in this G1, which is the first elliptic curve we're going to use,"
		},
		{
			"timestamps": {
				"from": "00:24:01,000",
				"to": "00:24:06,000"
			},
			"offsets": {
				"from": 1441000,
				"to": 1446000
			},
			"text": " we later will need another one, we define that at X times G1."
		},
		{
			"timestamps": {
				"from": "00:24:06,000",
				"to": "00:24:14,000"
			},
			"offsets": {
				"from": 1446000,
				"to": 1454000
			},
			"text": " And so basically when you see these square brackets, think of it as like, this is a prime field element in this black box,"
		},
		{
			"timestamps": {
				"from": "00:24:14,000",
				"to": "00:24:16,000"
			},
			"offsets": {
				"from": 1454000,
				"to": 1456000
			},
			"text": " in this elliptic curve black box."
		},
		{
			"timestamps": {
				"from": "00:24:16,000",
				"to": "00:24:24,000"
			},
			"offsets": {
				"from": 1456000,
				"to": 1464000
			},
			"text": " So we can put stuff inside, and there's no easy way to take them back out, but we can do some computations while they're in there."
		},
		{
			"timestamps": {
				"from": "00:24:24,000",
				"to": "00:24:25,000"
			},
			"offsets": {
				"from": 1464000,
				"to": 1465000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:24:25,000",
				"to": "00:24:29,000"
			},
			"offsets": {
				"from": 1465000,
				"to": 1469000
			},
			"text": " And with this, we are ready to introduce KZG commitments."
		},
		{
			"timestamps": {
				"from": "00:24:29,000",
				"to": "00:24:30,000"
			},
			"offsets": {
				"from": 1469000,
				"to": 1470000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:24:30,000",
				"to": "00:24:34,000"
			},
			"offsets": {
				"from": 1470000,
				"to": 1474000
			},
			"text": " So what we're going to do is we're going to introduce a trusted setup."
		},
		{
			"timestamps": {
				"from": "00:24:34,000",
				"to": "00:24:47,000"
			},
			"offsets": {
				"from": 1474000,
				"to": 1487000
			},
			"text": " So we're going to assume that someone has computed, has taken a random number S,"
		},
		{
			"timestamps": {
				"from": "00:24:47,000",
				"to": "00:24:56,000"
			},
			"offsets": {
				"from": 1487000,
				"to": 1496000
			},
			"text": " and they've computed inside this block box and given to us the powers of S, S to the power of 0, 1, 2, 3, and so on,"
		},
		{
			"timestamps": {
				"from": "00:24:56,000",
				"to": "00:24:58,000"
			},
			"offsets": {
				"from": 1496000,
				"to": 1498000
			},
			"text": " in our black box."
		},
		{
			"timestamps": {
				"from": "00:24:58,000",
				"to": "00:25:02,000"
			},
			"offsets": {
				"from": 1498000,
				"to": 1502000
			},
			"text": " And actually forget this second one, for now we'll come to that later."
		},
		{
			"timestamps": {
				"from": "00:25:02,000",
				"to": "00:25:10,000"
			},
			"offsets": {
				"from": 1502000,
				"to": 1510000
			},
			"text": " And so if we take a polynomial function, so we've defined this previously, so it looks like this,"
		},
		{
			"timestamps": {
				"from": "00:25:10,000",
				"to": "00:25:14,000"
			},
			"offsets": {
				"from": 1510000,
				"to": 1514000
			},
			"text": " it's like a sum of coefficients times powers of X."
		},
		{
			"timestamps": {
				"from": "00:25:14,000",
				"to": "00:25:22,000"
			},
			"offsets": {
				"from": 1514000,
				"to": 1522000
			},
			"text": " And we define the KZG commitments as this sum, which we can evaluate."
		},
		{
			"timestamps": {
				"from": "00:25:22,000",
				"to": "00:25:30,000"
			},
			"offsets": {
				"from": 1522000,
				"to": 1530000
			},
			"text": " So we take the coefficients and we replace X to the power of I by S to the power of I inside this black box."
		},
		{
			"timestamps": {
				"from": "00:25:30,000",
				"to": "00:25:33,000"
			},
			"offsets": {
				"from": 1530000,
				"to": 1533000
			},
			"text": " And here on the left, like so this is something we can obviously compute."
		},
		{
			"timestamps": {
				"from": "00:25:33,000",
				"to": "00:25:40,000"
			},
			"offsets": {
				"from": 1533000,
				"to": 1540000
			},
			"text": " It's just a linear combination of these elements which we have been given as part of the trusted setup."
		},
		{
			"timestamps": {
				"from": "00:25:40,000",
				"to": "00:25:52,000"
			},
			"offsets": {
				"from": 1540000,
				"to": 1552000
			},
			"text": " And the cool thing is if you write this out in effect, if it is just F to the power of S evaluated inside this black box."
		},
		{
			"timestamps": {
				"from": "00:25:52,000",
				"to": "00:26:00,000"
			},
			"offsets": {
				"from": 1552000,
				"to": 1560000
			},
			"text": " So effectively we've come back to what we said before, it's just this random evaluation."
		},
		{
			"timestamps": {
				"from": "00:26:00,000",
				"to": "00:26:08,000"
			},
			"offsets": {
				"from": 1560000,
				"to": 1568000
			},
			"text": " But we've managed to now randomly evaluate this polynomial inside a black box at the secret point."
		},
		{
			"timestamps": {
				"from": "00:26:08,000",
				"to": "00:26:19,000"
			},
			"offsets": {
				"from": 1568000,
				"to": 1579000
			},
			"text": " And yes, and this we call the KZG commitment to the function F."
		},
		{
			"timestamps": {
				"from": "00:26:19,000",
				"to": "00:26:28,000"
			},
			"offsets": {
				"from": 1579000,
				"to": 1588000
			},
			"text": " And now in order to do interesting things with this, we'll need to introduce elliptic curve pairing."
		},
		{
			"timestamps": {
				"from": "00:26:28,000",
				"to": "00:26:31,000"
			},
			"offsets": {
				"from": 1588000,
				"to": 1591000
			},
			"text": " So this is where we get our second group."
		},
		{
			"timestamps": {
				"from": "00:26:31,000",
				"to": "00:26:33,000"
			},
			"offsets": {
				"from": 1591000,
				"to": 1593000
			},
			"text": " So we actually need a total of two groups."
		},
		{
			"timestamps": {
				"from": "00:26:33,000",
				"to": "00:26:41,000"
			},
			"offsets": {
				"from": 1593000,
				"to": 1601000
			},
			"text": " And what we'll have is we have a pairing as a function from two elliptic curve groups and a target group,"
		},
		{
			"timestamps": {
				"from": "00:26:41,000",
				"to": "00:26:46,000"
			},
			"offsets": {
				"from": 1601000,
				"to": 1606000
			},
			"text": " which is a different kind of group, it's actually not an elliptic curve, but that's not too important here."
		},
		{
			"timestamps": {
				"from": "00:26:46,000",
				"to": "00:26:52,000"
			},
			"offsets": {
				"from": 1606000,
				"to": 1612000
			},
			"text": " And it takes basically these two elliptic curve elements, one in G1 and one in G2."
		},
		{
			"timestamps": {
				"from": "00:26:52,000",
				"to": "00:26:58,000"
			},
			"offsets": {
				"from": 1612000,
				"to": 1618000
			},
			"text": " And it has the cool property that it is what we call bilinear."
		},
		{
			"timestamps": {
				"from": "00:26:58,000",
				"to": "00:27:08,000"
			},
			"offsets": {
				"from": 1618000,
				"to": 1628000
			},
			"text": " And so that what that means is that you can compute this linear combination."
		},
		{
			"timestamps": {
				"from": "00:27:08,000",
				"to": "00:27:16,000"
			},
			"offsets": {
				"from": 1628000,
				"to": 1636000
			},
			"text": " So for example, if you have the pairing of A times X and Z, that's basically you can take this A out,"
		},
		{
			"timestamps": {
				"from": "00:27:16,000",
				"to": "00:27:20,000"
			},
			"offsets": {
				"from": 1636000,
				"to": 1640000
			},
			"text": " the same in the second coefficient."
		},
		{
			"timestamps": {
				"from": "00:27:20,000",
				"to": "00:27:31,000"
			},
			"offsets": {
				"from": 1640000,
				"to": 1651000
			},
			"text": " And in addition, if you have the sum, then basically what it does, it splits into these two."
		},
		{
			"timestamps": {
				"from": "00:27:31,000",
				"to": "00:27:38,000"
			},
			"offsets": {
				"from": 1651000,
				"to": 1658000
			},
			"text": " So it's like a distributive law here, X plus Y times Z is E of X, Z and E of Y, Z."
		},
		{
			"timestamps": {
				"from": "00:27:38,000",
				"to": "00:27:44,000"
			},
			"offsets": {
				"from": 1658000,
				"to": 1664000
			},
			"text": " And the same goes again in the second parameter of this function."
		},
		{
			"timestamps": {
				"from": "00:27:44,000",
				"to": "00:27:55,000"
			},
			"offsets": {
				"from": 1664000,
				"to": 1675000
			},
			"text": " And basically the cool thing is that what we couldn't do before inside, yeah?"
		},
		{
			"timestamps": {
				"from": "00:27:55,000",
				"to": "00:28:02,000"
			},
			"offsets": {
				"from": 1675000,
				"to": 1682000
			},
			"text": " [INAUDIBLE]"
		},
		{
			"timestamps": {
				"from": "00:28:02,000",
				"to": "00:28:03,000"
			},
			"offsets": {
				"from": 1682000,
				"to": 1683000
			},
			"text": " Yes."
		},
		{
			"timestamps": {
				"from": "00:28:03,000",
				"to": "00:28:17,000"
			},
			"offsets": {
				"from": 1683000,
				"to": 1697000
			},
			"text": " And so what we couldn't do previously between elliptic curve points, which is multiply to elliptic curve points,"
		},
		{
			"timestamps": {
				"from": "00:28:17,000",
				"to": "00:28:22,000"
			},
			"offsets": {
				"from": 1697000,
				"to": 1702000
			},
			"text": " is that we can do in a way between pairings."
		},
		{
			"timestamps": {
				"from": "00:28:22,000",
				"to": "00:28:28,000"
			},
			"offsets": {
				"from": 1702000,
				"to": 1708000
			},
			"text": " So if we have one of our points in this first group and one in the second group,"
		},
		{
			"timestamps": {
				"from": "00:28:28,000",
				"to": "00:28:35,000"
			},
			"offsets": {
				"from": 1708000,
				"to": 1715000
			},
			"text": " then due to this bilinear property, we can add it actually in the target group,"
		},
		{
			"timestamps": {
				"from": "00:28:35,000",
				"to": "00:28:38,000"
			},
			"offsets": {
				"from": 1715000,
				"to": 1718000
			},
			"text": " it computes something like X times Y, right?"
		},
		{
			"timestamps": {
				"from": "00:28:38,000",
				"to": "00:28:40,000"
			},
			"offsets": {
				"from": 1718000,
				"to": 1720000
			},
			"text": " So it has this property."
		},
		{
			"timestamps": {
				"from": "00:28:40,000",
				"to": "00:28:43,000"
			},
			"offsets": {
				"from": 1720000,
				"to": 1723000
			},
			"text": " We define this additional notation for the target group."
		},
		{
			"timestamps": {
				"from": "00:28:43,000",
				"to": "00:28:49,000"
			},
			"offsets": {
				"from": 1723000,
				"to": 1729000
			},
			"text": " And then we have this very clean and nice equation that the pairing of X as a black box element,"
		},
		{
			"timestamps": {
				"from": "00:28:49,000",
				"to": "00:28:53,000"
			},
			"offsets": {
				"from": 1729000,
				"to": 1733000
			},
			"text": " Y as a black box element, is X times Y."
		},
		{
			"timestamps": {
				"from": "00:28:53,000",
				"to": "00:28:55,000"
			},
			"offsets": {
				"from": 1733000,
				"to": 1735000
			},
			"text": " So this is very important, basically."
		},
		{
			"timestamps": {
				"from": "00:28:55,000",
				"to": "00:28:58,000"
			},
			"offsets": {
				"from": 1735000,
				"to": 1738000
			},
			"text": " At this point when we have the pairings and that's why we really need them,"
		},
		{
			"timestamps": {
				"from": "00:28:58,000",
				"to": "00:29:00,000"
			},
			"offsets": {
				"from": 1738000,
				"to": 1740000
			},
			"text": " we can do one multiplication."
		},
		{
			"timestamps": {
				"from": "00:29:00,000",
				"to": "00:29:04,000"
			},
			"offsets": {
				"from": 1740000,
				"to": 1744000
			},
			"text": " We can only do one because afterwards we get this target group element"
		},
		{
			"timestamps": {
				"from": "00:29:04,000",
				"to": "00:29:07,000"
			},
			"offsets": {
				"from": 1744000,
				"to": 1747000
			},
			"text": " and that we can't really do anything with."
		},
		{
			"timestamps": {
				"from": "00:29:07,000",
				"to": "00:29:15,000"
			},
			"offsets": {
				"from": 1747000,
				"to": 1755000
			},
			"text": " But it turns out that this is actually enough to do a lot of very useful stuff in elliptic curves."
		},
		{
			"timestamps": {
				"from": "00:29:15,000",
				"to": "00:29:21,000"
			},
			"offsets": {
				"from": 1755000,
				"to": 1761000
			},
			"text": " Okay, now let's assume that we have two polynomials, F and G."
		},
		{
			"timestamps": {
				"from": "00:29:21,000",
				"to": "00:29:27,000"
			},
			"offsets": {
				"from": 1761000,
				"to": 1767000
			},
			"text": " And we commit to those polynomials, but we commit to F and G1"
		},
		{
			"timestamps": {
				"from": "00:29:27,000",
				"to": "00:29:31,000"
			},
			"offsets": {
				"from": 1767000,
				"to": 1771000
			},
			"text": " and G and G2 in the different groups."
		},
		{
			"timestamps": {
				"from": "00:29:31,000",
				"to": "00:29:38,000"
			},
			"offsets": {
				"from": 1771000,
				"to": 1778000
			},
			"text": " Then basically the pairing actually lets us compute this,"
		},
		{
			"timestamps": {
				"from": "00:29:38,000",
				"to": "00:29:43,000"
			},
			"offsets": {
				"from": 1778000,
				"to": 1783000
			},
			"text": " like the product of these two commitments in the target group."
		},
		{
			"timestamps": {
				"from": "00:29:43,000",
				"to": "00:29:48,000"
			},
			"offsets": {
				"from": 1783000,
				"to": 1788000
			},
			"text": " So basically in this really cool polynomial hash that we have defined,"
		},
		{
			"timestamps": {
				"from": "00:29:48,000",
				"to": "00:29:54,000"
			},
			"offsets": {
				"from": 1788000,
				"to": 1794000
			},
			"text": " we can now, if we commit to them in the right groups,"
		},
		{
			"timestamps": {
				"from": "00:29:54,000",
				"to": "00:29:59,000"
			},
			"offsets": {
				"from": 1794000,
				"to": 1799000
			},
			"text": " we can now multiply two polynomials that are committed in this way."
		},
		{
			"timestamps": {
				"from": "00:29:59,000",
				"to": "00:30:04,000"
			},
			"offsets": {
				"from": 1799000,
				"to": 1804000
			},
			"text": " So we can multiply the polynomial without even knowing the polynomials themselves."
		},
		{
			"timestamps": {
				"from": "00:30:04,000",
				"to": "00:30:08,000"
			},
			"offsets": {
				"from": 1804000,
				"to": 1808000
			},
			"text": " Okay, cool."
		},
		{
			"timestamps": {
				"from": "00:30:08,000",
				"to": "00:30:13,000"
			},
			"offsets": {
				"from": 1808000,
				"to": 1813000
			},
			"text": " Okay, so we will need to introduce one last missing piece"
		},
		{
			"timestamps": {
				"from": "00:30:13,000",
				"to": "00:30:18,000"
			},
			"offsets": {
				"from": 1813000,
				"to": 1818000
			},
			"text": " in order to fully come to how KZG commitments work"
		},
		{
			"timestamps": {
				"from": "00:30:18,000",
				"to": "00:30:22,000"
			},
			"offsets": {
				"from": 1818000,
				"to": 1822000
			},
			"text": " and how we can construct proofs and that is quotient of polynomials."
		},
		{
			"timestamps": {
				"from": "00:30:22,000",
				"to": "00:30:29,000"
			},
			"offsets": {
				"from": 1822000,
				"to": 1829000
			},
			"text": " Okay, so let's say we have a polynomial, F of X,"
		},
		{
			"timestamps": {
				"from": "00:30:29,000",
				"to": "00:30:33,000"
			},
			"offsets": {
				"from": 1829000,
				"to": 1833000
			},
			"text": " and we have two field elements Y and Z."
		},
		{
			"timestamps": {
				"from": "00:30:33,000",
				"to": "00:30:38,000"
			},
			"offsets": {
				"from": 1833000,
				"to": 1838000
			},
			"text": " And then we can compute this quotient, Q of X."
		},
		{
			"timestamps": {
				"from": "00:30:38,000",
				"to": "00:30:40,000"
			},
			"offsets": {
				"from": 1838000,
				"to": 1840000
			},
			"text": " This is a rational function, right?"
		},
		{
			"timestamps": {
				"from": "00:30:40,000",
				"to": "00:30:45,000"
			},
			"offsets": {
				"from": 1840000,
				"to": 1845000
			},
			"text": " So like a polynomial divided by a polynomial is in general a rational function."
		},
		{
			"timestamps": {
				"from": "00:30:45,000",
				"to": "00:30:49,000"
			},
			"offsets": {
				"from": 1845000,
				"to": 1849000
			},
			"text": " So you can just see this as like a formal expression."
		},
		{
			"timestamps": {
				"from": "00:30:49,000",
				"to": "00:30:52,000"
			},
			"offsets": {
				"from": 1849000,
				"to": 1852000
			},
			"text": " But sometimes this quotient is exact."
		},
		{
			"timestamps": {
				"from": "00:30:52,000",
				"to": "00:30:57,000"
			},
			"offsets": {
				"from": 1852000,
				"to": 1857000
			},
			"text": " So sometimes like this quotient will actually result in another polynomial."
		},
		{
			"timestamps": {
				"from": "00:30:57,000",
				"to": "00:31:01,000"
			},
			"offsets": {
				"from": 1857000,
				"to": 1861000
			},
			"text": " And basically there's a theorem that's called the factor theorem."
		},
		{
			"timestamps": {
				"from": "00:31:01,000",
				"to": "00:31:03,000"
			},
			"offsets": {
				"from": 1861000,
				"to": 1863000
			},
			"text": " It's relatively elementary math."
		},
		{
			"timestamps": {
				"from": "00:31:03,000",
				"to": "00:31:06,000"
			},
			"offsets": {
				"from": 1863000,
				"to": 1866000
			},
			"text": " You've probably learned it in school at some point without calling it that."
		},
		{
			"timestamps": {
				"from": "00:31:06,000",
				"to": "00:31:13,000"
			},
			"offsets": {
				"from": 1866000,
				"to": 1873000
			},
			"text": " And that basically says that this is a polynomial, just quotient,"
		},
		{
			"timestamps": {
				"from": "00:31:13,000",
				"to": "00:31:18,000"
			},
			"offsets": {
				"from": 1873000,
				"to": 1878000
			},
			"text": " exactly if F of Z equals Y."
		},
		{
			"timestamps": {
				"from": "00:31:18,000",
				"to": "00:31:27,000"
			},
			"offsets": {
				"from": 1878000,
				"to": 1887000
			},
			"text": " And I mean you can kind of see like that in one direction because if of F of X equals Y,"
		},
		{
			"timestamps": {
				"from": "00:31:27,000",
				"to": "00:31:35,000"
			},
			"offsets": {
				"from": 1887000,
				"to": 1895000
			},
			"text": " then if you set X equals Y, you get a zero here."
		},
		{
			"timestamps": {
				"from": "00:31:35,000",
				"to": "00:31:38,000"
			},
			"offsets": {
				"from": 1895000,
				"to": 1898000
			},
			"text": " F of Z here, and you get a zero here."
		},
		{
			"timestamps": {
				"from": "00:31:38,000",
				"to": "00:31:45,000"
			},
			"offsets": {
				"from": 1898000,
				"to": 1905000
			},
			"text": " So like zero by zero, that can only like that."
		},
		{
			"timestamps": {
				"from": "00:31:45,000",
				"to": "00:31:47,000"
			},
			"offsets": {
				"from": 1905000,
				"to": 1907000
			},
			"text": " Yeah, so sorry."
		},
		{
			"timestamps": {
				"from": "00:31:47,000",
				"to": "00:31:52,000"
			},
			"offsets": {
				"from": 1907000,
				"to": 1912000
			},
			"text": " If the quotient is zero at Z, that can only work if this is also zero at that."
		},
		{
			"timestamps": {
				"from": "00:31:52,000",
				"to": "00:31:56,000"
			},
			"offsets": {
				"from": 1912000,
				"to": 1916000
			},
			"text": " So it can only really work if this is correct."
		},
		{
			"timestamps": {
				"from": "00:31:56,000",
				"to": "00:32:02,000"
			},
			"offsets": {
				"from": 1916000,
				"to": 1922000
			},
			"text": " But the other direction is slightly more complicated."
		},
		{
			"timestamps": {
				"from": "00:32:02,000",
				"to": "00:32:09,000"
			},
			"offsets": {
				"from": 1922000,
				"to": 1929000
			},
			"text": " So if we restate this, basically we get the fact that we get the polynomial that fulfills the equation,"
		},
		{
			"timestamps": {
				"from": "00:32:09,000",
				"to": "00:32:10,000"
			},
			"offsets": {
				"from": 1929000,
				"to": 1930000
			},
			"text": " this equation."
		},
		{
			"timestamps": {
				"from": "00:32:10,000",
				"to": "00:32:17,000"
			},
			"offsets": {
				"from": 1930000,
				"to": 1937000
			},
			"text": " So we just put the X minus C on the other side, Q of X times X minus C equals F of X minus Y,"
		},
		{
			"timestamps": {
				"from": "00:32:17,000",
				"to": "00:32:26,000"
			},
			"offsets": {
				"from": 1937000,
				"to": 1946000
			},
			"text": " if and only F of Z equals Y."
		},
		{
			"timestamps": {
				"from": "00:32:26,000",
				"to": "00:32:27,000"
			},
			"offsets": {
				"from": 1946000,
				"to": 1947000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:32:27,000",
				"to": "00:32:31,000"
			},
			"offsets": {
				"from": 1947000,
				"to": 1951000
			},
			"text": " And now we get to how the KZG proves work."
		},
		{
			"timestamps": {
				"from": "00:32:31,000",
				"to": "00:32:37,000"
			},
			"offsets": {
				"from": 1951000,
				"to": 1957000
			},
			"text": " So if a prover wants to prove that F of Z equals Y,"
		},
		{
			"timestamps": {
				"from": "00:32:37,000",
				"to": "00:32:42,000"
			},
			"offsets": {
				"from": 1957000,
				"to": 1962000
			},
			"text": " they compute this quotient Q of X, which is F of X minus Y divided by X minus C,"
		},
		{
			"timestamps": {
				"from": "00:32:42,000",
				"to": "00:32:52,000"
			},
			"offsets": {
				"from": 1962000,
				"to": 1972000
			},
			"text": " and send the proof pi, which is Q of S, so the commitment to the polynomial Q."
		},
		{
			"timestamps": {
				"from": "00:32:52,000",
				"to": "00:33:02,000"
			},
			"offsets": {
				"from": 1972000,
				"to": 1982000
			},
			"text": " And in order to verify this, what the verifier will do is they'll take this quotient,"
		},
		{
			"timestamps": {
				"from": "00:33:02,000",
				"to": "00:33:08,000"
			},
			"offsets": {
				"from": 1982000,
				"to": 1988000
			},
			"text": " and they will multiply it by the commitment to S minus Z."
		},
		{
			"timestamps": {
				"from": "00:33:08,000",
				"to": "00:33:14,000"
			},
			"offsets": {
				"from": 1988000,
				"to": 1994000
			},
			"text": " And check that this is the same as original polynomial, the commitment to that, minus Y."
		},
		{
			"timestamps": {
				"from": "00:33:14,000",
				"to": "00:33:20,000"
			},
			"offsets": {
				"from": 1994000,
				"to": 2000000
			},
			"text": " And so this is unfortunate, very readable on this background."
		},
		{
			"timestamps": {
				"from": "00:33:20,000",
				"to": "00:33:26,000"
			},
			"offsets": {
				"from": 2000000,
				"to": 2006000
			},
			"text": " Because if you write it out in this pairing group, then you get on the right hand side,"
		},
		{
			"timestamps": {
				"from": "00:33:26,000",
				"to": "00:33:34,000"
			},
			"offsets": {
				"from": 2006000,
				"to": 2014000
			},
			"text": " Q of S times S minus Z in the target group equals F of S minus Y."
		},
		{
			"timestamps": {
				"from": "00:33:34,000",
				"to": "00:33:37,000"
			},
			"offsets": {
				"from": 2014000,
				"to": 2017000
			},
			"text": " And this is the same as the second equation."
		},
		{
			"timestamps": {
				"from": "00:33:37,000",
				"to": "00:33:43,000"
			},
			"offsets": {
				"from": 2017000,
				"to": 2023000
			},
			"text": " So the cool thing is we can verify this equation because we are able to multiply"
		},
		{
			"timestamps": {
				"from": "00:33:43,000",
				"to": "00:33:48,000"
			},
			"offsets": {
				"from": 2023000,
				"to": 2028000
			},
			"text": " two polynomial commitments inside using the pairing."
		},
		{
			"timestamps": {
				"from": "00:33:48,000",
				"to": "00:33:56,000"
			},
			"offsets": {
				"from": 2028000,
				"to": 2036000
			},
			"text": " And this way we can verify that the quotient was actually computed correctly."
		},
		{
			"timestamps": {
				"from": "00:33:56,000",
				"to": "00:33:58,000"
			},
			"offsets": {
				"from": 2036000,
				"to": 2038000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:33:58,000",
				"to": "00:33:59,000"
			},
			"offsets": {
				"from": 2038000,
				"to": 2039000
			},
			"text": " Yeah."
		},
		{
			"timestamps": {
				"from": "00:33:59,000",
				"to": "00:34:03,000"
			},
			"offsets": {
				"from": 2039000,
				"to": 2043000
			},
			"text": " And that is basically, that is how KZG commitments work."
		},
		{
			"timestamps": {
				"from": "00:34:03,000",
				"to": "00:34:11,000"
			},
			"offsets": {
				"from": 2043000,
				"to": 2051000
			},
			"text": " So the idea is just, if you can compute this quotient,"
		},
		{
			"timestamps": {
				"from": "00:34:11,000",
				"to": "00:34:14,000"
			},
			"offsets": {
				"from": 2051000,
				"to": 2054000
			},
			"text": " then you'll be able to find something that fulfills this equation."
		},
		{
			"timestamps": {
				"from": "00:34:14,000",
				"to": "00:34:21,000"
			},
			"offsets": {
				"from": 2054000,
				"to": 2061000
			},
			"text": " And using the factor theorem that we mentioned previously, if F of Z is not Y,"
		},
		{
			"timestamps": {
				"from": "00:34:21,000",
				"to": "00:34:23,000"
			},
			"offsets": {
				"from": 2061000,
				"to": 2063000
			},
			"text": " then you cannot compute this."
		},
		{
			"timestamps": {
				"from": "00:34:23,000",
				"to": "00:34:24,000"
			},
			"offsets": {
				"from": 2063000,
				"to": 2064000
			},
			"text": " It doesn't exist."
		},
		{
			"timestamps": {
				"from": "00:34:24,000",
				"to": "00:34:28,000"
			},
			"offsets": {
				"from": 2064000,
				"to": 2068000
			},
			"text": " It's not a polynomial, and we can only commit to polynomials."
		},
		{
			"timestamps": {
				"from": "00:34:28,000",
				"to": "00:34:32,000"
			},
			"offsets": {
				"from": 2068000,
				"to": 2072000
			},
			"text": " So yeah, this is the recap on the KZG commitment."
		},
		{
			"timestamps": {
				"from": "00:34:32,000",
				"to": "00:34:37,000"
			},
			"offsets": {
				"from": 2072000,
				"to": 2077000
			},
			"text": " We can commit to any polynomial using a single element in G1."
		},
		{
			"timestamps": {
				"from": "00:34:37,000",
				"to": "00:34:48,000"
			},
			"offsets": {
				"from": 2077000,
				"to": 2088000
			},
			"text": " And this is just the evaluation of the polynomial at the secret point S inside the black box."
		},
		{
			"timestamps": {
				"from": "00:34:48,000",
				"to": "00:34:55,000"
			},
			"offsets": {
				"from": 2088000,
				"to": 2095000
			},
			"text": " We can open the commitment at any point, so we can compute F of Z."
		},
		{
			"timestamps": {
				"from": "00:34:55,000",
				"to": "00:35:04,000"
			},
			"offsets": {
				"from": 2095000,
				"to": 2104000
			},
			"text": " And by computing the quotient Q of X, we can compute this proof, which is Q of S in the black box."
		},
		{
			"timestamps": {
				"from": "00:35:04,000",
				"to": "00:35:08,000"
			},
			"offsets": {
				"from": 2104000,
				"to": 2108000
			},
			"text": " And in order to verify that proof, we use this pairing equation."
		},
		{
			"timestamps": {
				"from": "00:35:08,000",
				"to": "00:35:21,000"
			},
			"offsets": {
				"from": 2108000,
				"to": 2121000
			},
			"text": " And that shows the verifier that this evaluation is correct."
		},
		{
			"timestamps": {
				"from": "00:35:21,000",
				"to": "00:35:22,000"
			},
			"offsets": {
				"from": 2121000,
				"to": 2122000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:35:22,000",
				"to": "00:35:27,000"
			},
			"offsets": {
				"from": 2122000,
				"to": 2127000
			},
			"text": " So that is KZG, how KZG commitments work."
		},
		{
			"timestamps": {
				"from": "00:35:27,000",
				"to": "00:35:35,000"
			},
			"offsets": {
				"from": 2127000,
				"to": 2135000
			},
			"text": " Now, I want to do something slightly more, which is a technique that we use quite a lot."
		},
		{
			"timestamps": {
				"from": "00:35:35,000",
				"to": "00:35:38,000"
			},
			"offsets": {
				"from": 2135000,
				"to": 2138000
			},
			"text": " We have even using it in AAP4844."
		},
		{
			"timestamps": {
				"from": "00:35:38,000",
				"to": "00:35:47,000"
			},
			"offsets": {
				"from": 2138000,
				"to": 2147000
			},
			"text": " And so I want to give a quick introduction into how it works, which is with the random evaluation trick."
		},
		{
			"timestamps": {
				"from": "00:35:47,000",
				"to": "00:35:49,000"
			},
			"offsets": {
				"from": 2147000,
				"to": 2149000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:35:49,000",
				"to": "00:36:01,000"
			},
			"offsets": {
				"from": 2149000,
				"to": 2161000
			},
			"text": " So basically, let's recall that KZG commitments are nothing but evaluating a polynomial F at a secret point S inside this elliptic curve black box."
		},
		{
			"timestamps": {
				"from": "00:36:01,000",
				"to": "00:36:05,000"
			},
			"offsets": {
				"from": 2161000,
				"to": 2165000
			},
			"text": " And so in a way, this is already like a random evaluation."
		},
		{
			"timestamps": {
				"from": "00:36:05,000",
				"to": "00:36:10,000"
			},
			"offsets": {
				"from": 2165000,
				"to": 2170000
			},
			"text": " Like, but basically what we've done is we've identified this polynomial using a random evaluation,"
		},
		{
			"timestamps": {
				"from": "00:36:10,000",
				"to": "00:36:18,000"
			},
			"offsets": {
				"from": 2170000,
				"to": 2178000
			},
			"text": " and we kind of, we somehow found that this is good enough to hash a polynomial in a way that it's valuable."
		},
		{
			"timestamps": {
				"from": "00:36:18,000",
				"to": "00:36:21,000"
			},
			"offsets": {
				"from": 2178000,
				"to": 2181000
			},
			"text": " It's very difficult to create collision."
		},
		{
			"timestamps": {
				"from": "00:36:21,000",
				"to": "00:36:29,000"
			},
			"offsets": {
				"from": 2181000,
				"to": 2189000
			},
			"text": " And more generally, this random evaluation trick can be used to verify polynomial identities."
		},
		{
			"timestamps": {
				"from": "00:36:29,000",
				"to": "00:36:33,000"
			},
			"offsets": {
				"from": 2189000,
				"to": 2193000
			},
			"text": " And the reason for that is the Schwadst simple lemma."
		},
		{
			"timestamps": {
				"from": "00:36:33,000",
				"to": "00:36:38,000"
			},
			"offsets": {
				"from": 2193000,
				"to": 2198000
			},
			"text": " And I will just formulate it as a more general one, but let's say what it says in one dimension."
		},
		{
			"timestamps": {
				"from": "00:36:38,000",
				"to": "00:36:44,000"
			},
			"offsets": {
				"from": 2198000,
				"to": 2204000
			},
			"text": " So let's have a degree, a polynomial of degree less than n."
		},
		{
			"timestamps": {
				"from": "00:36:44,000",
				"to": "00:36:46,000"
			},
			"offsets": {
				"from": 2204000,
				"to": 2206000
			},
			"text": " That is not identical to zero."
		},
		{
			"timestamps": {
				"from": "00:36:46,000",
				"to": "00:36:49,000"
			},
			"offsets": {
				"from": 2206000,
				"to": 2209000
			},
			"text": " So there's one particular polynomial that is zero everywhere."
		},
		{
			"timestamps": {
				"from": "00:36:49,000",
				"to": "00:36:51,000"
			},
			"offsets": {
				"from": 2209000,
				"to": 2211000
			},
			"text": " That's just like all zeros, right?"
		},
		{
			"timestamps": {
				"from": "00:36:51,000",
				"to": "00:36:53,000"
			},
			"offsets": {
				"from": 2211000,
				"to": 2213000
			},
			"text": " That's a very special polynomial."
		},
		{
			"timestamps": {
				"from": "00:36:53,000",
				"to": "00:36:55,000"
			},
			"offsets": {
				"from": 2213000,
				"to": 2215000
			},
			"text": " So let's say it's not that."
		},
		{
			"timestamps": {
				"from": "00:36:55,000",
				"to": "00:37:01,000"
			},
			"offsets": {
				"from": 2215000,
				"to": 2221000
			},
			"text": " Now let's take a random point Z in FP."
		},
		{
			"timestamps": {
				"from": "00:37:01,000",
				"to": "00:37:05,000"
			},
			"offsets": {
				"from": 2221000,
				"to": 2225000
			},
			"text": " Then the probability that F of Z is zero is at most n over P."
		},
		{
			"timestamps": {
				"from": "00:37:05,000",
				"to": "00:37:09,000"
			},
			"offsets": {
				"from": 2225000,
				"to": 2229000
			},
			"text": " And that's because it can have at most n zeros."
		},
		{
			"timestamps": {
				"from": "00:37:09,000",
				"to": "00:37:14,000"
			},
			"offsets": {
				"from": 2229000,
				"to": 2234000
			},
			"text": " And so this is a very useful thing because our P is very, very large."
		},
		{
			"timestamps": {
				"from": "00:37:14,000",
				"to": "00:37:18,000"
			},
			"offsets": {
				"from": 2234000,
				"to": 2238000
			},
			"text": " And our degree is relatively small compared to it in our case."
		},
		{
			"timestamps": {
				"from": "00:37:18,000",
				"to": "00:37:23,000"
			},
			"offsets": {
				"from": 2238000,
				"to": 2243000
			},
			"text": " So for example, in 4B last 1231, P is 255 bits."
		},
		{
			"timestamps": {
				"from": "00:37:23,000",
				"to": "00:37:27,000"
			},
			"offsets": {
				"from": 2243000,
				"to": 2247000
			},
			"text": " Say we commit to a polynomial of degree to the 12."
		},
		{
			"timestamps": {
				"from": "00:37:27,000",
				"to": "00:37:32,000"
			},
			"offsets": {
				"from": 2247000,
				"to": 2252000
			},
			"text": " Then this probability is something like 2 to the minus 240."
		},
		{
			"timestamps": {
				"from": "00:37:32,000",
				"to": "00:37:39,000"
			},
			"offsets": {
				"from": 2252000,
				"to": 2259000
			},
			"text": " So that gets a very, very small probability."
		},
		{
			"timestamps": {
				"from": "00:37:39,000",
				"to": "00:37:45,000"
			},
			"offsets": {
				"from": 2259000,
				"to": 2265000
			},
			"text": " And so here is the first way in which we can use this."
		},
		{
			"timestamps": {
				"from": "00:37:45,000",
				"to": "00:37:50,000"
			},
			"offsets": {
				"from": 2265000,
				"to": 2270000
			},
			"text": " So we have these transaction blobs that will be fine for 4844."
		},
		{
			"timestamps": {
				"from": "00:37:50,000",
				"to": "00:37:58,000"
			},
			"offsets": {
				"from": 2270000,
				"to": 2278000
			},
			"text": " So it's like they are commitments to polynomials with 4,000 degree 4,095."
		},
		{
			"timestamps": {
				"from": "00:37:58,000",
				"to": "00:38:01,000"
			},
			"offsets": {
				"from": 2278000,
				"to": 2281000
			},
			"text": " So in total 4,096 points."
		},
		{
			"timestamps": {
				"from": "00:38:01,000",
				"to": "00:38:05,000"
			},
			"offsets": {
				"from": 2281000,
				"to": 2285000
			},
			"text": " And computing such a commitment is not very expensive,"
		},
		{
			"timestamps": {
				"from": "00:38:05,000",
				"to": "00:38:06,000"
			},
			"offsets": {
				"from": 2285000,
				"to": 2286000
			},
			"text": " but it is expensive."
		},
		{
			"timestamps": {
				"from": "00:38:06,000",
				"to": "00:38:08,000"
			},
			"offsets": {
				"from": 2286000,
				"to": 2288000
			},
			"text": " It's like 50 milliseconds to do this."
		},
		{
			"timestamps": {
				"from": "00:38:08,000",
				"to": "00:38:12,000"
			},
			"offsets": {
				"from": 2288000,
				"to": 2292000
			},
			"text": " But verifying one KZG proof is quite a lot cheaper."
		},
		{
			"timestamps": {
				"from": "00:38:12,000",
				"to": "00:38:14,000"
			},
			"offsets": {
				"from": 2292000,
				"to": 2294000
			},
			"text": " It only costs about 2 milliseconds."
		},
		{
			"timestamps": {
				"from": "00:38:14,000",
				"to": "00:38:18,000"
			},
			"offsets": {
				"from": 2294000,
				"to": 2298000
			},
			"text": " So we can use this to our advantage."
		},
		{
			"timestamps": {
				"from": "00:38:18,000",
				"to": "00:38:21,000"
			},
			"offsets": {
				"from": 2298000,
				"to": 2301000
			},
			"text": " And so the idea is this."
		},
		{
			"timestamps": {
				"from": "00:38:21,000",
				"to": "00:38:28,000"
			},
			"offsets": {
				"from": 2301000,
				"to": 2308000
			},
			"text": " We take our commitment to the polynomial C and we take the polynomial F itself."
		},
		{
			"timestamps": {
				"from": "00:38:28,000",
				"to": "00:38:32,000"
			},
			"offsets": {
				"from": 2308000,
				"to": 2312000
			},
			"text": " So what we want to verify is that we have the polynomial."
		},
		{
			"timestamps": {
				"from": "00:38:32,000",
				"to": "00:38:33,000"
			},
			"offsets": {
				"from": 2312000,
				"to": 2313000
			},
			"text": " It's given to us."
		},
		{
			"timestamps": {
				"from": "00:38:33,000",
				"to": "00:38:36,000"
			},
			"offsets": {
				"from": 2313000,
				"to": 2316000
			},
			"text": " In this case we have all the data and we have the commitment."
		},
		{
			"timestamps": {
				"from": "00:38:36,000",
				"to": "00:38:40,000"
			},
			"offsets": {
				"from": 2316000,
				"to": 2320000
			},
			"text": " And in a naive way we can just compute the commitment from the polynomial."
		},
		{
			"timestamps": {
				"from": "00:38:40,000",
				"to": "00:38:41,000"
			},
			"offsets": {
				"from": 2320000,
				"to": 2321000
			},
			"text": " But that's expensive."
		},
		{
			"timestamps": {
				"from": "00:38:41,000",
				"to": "00:38:45,000"
			},
			"offsets": {
				"from": 2321000,
				"to": 2325000
			},
			"text": " Okay, how can we do it cheaper?"
		},
		{
			"timestamps": {
				"from": "00:38:45,000",
				"to": "00:38:49,000"
			},
			"offsets": {
				"from": 2325000,
				"to": 2329000
			},
			"text": " We do, we compute a random point."
		},
		{
			"timestamps": {
				"from": "00:38:49,000",
				"to": "00:38:52,000"
			},
			"offsets": {
				"from": 2329000,
				"to": 2332000
			},
			"text": " And one way to get a random point is actually very cool technique."
		},
		{
			"timestamps": {
				"from": "00:38:52,000",
				"to": "00:38:54,000"
			},
			"offsets": {
				"from": 2332000,
				"to": 2334000
			},
			"text": " It's called Fia chamier."
		},
		{
			"timestamps": {
				"from": "00:38:54,000",
				"to": "00:38:56,000"
			},
			"offsets": {
				"from": 2334000,
				"to": 2336000
			},
			"text": " We take all our inputs."
		},
		{
			"timestamps": {
				"from": "00:38:56,000",
				"to": "00:39:01,000"
			},
			"offsets": {
				"from": 2336000,
				"to": 2341000
			},
			"text": " So we compute that as the hash of the commitment and the polynomial."
		},
		{
			"timestamps": {
				"from": "00:39:01,000",
				"to": "00:39:03,000"
			},
			"offsets": {
				"from": 2341000,
				"to": 2343000
			},
			"text": " Why is that kind of random?"
		},
		{
			"timestamps": {
				"from": "00:39:03,000",
				"to": "00:39:09,000"
			},
			"offsets": {
				"from": 2343000,
				"to": 2349000
			},
			"text": " If an attacker tries to craft something, if they try to adversarially compute either C or F,"
		},
		{
			"timestamps": {
				"from": "00:39:09,000",
				"to": "00:39:11,000"
			},
			"offsets": {
				"from": 2349000,
				"to": 2351000
			},
			"text": " it will always change the point Z."
		},
		{
			"timestamps": {
				"from": "00:39:11,000",
				"to": "00:39:17,000"
			},
			"offsets": {
				"from": 2351000,
				"to": 2357000
			},
			"text": " So it's very hard for them to find some like to craft them in a way that breaks our construction."
		},
		{
			"timestamps": {
				"from": "00:39:17,000",
				"to": "00:39:25,000"
			},
			"offsets": {
				"from": 2357000,
				"to": 2365000
			},
			"text": " So basically this is a common technique and cryptography to get something random that the attacker cannot control."
		},
		{
			"timestamps": {
				"from": "00:39:25,000",
				"to": "00:39:28,000"
			},
			"offsets": {
				"from": 2365000,
				"to": 2368000
			},
			"text": " And so we evaluate this polynomial."
		},
		{
			"timestamps": {
				"from": "00:39:28,000",
				"to": "00:39:29,000"
			},
			"offsets": {
				"from": 2368000,
				"to": 2369000
			},
			"text": " Why?"
		},
		{
			"timestamps": {
				"from": "00:39:29,000",
				"to": "00:39:37,000"
			},
			"offsets": {
				"from": 2369000,
				"to": 2377000
			},
			"text": " At this random point that we've taken and then we compute a kzg proof that F of Z equals Y."
		},
		{
			"timestamps": {
				"from": "00:39:37,000",
				"to": "00:39:44,000"
			},
			"offsets": {
				"from": 2377000,
				"to": 2384000
			},
			"text": " And basically that, then what we'll do is we just add this proof to our transaction block wrapper,"
		},
		{
			"timestamps": {
				"from": "00:39:44,000",
				"to": "00:39:47,000"
			},
			"offsets": {
				"from": 2384000,
				"to": 2387000
			},
			"text": " which is the way we're sending transactions."
		},
		{
			"timestamps": {
				"from": "00:39:47,000",
				"to": "00:39:54,000"
			},
			"offsets": {
				"from": 2387000,
				"to": 2394000
			},
			"text": " And then like to verify this, you compute Z as this hash."
		},
		{
			"timestamps": {
				"from": "00:39:54,000",
				"to": "00:40:02,000"
			},
			"offsets": {
				"from": 2394000,
				"to": 2402000
			},
			"text": " You also compute F of Z, which you can do because you have the data for that, and you take the proof pi, the kzg proof."
		},
		{
			"timestamps": {
				"from": "00:40:02,000",
				"to": "00:40:03,000"
			},
			"offsets": {
				"from": 2402000,
				"to": 2403000
			},
			"text": " And that's done."
		},
		{
			"timestamps": {
				"from": "00:40:03,000",
				"to": "00:40:06,000"
			},
			"offsets": {
				"from": 2403000,
				"to": 2406000
			},
			"text": " And that's much, much cheaper than computing the commitment."
		},
		{
			"timestamps": {
				"from": "00:40:06,000",
				"to": "00:40:15,000"
			},
			"offsets": {
				"from": 2406000,
				"to": 2415000
			},
			"text": " So that's one way in which we can use random evaluations to like save us a lot of work and making things more efficient."
		},
		{
			"timestamps": {
				"from": "00:40:15,000",
				"to": "00:40:17,000"
			},
			"offsets": {
				"from": 2415000,
				"to": 2417000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:40:17,000",
				"to": "00:40:22,000"
			},
			"offsets": {
				"from": 2417000,
				"to": 2422000
			},
			"text": " So here's another way in which we can use this random evaluation technique."
		},
		{
			"timestamps": {
				"from": "00:40:22,000",
				"to": "00:40:30,000"
			},
			"offsets": {
				"from": 2422000,
				"to": 2430000
			},
			"text": " And so ZK rollups, they use many different proof schemes."
		},
		{
			"timestamps": {
				"from": "00:40:30,000",
				"to": "00:40:41,000"
			},
			"offsets": {
				"from": 2430000,
				"to": 2441000
			},
			"text": " And so only a handful, I don't know if actually there any right now, will use natively kzg commitments over BLS 12381."
		},
		{
			"timestamps": {
				"from": "00:40:41,000",
				"to": "00:40:51,000"
			},
			"offsets": {
				"from": 2441000,
				"to": 2451000
			},
			"text": " And so the question is like, how do all the other work makes efficient use of our blob commitments that we want to add with 4844 and then full charting."
		},
		{
			"timestamps": {
				"from": "00:40:51,000",
				"to": "00:41:00,000"
			},
			"offsets": {
				"from": 2451000,
				"to": 2460000
			},
			"text": " Because computing a kzg commitments inside a proof or computing pairings, that is pretty expensive."
		},
		{
			"timestamps": {
				"from": "00:41:00,000",
				"to": "00:41:05,000"
			},
			"offsets": {
				"from": 2460000,
				"to": 2465000
			},
			"text": " Like that's a very expensive operation in a zero-knowledge proof."
		},
		{
			"timestamps": {
				"from": "00:41:05,000",
				"to": "00:41:16,000"
			},
			"offsets": {
				"from": 2465000,
				"to": 2476000
			},
			"text": " And so what we do in this case is basically you have to, you commit to the data in a different way."
		},
		{
			"timestamps": {
				"from": "00:41:16,000",
				"to": "00:41:19,000"
			},
			"offsets": {
				"from": 2476000,
				"to": 2479000
			},
			"text": " So we have three different inputs."
		},
		{
			"timestamps": {
				"from": "00:41:19,000",
				"to": "00:41:23,000"
			},
			"offsets": {
				"from": 2479000,
				"to": 2483000
			},
			"text": " So we have our blob data, which is this function F itself."
		},
		{
			"timestamps": {
				"from": "00:41:23,000",
				"to": "00:41:27,000"
			},
			"offsets": {
				"from": 2483000,
				"to": 2487000
			},
			"text": " And we have two different type of commitments."
		},
		{
			"timestamps": {
				"from": "00:41:27,000",
				"to": "00:41:35,000"
			},
			"offsets": {
				"from": 2487000,
				"to": 2495000
			},
			"text": " Now we have C, which is our blob commitment, which is what we'll use inside Ethereum for 4844."
		},
		{
			"timestamps": {
				"from": "00:41:35,000",
				"to": "00:41:43,000"
			},
			"offsets": {
				"from": 2495000,
				"to": 2503000
			},
			"text": " And we have another way of committing to this data, which is using the ZK rollups native commitment."
		},
		{
			"timestamps": {
				"from": "00:41:43,000",
				"to": "00:41:52,000"
			},
			"offsets": {
				"from": 2503000,
				"to": 2512000
			},
			"text": " So they will in some way, it will also have some way of committing to data that works well for the as-you-know-ench proof scheme."
		},
		{
			"timestamps": {
				"from": "00:41:52,000",
				"to": "00:42:00,000"
			},
			"offsets": {
				"from": 2512000,
				"to": 2520000
			},
			"text": " And so in this case, what we'll do is we'll take Z as a hash of C and R, like these two different commitments."
		},
		{
			"timestamps": {
				"from": "00:42:00,000",
				"to": "00:42:05,000"
			},
			"offsets": {
				"from": 2520000,
				"to": 2525000
			},
			"text": " And we will compute Y again as F of Z."
		},
		{
			"timestamps": {
				"from": "00:42:05,000",
				"to": "00:42:12,000"
			},
			"offsets": {
				"from": 2525000,
				"to": 2532000
			},
			"text": " And we'll add Pi as a proof that F of Z equals 1."
		},
		{
			"timestamps": {
				"from": "00:42:12,000",
				"to": "00:42:22,000"
			},
			"offsets": {
				"from": 2532000,
				"to": 2542000
			},
			"text": " And we'll add this precompile that allows us in the Ethereum virtual machine to verify that the KZG proof Pi."
		},
		{
			"timestamps": {
				"from": "00:42:22,000",
				"to": "00:42:28,000"
			},
			"offsets": {
				"from": 2542000,
				"to": 2548000
			},
			"text": " So we will know that C is a correct commitment to F."
		},
		{
			"timestamps": {
				"from": "00:42:28,000",
				"to": "00:42:35,000"
			},
			"offsets": {
				"from": 2548000,
				"to": 2555000
			},
			"text": " And what we'll need to add is to add the proof that R is also a correct commitment."
		},
		{
			"timestamps": {
				"from": "00:42:35,000",
				"to": "00:42:41,000"
			},
			"offsets": {
				"from": 2555000,
				"to": 2561000
			},
			"text": " And the ZK rollup can do that inside the proof. So they will inside the proof."
		},
		{
			"timestamps": {
				"from": "00:42:41,000",
				"to": "00:42:47,000"
			},
			"offsets": {
				"from": 2561000,
				"to": 2567000
			},
			"text": " They also have to somehow get C and R as an input and hash them and compute that."
		},
		{
			"timestamps": {
				"from": "00:42:47,000",
				"to": "00:42:55,000"
			},
			"offsets": {
				"from": 2567000,
				"to": 2575000
			},
			"text": " And then they can evaluate. So they will also have F because the rollup wants to use the data."
		},
		{
			"timestamps": {
				"from": "00:42:55,000",
				"to": "00:42:57,000"
			},
			"offsets": {
				"from": 2575000,
				"to": 2577000
			},
			"text": " So F is completely available to them."
		},
		{
			"timestamps": {
				"from": "00:42:57,000",
				"to": "00:43:06,000"
			},
			"offsets": {
				"from": 2577000,
				"to": 2586000
			},
			"text": " And they just have to compute Y equals F of Z. And use some technique to verify that the F is the same as they are."
		},
		{
			"timestamps": {
				"from": "00:43:06,000",
				"to": "00:43:13,000"
			},
			"offsets": {
				"from": 2586000,
				"to": 2593000
			},
			"text": " But that's there are ways to make this easy. And then they can verify that they have the same data as was committed through C."
		},
		{
			"timestamps": {
				"from": "00:43:13,000",
				"to": "00:43:22,000"
			},
			"offsets": {
				"from": 2593000,
				"to": 2602000
			},
			"text": " So that will make it much easier to use these commitments in the K rollups."
		},
		{
			"timestamps": {
				"from": "00:43:22,000",
				"to": "00:43:23,000"
			},
			"offsets": {
				"from": 2602000,
				"to": 2603000
			},
			"text": " Cool."
		},
		{
			"timestamps": {
				"from": "00:43:23,000",
				"to": "00:43:31,000"
			},
			"offsets": {
				"from": 2603000,
				"to": 2611000
			},
			"text": " And yeah, I collected some resources if you want to read further on this."
		},
		{
			"timestamps": {
				"from": "00:43:31,000",
				"to": "00:43:37,000"
			},
			"offsets": {
				"from": 2611000,
				"to": 2617000
			},
			"text": " So Vitalik wrote a biolego opposed to elliptic curve pairings."
		},
		{
			"timestamps": {
				"from": "00:43:37,000",
				"to": "00:43:49,000"
			},
			"offsets": {
				"from": 2617000,
				"to": 2629000
			},
			"text": " I, because there was a lot of interest in that, I wrote some notes on how on this last part how to use KZ commitments in ZK rollups."
		},
		{
			"timestamps": {
				"from": "00:43:49,000",
				"to": "00:43:59,000"
			},
			"offsets": {
				"from": 2629000,
				"to": 2639000
			},
			"text": " For those who are like kind of skeptical and are like wondering, do we really need this like advanced cryptography and trusted set up and so on."
		},
		{
			"timestamps": {
				"from": "00:43:59,000",
				"to": "00:44:07,000"
			},
			"offsets": {
				"from": 2639000,
				"to": 2647000
			},
			"text": " Vitalik recently wrote a summary on like what what the difficulties are with alternatives to KZ commitments."
		},
		{
			"timestamps": {
				"from": "00:44:07,000",
				"to": "00:44:18,000"
			},
			"offsets": {
				"from": 2647000,
				"to": 2658000
			},
			"text": " And here this is if you want kind of it's very similar to this talk, but I wrote a blog post about KZ commitments."
		},
		{
			"timestamps": {
				"from": "00:44:18,000",
				"to": "00:44:24,000"
			},
			"offsets": {
				"from": 2658000,
				"to": 2664000
			},
			"text": " And then of course, if you want to dive deep there's the case, the original KZG paper."
		},
		{
			"timestamps": {
				"from": "00:44:24,000",
				"to": "00:44:28,000"
			},
			"offsets": {
				"from": 2664000,
				"to": 2668000
			},
			"text": " And if you scan this QR code, there will be all these links."
		},
		{
			"timestamps": {
				"from": "00:44:28,000",
				"to": "00:44:30,000"
			},
			"offsets": {
				"from": 2668000,
				"to": 2670000
			},
			"text": " Yep."
		},
		{
			"timestamps": {
				"from": "00:44:30,000",
				"to": "00:44:40,000"
			},
			"offsets": {
				"from": 2670000,
				"to": 2680000
			},
			"text": " So I guess we have the thing."
		},
		{
			"timestamps": {
				"from": "00:44:40,000",
				"to": "00:44:46,000"
			},
			"offsets": {
				"from": 2680000,
				"to": 2686000
			},
			"text": " I don't understand."
		},
		{
			"timestamps": {
				"from": "00:44:46,000",
				"to": "00:44:54,000"
			},
			"offsets": {
				"from": 2686000,
				"to": 2694000
			},
			"text": " Wait, where do you want to open multiple times?"
		},
		{
			"timestamps": {
				"from": "00:44:54,000",
				"to": "00:45:02,000"
			},
			"offsets": {
				"from": 2694000,
				"to": 2702000
			},
			"text": " Right."
		},
		{
			"timestamps": {
				"from": "00:45:02,000",
				"to": "00:45:10,000"
			},
			"offsets": {
				"from": 2702000,
				"to": 2710000
			},
			"text": " Each time you would have your separation."
		},
		{
			"timestamps": {
				"from": "00:45:10,000",
				"to": "00:45:14,000"
			},
			"offsets": {
				"from": 2710000,
				"to": 2714000
			},
			"text": " Are you talking about S or like the the the the trusted set up?"
		},
		{
			"timestamps": {
				"from": "00:45:14,000",
				"to": "00:45:15,000"
			},
			"offsets": {
				"from": 2714000,
				"to": 2715000
			},
			"text": " Yes."
		},
		{
			"timestamps": {
				"from": "00:45:15,000",
				"to": "00:45:35,000"
			},
			"offsets": {
				"from": 2715000,
				"to": 2735000
			},
			"text": " Yes."
		},
		{
			"timestamps": {
				"from": "00:45:35,000",
				"to": "00:45:36,000"
			},
			"offsets": {
				"from": 2735000,
				"to": 2736000
			},
			"text": " Right."
		},
		{
			"timestamps": {
				"from": "00:45:36,000",
				"to": "00:45:45,000"
			},
			"offsets": {
				"from": 2736000,
				"to": 2745000
			},
			"text": " But this is a cryptographic probability, right? We're talking about, I mean, that's why we're setting the security to two to the minus 128."
		},
		{
			"timestamps": {
				"from": "00:45:45,000",
				"to": "00:45:53,000"
			},
			"offsets": {
				"from": 2745000,
				"to": 2753000
			},
			"text": " So two, two hundred."
		},
		{
			"timestamps": {
				"from": "00:45:53,000",
				"to": "00:46:00,000"
			},
			"offsets": {
				"from": 2753000,
				"to": 2760000
			},
			"text": " Yeah."
		},
		{
			"timestamps": {
				"from": "00:46:00,000",
				"to": "00:46:06,000"
			},
			"offsets": {
				"from": 2760000,
				"to": 2766000
			},
			"text": " Yes."
		},
		{
			"timestamps": {
				"from": "00:46:06,000",
				"to": "00:46:26,000"
			},
			"offsets": {
				"from": 2766000,
				"to": 2786000
			},
			"text": " Yes. But so we're setting in cryptography, we're setting our security parameter already in the assumption that an attacker will do a lot of computation to try to break it like two to the 50 to the 60 or more computation power."
		},
		{
			"timestamps": {
				"from": "00:46:26,000",
				"to": "00:46:31,000"
			},
			"offsets": {
				"from": 2786000,
				"to": 2791000
			},
			"text": " This is much, much more than however we ever use it in the in the actual protocol."
		},
		{
			"timestamps": {
				"from": "00:46:31,000",
				"to": "00:46:41,000"
			},
			"offsets": {
				"from": 2791000,
				"to": 2801000
			},
			"text": " So like this is all already covered by the by the cryptographic construction."
		},
		{
			"timestamps": {
				"from": "00:46:41,000",
				"to": "00:46:53,000"
			},
			"offsets": {
				"from": 2801000,
				"to": 2813000
			},
			"text": " I mean, you can do it, but the random probabilities like the probability of randomly hitting that are extremely extremely low. Like if you, like, if you construct it so that the probability."
		},
		{
			"timestamps": {
				"from": "00:46:53,000",
				"to": "00:47:04,000"
			},
			"offsets": {
				"from": 2813000,
				"to": 2824000
			},
			"text": " So yeah, yeah, like randomly they are less than two to the minus two hundred or something like that. It's like solo. You cannot even like, yeah, think about it. Yeah."
		},
		{
			"timestamps": {
				"from": "00:47:04,000",
				"to": "00:47:09,000"
			},
			"offsets": {
				"from": 2824000,
				"to": 2829000
			},
			"text": " Very many questions using the variations."
		},
		{
			"timestamps": {
				"from": "00:47:09,000",
				"to": "00:47:15,000"
			},
			"offsets": {
				"from": 2829000,
				"to": 2835000
			},
			"text": " I wonder how many in which something like X to the B minus X from the zero, but not."
		},
		{
			"timestamps": {
				"from": "00:47:15,000",
				"to": "00:47:19,000"
			},
			"offsets": {
				"from": 2835000,
				"to": 2839000
			},
			"text": " X to the P minus X."
		},
		{
			"timestamps": {
				"from": "00:47:19,000",
				"to": "00:47:21,000"
			},
			"offsets": {
				"from": 2839000,
				"to": 2841000
			},
			"text": " Right."
		},
		{
			"timestamps": {
				"from": "00:47:21,000",
				"to": "00:47:34,000"
			},
			"offsets": {
				"from": 2841000,
				"to": 2854000
			},
			"text": " Yes. Yeah. No, you can't, but that's fine. Okay. So we are always because we're limiting the degree of our polynomials. Right. So our trust that set up will only go to a certain power, for example, to the power of 12."
		},
		{
			"timestamps": {
				"from": "00:47:34,000",
				"to": "00:47:39,000"
			},
			"offsets": {
				"from": 2854000,
				"to": 2859000
			},
			"text": " And inside that space, there's only zero polynomial. Yes."
		},
		{
			"timestamps": {
				"from": "00:47:39,000",
				"to": "00:47:50,000"
			},
			"offsets": {
				"from": 2859000,
				"to": 2870000
			},
			"text": " Yeah. Yeah. So like if you have no limits on the polynomial degrees, then it doesn't work. But we always have a limit."
		},
		{
			"timestamps": {
				"from": "00:47:50,000",
				"to": "00:47:51,000"
			},
			"offsets": {
				"from": 2870000,
				"to": 2871000
			},
			"text": " Yeah."
		},
		{
			"timestamps": {
				"from": "00:47:51,000",
				"to": "00:47:52,000"
			},
			"offsets": {
				"from": 2871000,
				"to": 2872000
			},
			"text": " Yeah."
		},
		{
			"timestamps": {
				"from": "00:47:52,000",
				"to": "00:47:53,000"
			},
			"offsets": {
				"from": 2872000,
				"to": 2873000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "00:47:53,000",
				"to": "00:47:54,000"
			},
			"offsets": {
				"from": 2873000,
				"to": 2874000
			},
			"text": " And."
		},
		{
			"timestamps": {
				"from": "00:47:54,000",
				"to": "00:47:55,000"
			},
			"offsets": {
				"from": 2874000,
				"to": 2875000
			},
			"text": " Thank you."
		},
		{
			"timestamps": {
				"from": "00:47:55,000",
				"to": "00:48:18,000"
			},
			"offsets": {
				"from": 2875000,
				"to": 2898000
			},
			"text": " Excellent. Excellent. Thank you, Dan Krad for the math. So, okay. So now we're going to go into the bit more like kind of like we're in the sky of math and we're kind of like toning it down into like the protocol stuff."
		},
		{
			"timestamps": {
				"from": "00:48:18,000",
				"to": "00:48:37,000"
			},
			"offsets": {
				"from": 2898000,
				"to": 2917000
			},
			"text": " So I'm going to start with a small like explanation of how all this math stuff going to our protocol and how like, you know, all the extra bandwidth of for it for for travels around and gets verified, then protest going to take it and tone it"
		},
		{
			"timestamps": {
				"from": "00:48:37,000",
				"to": "00:48:49,000"
			},
			"offsets": {
				"from": 2917000,
				"to": 2929000
			},
			"text": " down into more practical stuff like how the L2s are going to use the data. And then, and it's going to tone it even more down and basically explain how people pay for this data. So, okay."
		},
		{
			"timestamps": {
				"from": "00:48:49,000",
				"to": "00:49:06,000"
			},
			"offsets": {
				"from": 2929000,
				"to": 2946000
			},
			"text": " So basically, this is a graph that shows how like, optimism and L2, what is its costs. And you can see that like this blue stuff is the data fees, like how much money they are paying for the data."
		},
		{
			"timestamps": {
				"from": "00:49:06,000",
				"to": "00:49:27,000"
			},
			"offsets": {
				"from": 2946000,
				"to": 2967000
			},
			"text": " They put on chain and the other like white stuff is some other stuff, but you can see that the blue, the data is dominating all the costs. So basically what for eight for four is it's like a mechanism that drastically increases the amount of data people can post on chain."
		},
		{
			"timestamps": {
				"from": "00:49:27,000",
				"to": "00:49:52,000"
			},
			"offsets": {
				"from": 2967000,
				"to": 2992000
			},
			"text": " And this is all it is right. So, okay. So, basically, what we want to do, if we want to increase the amount of data. So, on this very simple picture on the left side, you can see our data, which we call blob, because it's a bunch of data that also corresponds to polynomial."
		},
		{
			"timestamps": {
				"from": "00:49:52,000",
				"to": "00:50:10,000"
			},
			"offsets": {
				"from": 2992000,
				"to": 3010000
			},
			"text": " And on the right hand side, you can see a small thing, a commitment that represents that data commits to that data. And the, like, rough idea is that, you know, commitment goes and change forever, whereas the blob is kind of like, you know, therefore a bit and then disappears."
		},
		{
			"timestamps": {
				"from": "00:50:10,000",
				"to": "00:50:21,000"
			},
			"offsets": {
				"from": 3010000,
				"to": 3021000
			},
			"text": " So, this is like the high level strategy of how we increase bandwidth. We commit to data. We keep the commitment forever, but the data is ephemeral in a way."
		},
		{
			"timestamps": {
				"from": "00:50:21,000",
				"to": "00:50:30,000"
			},
			"offsets": {
				"from": 3021000,
				"to": 3030000
			},
			"text": " Okay. So, let's talk a bit about what this data is, what these blocks are, how do polynomials enter this picture."
		},
		{
			"timestamps": {
				"from": "00:50:30,000",
				"to": "00:50:42,000"
			},
			"offsets": {
				"from": 3030000,
				"to": 3042000
			},
			"text": " So, okay, this is a polynomial. I think by now you're very familiar with it based on the last talk. The question is, like, how do we put data into this polynomial?"
		},
		{
			"timestamps": {
				"from": "00:50:42,000",
				"to": "00:50:54,000"
			},
			"offsets": {
				"from": 3042000,
				"to": 3054000
			},
			"text": " And, like, the basic idea is, you know, you have these coefficients, the A1, A2, A, whatever, and each, you can basically put data into this coefficient."
		},
		{
			"timestamps": {
				"from": "00:50:54,000",
				"to": "00:51:09,000"
			},
			"offsets": {
				"from": 3054000,
				"to": 3069000
			},
			"text": " So, you know, if you have some data, one, four, one, six, you can put it in the coefficient, so you make this little polynomial on the bottom, and that's like a very straightforward way to put data into polynomial."
		},
		{
			"timestamps": {
				"from": "00:51:09,000",
				"to": "00:51:26,000"
			},
			"offsets": {
				"from": 3069000,
				"to": 3086000
			},
			"text": " So, right, so let's now think of, like, what, like, in our case, let's see about these numbers, one, four, one, six, how they can resemble real data."
		},
		{
			"timestamps": {
				"from": "00:51:26,000",
				"to": "00:51:40,000"
			},
			"offsets": {
				"from": 3086000,
				"to": 3100000
			},
			"text": " So, in our case, the numbers are going to be finite fields, so they're going to be parts of the finite field, which is going to be like a number between zero and this insanely huge prime number."
		},
		{
			"timestamps": {
				"from": "00:51:40,000",
				"to": "00:52:02,000"
			},
			"offsets": {
				"from": 3100000,
				"to": 3122000
			},
			"text": " And so each coefficient is going to be a number between these two things, and that's about 254 bits, that's about 31 bytes. So, a coefficient with a polynomial with, like, 4096 coefficients can store about 128 kilobytes by putting the stuff into the coefficients."
		},
		{
			"timestamps": {
				"from": "00:52:02,000",
				"to": "00:52:21,000"
			},
			"offsets": {
				"from": 3122000,
				"to": 3141000
			},
			"text": " So, you know, now we know the way to store 128 kilobytes into polynomial, and that's kind of interesting because, like, you know, right now, roll ups, they don't even use close to that number, like, maybe they use one kilobyte."
		},
		{
			"timestamps": {
				"from": "00:52:21,000",
				"to": "00:52:34,000"
			},
			"offsets": {
				"from": 3141000,
				"to": 3154000
			},
			"text": " So, we're basically giving lots and lots of space, maybe even uncomfortably lots of space to roll ups to put their stuff in. But this is like the whole idea of 48 for four."
		},
		{
			"timestamps": {
				"from": "00:52:34,000",
				"to": "00:52:48,000"
			},
			"offsets": {
				"from": 3154000,
				"to": 3168000
			},
			"text": " Of course, in reality, we don't put the data into the coefficients and we put them into the valuations and then we do interpolation, but, like, whatever, this is not so relevant for this case, the idea is that, like, you know, when code data into polynomial"
		},
		{
			"timestamps": {
				"from": "00:52:48,000",
				"to": "00:52:55,000"
			},
			"offsets": {
				"from": 3168000,
				"to": 3175000
			},
			"text": " and we have polynomial just correspond to a big amount of data, and that's a blob. Right."
		},
		{
			"timestamps": {
				"from": "00:52:55,000",
				"to": "00:53:12,000"
			},
			"offsets": {
				"from": 3175000,
				"to": 3192000
			},
			"text": " And, you know, then we have KZG, which is what Danckred was explaining for the past 45 minutes, which is basically like a black box, where you give a polynomial to the black box and spits out the commitment, and the commitment is tiny, and the data is big."
		},
		{
			"timestamps": {
				"from": "00:53:12,000",
				"to": "00:53:23,000"
			},
			"offsets": {
				"from": 3192000,
				"to": 3203000
			},
			"text": " So, you end up with a situation where, you know, you end up posting on chain lots of data and then a small commitment."
		},
		{
			"timestamps": {
				"from": "00:53:23,000",
				"to": "00:53:26,000"
			},
			"offsets": {
				"from": 3203000,
				"to": 3206000
			},
			"text": " And this is like the rough idea."
		},
		{
			"timestamps": {
				"from": "00:53:26,000",
				"to": "00:53:39,000"
			},
			"offsets": {
				"from": 3206000,
				"to": 3219000
			},
			"text": " So, just to talk a bit about, like, when this data travels, what the network is supposed to do, you know, like, when you see a commitment, that corresponds to lots of data."
		},
		{
			"timestamps": {
				"from": "00:53:39,000",
				"to": "00:53:47,000"
			},
			"offsets": {
				"from": 3219000,
				"to": 3227000
			},
			"text": " What the network needs to do is, like, they need to make sure that the data corresponds to that commitment."
		},
		{
			"timestamps": {
				"from": "00:53:47,000",
				"to": "00:54:08,000"
			},
			"offsets": {
				"from": 3227000,
				"to": 3248000
			},
			"text": " And, like, the basic thing to do there, the basic strategy of the verifier to make sure that someone is not, like, you know, fooling us and giving us a wrong commitment to other data, which would be catastrophic, is to, you know, like, commit to P of X, use this black box again, commit to it, and then check"
		},
		{
			"timestamps": {
				"from": "00:54:08,000",
				"to": "00:54:13,000"
			},
			"offsets": {
				"from": 3248000,
				"to": 3253000
			},
			"text": " that commitment that the verifier computed matches the commitment that the guy gave you."
		},
		{
			"timestamps": {
				"from": "00:54:13,000",
				"to": "00:54:31,000"
			},
			"offsets": {
				"from": 3253000,
				"to": 3271000
			},
			"text": " So, that's basically pretty straightforward way to verify that polynomial matches the commitment, but, you know, then we have more data and more commitments, you know, in a transaction, you can have lots of those in a block, you can have lots of those."
		},
		{
			"timestamps": {
				"from": "00:54:31,000",
				"to": "00:54:49,000"
			},
			"offsets": {
				"from": 3271000,
				"to": 3289000
			},
			"text": " And that starts being quite expensive. So, what we end up doing, because, you know, like, it's 50 milliseconds to do each of the commitments, so, and it scales linearly, so that ends up being quite expensive, especially, you know, for mempool and this kind of stuff."
		},
		{
			"timestamps": {
				"from": "00:54:49,000",
				"to": "00:55:08,000"
			},
			"offsets": {
				"from": 3289000,
				"to": 3308000
			},
			"text": " So, in the end, what we're using, we're using KZG proofs, and this called random evaluation trick that Dan Krad taught you before, and basically, for it, like, data and commitment, we also put a proof of a random evaluation."
		},
		{
			"timestamps": {
				"from": "00:55:08,000",
				"to": "00:55:36,000"
			},
			"offsets": {
				"from": 3308000,
				"to": 3336000
			},
			"text": " So basically, the proof is a helper that helps you do this small verification. I don't have enough time to go into the details, but like the idea is that, you know, like the proof tells you that the committed polynomial evaluates to Y at Z, and then you can also evaluate the polynomial on the left side at Z and get some other number, and if the Y one and Y two matches, you're certain that it's the same"
		},
		{
			"timestamps": {
				"from": "00:55:36,000",
				"to": "00:55:43,000"
			},
			"offsets": {
				"from": 3336000,
				"to": 3343000
			},
			"text": " that the polynomial matches the commitment, and this is much faster than doing the commitment manually."
		},
		{
			"timestamps": {
				"from": "00:55:43,000",
				"to": "00:56:04,000"
			},
			"offsets": {
				"from": 3343000,
				"to": 3364000
			},
			"text": " It's not my intention to go very deep into this, I'm just giving you some idea of how KZG is used on the protocol. So, I think I'm going to stop here and stop with the cryptography and pass it over to Protof who's going to go a bit deeper into the actual system."
		},
		{
			"timestamps": {
				"from": "00:56:04,000",
				"to": "00:56:10,000"
			},
			"offsets": {
				"from": 3364000,
				"to": 3370000
			},
			"text": " [Applause]"
		},
		{
			"timestamps": {
				"from": "00:56:10,000",
				"to": "00:56:16,000"
			},
			"offsets": {
				"from": 3370000,
				"to": 3376000
			},
			"text": " Hello, everyone. So let's talk about the Blop usage."
		},
		{
			"timestamps": {
				"from": "00:56:16,000",
				"to": "00:56:26,000"
			},
			"offsets": {
				"from": 3376000,
				"to": 3386000
			},
			"text": " So with EAP for it for far, we're introducing a transaction type to make -- to confirm these blobs in the EVM 10."
		},
		{
			"timestamps": {
				"from": "00:56:26,000",
				"to": "00:56:36,000"
			},
			"offsets": {
				"from": 3386000,
				"to": 3396000
			},
			"text": " So, I think the other thing to note is that it's a new concept here where we are having a transaction type with data outside of the transaction."
		},
		{
			"timestamps": {
				"from": "00:56:36,000",
				"to": "00:56:39,000"
			},
			"offsets": {
				"from": 3396000,
				"to": 3399000
			},
			"text": " That's now a responsibility of the consensus layer."
		},
		{
			"timestamps": {
				"from": "00:56:39,000",
				"to": "00:56:43,000"
			},
			"offsets": {
				"from": 3399000,
				"to": 3403000
			},
			"text": " So it's like a regular EAP month of offline transaction."
		},
		{
			"timestamps": {
				"from": "00:56:43,000",
				"to": "00:56:47,000"
			},
			"offsets": {
				"from": 3403000,
				"to": 3407000
			},
			"text": " The transaction contains some pointers or hashes really."
		},
		{
			"timestamps": {
				"from": "00:56:47,000",
				"to": "00:56:55,000"
			},
			"offsets": {
				"from": 3407000,
				"to": 3415000
			},
			"text": " Let's then commit to the blop data."
		},
		{
			"timestamps": {
				"from": "00:56:55,000",
				"to": "00:57:05,000"
			},
			"offsets": {
				"from": 3415000,
				"to": 3425000
			},
			"text": " This is the transaction in a little bit more detail. Something else to note is that it's not RLP, but as a Z that merkilizes nicely and it's better for layer two."
		},
		{
			"timestamps": {
				"from": "00:57:05,000",
				"to": "00:57:17,000"
			},
			"offsets": {
				"from": 3425000,
				"to": 3437000
			},
			"text": " And then note here that we have these data hashes committing or hashing the KZG commitments which then commits to the flop data."
		},
		{
			"timestamps": {
				"from": "00:57:17,000",
				"to": "00:57:25,000"
			},
			"offsets": {
				"from": 3437000,
				"to": 3445000
			},
			"text": " These data hashes are available in the EVM through an op-code, whereas the blop data lives outside of the EVM."
		},
		{
			"timestamps": {
				"from": "00:57:25,000",
				"to": "00:57:32,000"
			},
			"offsets": {
				"from": 3445000,
				"to": 3452000
			},
			"text": " So the blop content is unlike call data, not available in the EVM."
		},
		{
			"timestamps": {
				"from": "00:57:32,000",
				"to": "00:57:44,000"
			},
			"offsets": {
				"from": 3452000,
				"to": 3464000
			},
			"text": " Eventually, we can prune this blop data. It's not a long-term commitment to store off this blop data, but rather we are introducing this blop data just for the availability properties."
		},
		{
			"timestamps": {
				"from": "00:57:44,000",
				"to": "00:57:58,000"
			},
			"offsets": {
				"from": 3464000,
				"to": 3478000
			},
			"text": " So the layer two needs this data to help users sink the latest data permissionlessly without communicating directly with the sequencer or whichever operator exists on the roll-up."
		},
		{
			"timestamps": {
				"from": "00:57:58,000",
				"to": "00:58:02,000"
			},
			"offsets": {
				"from": 3478000,
				"to": 3482000
			},
			"text": " And then people can reconstruct the latest data."
		},
		{
			"timestamps": {
				"from": "00:58:02,000",
				"to": "00:58:11,000"
			},
			"offsets": {
				"from": 3482000,
				"to": 3491000
			},
			"text": " We can have a different solution for retrieving very old states like a month ago or two weeks ago."
		},
		{
			"timestamps": {
				"from": "00:58:11,000",
				"to": "00:58:17,000"
			},
			"offsets": {
				"from": 3491000,
				"to": 3497000
			},
			"text": " So there's the separation of data and the transaction itself."
		},
		{
			"timestamps": {
				"from": "00:58:17,000",
				"to": "00:58:22,000"
			},
			"offsets": {
				"from": 3497000,
				"to": 3502000
			},
			"text": " So this is what the life cycle looks like. As a layer two user, you submit the transaction."
		},
		{
			"timestamps": {
				"from": "00:58:22,000",
				"to": "00:58:28,000"
			},
			"offsets": {
				"from": 3502000,
				"to": 3508000
			},
			"text": " Then we have this bundling. As a layer two, we often combine the transactions."
		},
		{
			"timestamps": {
				"from": "00:58:28,000",
				"to": "00:58:31,000"
			},
			"offsets": {
				"from": 3508000,
				"to": 3511000
			},
			"text": " So you can pack them, compress them and so on."
		},
		{
			"timestamps": {
				"from": "00:58:31,000",
				"to": "00:58:34,000"
			},
			"offsets": {
				"from": 3511000,
				"to": 3514000
			},
			"text": " This is the task for the roll-up operator."
		},
		{
			"timestamps": {
				"from": "00:58:34,000",
				"to": "00:58:41,000"
			},
			"offsets": {
				"from": 3514000,
				"to": 3521000
			},
			"text": " And then as a roll-up operator, you publish your bundle two layer one with this new transaction type."
		},
		{
			"timestamps": {
				"from": "00:58:41,000",
				"to": "00:58:49,000"
			},
			"offsets": {
				"from": 3521000,
				"to": 3529000
			},
			"text": " And then in the transaction pool, we have both the transaction that pays the fee as well as to record data with the actual blop content."
		},
		{
			"timestamps": {
				"from": "00:58:49,000",
				"to": "00:59:03,000"
			},
			"offsets": {
				"from": 3529000,
				"to": 3543000
			},
			"text": " And then the layer one beacon proposal creates a block and the blobs make their way from the transaction pool in the exclusion layer to the consensus layer."
		},
		{
			"timestamps": {
				"from": "00:59:03,000",
				"to": "00:59:08,000"
			},
			"offsets": {
				"from": 3543000,
				"to": 3548000
			},
			"text": " At this point, the blobs don't get into the exclusion layer back."
		},
		{
			"timestamps": {
				"from": "00:59:08,000",
				"to": "00:59:11,000"
			},
			"offsets": {
				"from": 3548000,
				"to": 3551000
			},
			"text": " It's just a responsibility of the consensus layer."
		},
		{
			"timestamps": {
				"from": "00:59:11,000",
				"to": "00:59:22,000"
			},
			"offsets": {
				"from": 3551000,
				"to": 3562000
			},
			"text": " Pairs on the beacon chain, they sync the blobs bundled together with blobs from other transactions as a sidecar."
		},
		{
			"timestamps": {
				"from": "00:59:22,000",
				"to": "00:59:31,000"
			},
			"offsets": {
				"from": 3562000,
				"to": 3571000
			},
			"text": " And then the exclusion payload stays on layer one, whereas the blobs stay available for a sufficient amount of time to secure layer two,"
		},
		{
			"timestamps": {
				"from": "00:59:31,000",
				"to": "00:59:37,000"
			},
			"offsets": {
				"from": 3571000,
				"to": 3577000
			},
			"text": " but then can be pruned afterwards. So blob data is bounded."
		},
		{
			"timestamps": {
				"from": "00:59:37,000",
				"to": "00:59:40,000"
			},
			"offsets": {
				"from": 3577000,
				"to": 3580000
			},
			"text": " This is what it looks like on the network level."
		},
		{
			"timestamps": {
				"from": "00:59:40,000",
				"to": "00:59:47,000"
			},
			"offsets": {
				"from": 3580000,
				"to": 3587000
			},
			"text": " We have the layer two sequencer communicating with the transaction pool, the execution engine communicating with the beacon proposal,"
		},
		{
			"timestamps": {
				"from": "00:59:47,000",
				"to": "00:59:51,000"
			},
			"offsets": {
				"from": 3587000,
				"to": 3591000
			},
			"text": " then beacon nodes syncing the blobs with each other."
		},
		{
			"timestamps": {
				"from": "00:59:51,000",
				"to": "01:00:03,000"
			},
			"offsets": {
				"from": 3591000,
				"to": 3603000
			},
			"text": " And then there's the split of the data where the other beacon nodes, they give the exclusion payload, process the EPM and everything."
		},
		{
			"timestamps": {
				"from": "01:00:03,000",
				"to": "01:00:05,000"
			},
			"offsets": {
				"from": 3603000,
				"to": 3605000
			},
			"text": " Fees will be processed by everybody."
		},
		{
			"timestamps": {
				"from": "01:00:05,000",
				"to": "01:00:17,000"
			},
			"offsets": {
				"from": 3605000,
				"to": 3617000
			},
			"text": " With the blobs, they stay in the consensus layer until a layer two node retrieves them to reconstruct the layer two states."
		},
		{
			"timestamps": {
				"from": "01:00:17,000",
				"to": "01:00:21,000"
			},
			"offsets": {
				"from": 3617000,
				"to": 3621000
			},
			"text": " So how do rollups work with this?"
		},
		{
			"timestamps": {
				"from": "01:00:21,000",
				"to": "01:00:27,000"
			},
			"offsets": {
				"from": 3621000,
				"to": 3627000
			},
			"text": " Dan Kout already explains the proof of a equivalent trick."
		},
		{
			"timestamps": {
				"from": "01:00:27,000",
				"to": "01:00:32,000"
			},
			"offsets": {
				"from": 3627000,
				"to": 3632000
			},
			"text": " So I'll give you just a simplified overview of how we do this in the EPM."
		},
		{
			"timestamps": {
				"from": "01:00:32,000",
				"to": "01:00:37,000"
			},
			"offsets": {
				"from": 3632000,
				"to": 3637000
			},
			"text": " We introduce two new things in the EPM, an opcode and a precompile."
		},
		{
			"timestamps": {
				"from": "01:00:37,000",
				"to": "01:00:46,000"
			},
			"offsets": {
				"from": 3637000,
				"to": 3646000
			},
			"text": " The opcode simply retrieves the data hash, which is this hash that is part of the transaction,"
		},
		{
			"timestamps": {
				"from": "01:00:46,000",
				"to": "01:00:51,000"
			},
			"offsets": {
				"from": 3646000,
				"to": 3651000
			},
			"text": " just like the hashes in the access list from the Berlin transaction type."
		},
		{
			"timestamps": {
				"from": "01:00:51,000",
				"to": "01:00:56,000"
			},
			"offsets": {
				"from": 3651000,
				"to": 3656000
			},
			"text": " It can be retrieved through an opcode, pushed on the stack."
		},
		{
			"timestamps": {
				"from": "01:00:56,000",
				"to": "01:01:12,000"
			},
			"offsets": {
				"from": 3656000,
				"to": 3672000
			},
			"text": " And then there's this precompile, which you can provide as a proof, to verify that a certain data at a certain position matches the blob content committed to by the data hash."
		},
		{
			"timestamps": {
				"from": "01:01:12,000",
				"to": "01:01:33,000"
			},
			"offsets": {
				"from": 3672000,
				"to": 3693000
			},
			"text": " In the case of Ziga rollup, we do, so we use this precompile to do a random evaluation and prove that the data that the rollup is importing is equivalent equal to the data that the blobs is introducing."
		},
		{
			"timestamps": {
				"from": "01:01:36,000",
				"to": "01:01:46,000"
			},
			"offsets": {
				"from": 3696000,
				"to": 3706000
			},
			"text": " This precompile is versioned, so we can change the commitment scheme and in the future, I hope we can use it for other things, perhaps a FERCLE 3 verification."
		},
		{
			"timestamps": {
				"from": "01:01:46,000",
				"to": "01:01:49,000"
			},
			"offsets": {
				"from": 3706000,
				"to": 3709000
			},
			"text": " Then this is part two."
		},
		{
			"timestamps": {
				"from": "01:01:49,000",
				"to": "01:01:51,000"
			},
			"offsets": {
				"from": 3709000,
				"to": 3711000
			},
			"text": " Yes?"
		},
		{
			"timestamps": {
				"from": "01:01:51,000",
				"to": "01:01:57,000"
			},
			"offsets": {
				"from": 3711000,
				"to": 3717000
			},
			"text": " Can you evaluate that for the blob as the comment?"
		},
		{
			"timestamps": {
				"from": "01:01:57,000",
				"to": "01:01:58,000"
			},
			"offsets": {
				"from": 3717000,
				"to": 3718000
			},
			"text": " Ready?"
		},
		{
			"timestamps": {
				"from": "01:01:58,000",
				"to": "01:02:05,000"
			},
			"offsets": {
				"from": 3718000,
				"to": 3725000
			},
			"text": " So, going back, the proof, all the inputs that precompile are passed in as call data."
		},
		{
			"timestamps": {
				"from": "01:02:05,000",
				"to": "01:02:13,000"
			},
			"offsets": {
				"from": 3725000,
				"to": 3733000
			},
			"text": " The blob is completely separate, it's not involved in any of this computation."
		},
		{
			"timestamps": {
				"from": "01:02:13,000",
				"to": "01:02:22,000"
			},
			"offsets": {
				"from": 3733000,
				"to": 3742000
			},
			"text": " And so it's just the call data that we're passing in with a proof, the index of the pointer trying to fetch the file."
		},
		{
			"timestamps": {
				"from": "01:02:22,000",
				"to": "01:02:30,000"
			},
			"offsets": {
				"from": 3742000,
				"to": 3750000
			},
			"text": " The commitment that hashes to the hash that we retrieve from the op-cards and then the precompile will verify everything."
		},
		{
			"timestamps": {
				"from": "01:02:30,000",
				"to": "01:02:34,000"
			},
			"offsets": {
				"from": 3750000,
				"to": 3754000
			},
			"text": " Similarly, we have the Ziga state transition that needs to be verified."
		},
		{
			"timestamps": {
				"from": "01:02:34,000",
				"to": "01:02:39,000"
			},
			"offsets": {
				"from": 3754000,
				"to": 3759000
			},
			"text": " This is all Ziga rollup specific up to you to design this."
		},
		{
			"timestamps": {
				"from": "01:02:39,000",
				"to": "01:02:51,000"
			},
			"offsets": {
				"from": 3759000,
				"to": 3771000
			},
			"text": " But with the data that's verified and the Ziga proof that's verified, we can then get some outputs that we can persist and then use to enable the results."
		},
		{
			"timestamps": {
				"from": "01:02:51,000",
				"to": "01:02:56,000"
			},
			"offsets": {
				"from": 3771000,
				"to": 3776000
			},
			"text": " Then this is the first of the interactive optimistic rollups."
		},
		{
			"timestamps": {
				"from": "01:02:56,000",
				"to": "01:03:11,000"
			},
			"offsets": {
				"from": 3776000,
				"to": 3791000
			},
			"text": " Interactive optimistic rollups use this concept called a preimage oracle, where we do not access all the data at the same time, but rather the load preimages one at a time."
		},
		{
			"timestamps": {
				"from": "01:03:11,000",
				"to": "01:03:27,000"
			},
			"offsets": {
				"from": 3791000,
				"to": 3807000
			},
			"text": " And by setting an execution trace, we only really have to do a proof for a single step, a single execution of a single VM instruction."
		},
		{
			"timestamps": {
				"from": "01:03:27,000",
				"to": "01:03:30,000"
			},
			"offsets": {
				"from": 3807000,
				"to": 3810000
			},
			"text": " And this might be loading some data."
		},
		{
			"timestamps": {
				"from": "01:03:30,000",
				"to": "01:03:47,000"
			},
			"offsets": {
				"from": 3810000,
				"to": 3827000
			},
			"text": " So for example, the start of a layer one block header hash, then we retrieve the full block header as a preimage, then we retrieve the transactions by digging into the Merck 3 commitment in the transactions hash."
		},
		{
			"timestamps": {
				"from": "01:03:47,000",
				"to": "01:03:58,000"
			},
			"offsets": {
				"from": 3827000,
				"to": 3838000
			},
			"text": " And then we can get the data hashes from the transaction, and then from the data hash, we can get the gazd commitment, and then it's not a regular hash commitment anymore."
		},
		{
			"timestamps": {
				"from": "01:03:58,000",
				"to": "01:04:07,000"
			},
			"offsets": {
				"from": 3838000,
				"to": 3847000
			},
			"text": " There's a different type of commitment with the same oracle, very load, one point from the block that is committed to by the block transaction."
		},
		{
			"timestamps": {
				"from": "01:04:07,000",
				"to": "01:04:13,000"
			},
			"offsets": {
				"from": 3847000,
				"to": 3853000
			},
			"text": " And so this way we can load all the data into the fraud proof VM."
		},
		{
			"timestamps": {
				"from": "01:04:13,000",
				"to": "01:04:26,000"
			},
			"offsets": {
				"from": 3853000,
				"to": 3866000
			},
			"text": " Okay, yeah, hello everyone, I'm on Scott, I'm going to talk a little bit about now that we hopefully in the future will have this functionality, how can you pay for it, but also kind of conceptually, basically, I mean the theorem"
		},
		{
			"timestamps": {
				"from": "01:04:26,000",
				"to": "01:04:34,000"
			},
			"offsets": {
				"from": 3866000,
				"to": 3874000
			},
			"text": " function is already, you know, kind of pushing it's limits like where's the extra space resource wise for this basically where does the efficiency gain here come from."
		},
		{
			"timestamps": {
				"from": "01:04:34,000",
				"to": "01:04:41,000"
			},
			"offsets": {
				"from": 3874000,
				"to": 3881000
			},
			"text": " And to understand that first we have to just look in general about like how does resource pricing on Ethereum work today."
		},
		{
			"timestamps": {
				"from": "01:04:41,000",
				"to": "01:04:47,000"
			},
			"offsets": {
				"from": 3881000,
				"to": 3887000
			},
			"text": " So this is just kind of my way of thinking about like categorizing the different resource we have on Ethereum."
		},
		{
			"timestamps": {
				"from": "01:04:47,000",
				"to": "01:04:52,000"
			},
			"offsets": {
				"from": 3887000,
				"to": 3892000
			},
			"text": " So there's things like bandwidth, compute, state access, memory, state growth, history growth, right?"
		},
		{
			"timestamps": {
				"from": "01:04:52,000",
				"to": "01:04:55,000"
			},
			"offsets": {
				"from": 3892000,
				"to": 3895000
			},
			"text": " This is all the kind of things, and this is a non-exhaustive list, right?"
		},
		{
			"timestamps": {
				"from": "01:04:55,000",
				"to": "01:05:02,000"
			},
			"offsets": {
				"from": 3895000,
				"to": 3902000
			},
			"text": " But this is basically the kind of things that actually cause effort for nodes while they're processing a transaction."
		},
		{
			"timestamps": {
				"from": "01:05:02,000",
				"to": "01:05:15,000"
			},
			"offsets": {
				"from": 3902000,
				"to": 3915000
			},
			"text": " And if you're squinted this hard enough, you'll notice that there are basically two different types of resources here, and we call those basically burst limits and sustained limits."
		},
		{
			"timestamps": {
				"from": "01:05:15,000",
				"to": "01:05:25,000"
			},
			"offsets": {
				"from": 3915000,
				"to": 3925000
			},
			"text": " And so burst limits are things where basically they cause costs or can costs right at the moment that the block is propagated, right?"
		},
		{
			"timestamps": {
				"from": "01:05:25,000",
				"to": "01:05:38,000"
			},
			"offsets": {
				"from": 3925000,
				"to": 3938000
			},
			"text": " The bandwidth to propagate a block, the compute to actually verify it, all of this, the critical point there is that basically it has to be bounded in order for blocks to still be propagated in a timely manner."
		},
		{
			"timestamps": {
				"from": "01:05:38,000",
				"to": "01:05:42,000"
			},
			"offsets": {
				"from": 3938000,
				"to": 3942000
			},
			"text": " And in order for nodes to be able to verify them at all, right?"
		},
		{
			"timestamps": {
				"from": "01:05:42,000",
				"to": "01:05:43,000"
			},
			"offsets": {
				"from": 3942000,
				"to": 3943000
			},
			"text": " They might run out of resources."
		},
		{
			"timestamps": {
				"from": "01:05:43,000",
				"to": "01:05:47,000"
			},
			"offsets": {
				"from": 3943000,
				"to": 3947000
			},
			"text": " And the sustained limits, they don't matter so much block to block."
		},
		{
			"timestamps": {
				"from": "01:05:47,000",
				"to": "01:05:49,000"
			},
			"offsets": {
				"from": 3947000,
				"to": 3949000
			},
			"text": " Those are more things that accumulate over time."
		},
		{
			"timestamps": {
				"from": "01:05:49,000",
				"to": "01:06:01,000"
			},
			"offsets": {
				"from": 3949000,
				"to": 3961000
			},
			"text": " So let's say growth history grows these kind of things, where a single block can't really produce too much damage there, but over time, it just basically makes it more costly to run a full node."
		},
		{
			"timestamps": {
				"from": "01:06:01,000",
				"to": "01:06:15,000"
			},
			"offsets": {
				"from": 3961000,
				"to": 3975000
			},
			"text": " And as it turns out, if you look at this, there's some sort of structure to this, and you can actually reorder this a little bit, and it turns out that usually there's a relatively good matching between a specific burst limit and a specific sustained limit."
		},
		{
			"timestamps": {
				"from": "01:06:15,000",
				"to": "01:06:18,000"
			},
			"offsets": {
				"from": 3975000,
				"to": 3978000
			},
			"text": " So bandwidth and history growth, kind of they correspond, right?"
		},
		{
			"timestamps": {
				"from": "01:06:18,000",
				"to": "01:06:30,000"
			},
			"offsets": {
				"from": 3978000,
				"to": 3990000
			},
			"text": " Because the bigger block is, the bigger, like the more bandwidth you need to propagate, but then also the more disk space you need to just, you know, keep it around for history purposes, and similar with state, access and state growth, these kind of things."
		},
		{
			"timestamps": {
				"from": "01:06:30,000",
				"to": "01:06:33,000"
			},
			"offsets": {
				"from": 3990000,
				"to": 3993000
			},
			"text": " Now, specifically for for for for, right?"
		},
		{
			"timestamps": {
				"from": "01:06:33,000",
				"to": "01:06:35,000"
			},
			"offsets": {
				"from": 3993000,
				"to": 3995000
			},
			"text": " What we are introducing is this new type of data."
		},
		{
			"timestamps": {
				"from": "01:06:35,000",
				"to": "01:06:40,000"
			},
			"offsets": {
				"from": 3995000,
				"to": 4000000
			},
			"text": " So kind of the resources we're talking about here is this first kind of row."
		},
		{
			"timestamps": {
				"from": "01:06:40,000",
				"to": "01:06:42,000"
			},
			"offsets": {
				"from": 4000000,
				"to": 4002000
			},
			"text": " So it's on the best limit."
		},
		{
			"timestamps": {
				"from": "01:06:42,000",
				"to": "01:06:50,000"
			},
			"offsets": {
				"from": 4002000,
				"to": 4010000
			},
			"text": " It's the bandwidth, how big can blocks get, and then the sustained limit, it's just like how much resources do you need to store kind of the history of Ethereum."
		},
		{
			"timestamps": {
				"from": "01:06:50,000",
				"to": "01:07:09,000"
			},
			"offsets": {
				"from": 4010000,
				"to": 4029000
			},
			"text": " And if you, if you put a tension, and if you basically looked into the appeal a little bit, you already know that like, there is this limit in terms of history growth, we basically, we introduce this new new, basically a mechanism where blocks are only stored for a single month."
		},
		{
			"timestamps": {
				"from": "01:07:09,000",
				"to": "01:07:28,000"
			},
			"offsets": {
				"from": 4029000,
				"to": 4048000
			},
			"text": " And so this is basically why on the history growth side, we basically, it will, it does mean that there will be some extra requirement for node operators, but it's quite bounded because, unless normal history that today is stored forever, but even after this, this nice AP for for for for for, basically, even after that, it's still going to be stored for a year."
		},
		{
			"timestamps": {
				"from": "01:07:28,000",
				"to": "01:07:30,000"
			},
			"offsets": {
				"from": 4048000,
				"to": 4050000
			},
			"text": " Blobs are only stored for months."
		},
		{
			"timestamps": {
				"from": "01:07:30,000",
				"to": "01:07:36,000"
			},
			"offsets": {
				"from": 4050000,
				"to": 4056000
			},
			"text": " So basically in terms of sustained limits, it's not basically, it has like a very limited impact."
		},
		{
			"timestamps": {
				"from": "01:07:36,000",
				"to": "01:07:42,000"
			},
			"offsets": {
				"from": 4056000,
				"to": 4062000
			},
			"text": " The more interesting and also more tricky side of this picture is the the burst limit so so bandwidth."
		},
		{
			"timestamps": {
				"from": "01:07:42,000",
				"to": "01:07:48,000"
			},
			"offsets": {
				"from": 4062000,
				"to": 4068000
			},
			"text": " And to kind of understand like what, what the situation there is and how for it for for fits in."
		},
		{
			"timestamps": {
				"from": "01:07:48,000",
				"to": "01:07:52,000"
			},
			"offsets": {
				"from": 4068000,
				"to": 4072000
			},
			"text": " You have to first remember that today on Ethereum, basically, we only have a single gas price, right?"
		},
		{
			"timestamps": {
				"from": "01:07:52,000",
				"to": "01:08:05,000"
			},
			"offsets": {
				"from": 4072000,
				"to": 4085000
			},
			"text": " Whenever you send a transaction, you don't actually specify how much bandwidth and I want to use how much compute, how much memory you just spend, like one gas, one gas limit and then also like how basically how, how, what kind of base fee are you willing to pay for this, right?"
		},
		{
			"timestamps": {
				"from": "01:08:05,000",
				"to": "01:08:11,000"
			},
			"offsets": {
				"from": 4085000,
				"to": 4091000
			},
			"text": " And it's all basically mapped down into what you could think about it as like a single dimension for pricing."
		},
		{
			"timestamps": {
				"from": "01:08:11,000",
				"to": "01:08:19,000"
			},
			"offsets": {
				"from": 4091000,
				"to": 4099000
			},
			"text": " And that comes with basically very real trade-offs in terms of a kind of resource efficiency."
		},
		{
			"timestamps": {
				"from": "01:08:19,000",
				"to": "01:08:29,000"
			},
			"offsets": {
				"from": 4099000,
				"to": 4109000
			},
			"text": " So if you look at this kind of stylized picture of just looking at two different dimensions here, they could be at a data and compute or data and memory or whatever, two different dimensions."
		},
		{
			"timestamps": {
				"from": "01:08:29,000",
				"to": "01:08:36,000"
			},
			"offsets": {
				"from": 4109000,
				"to": 4116000
			},
			"text": " And basically the way the kind of the Ethereum gas works today and that's purely for simplicity, right?"
		},
		{
			"timestamps": {
				"from": "01:08:36,000",
				"to": "01:08:49,000"
			},
			"offsets": {
				"from": 4116000,
				"to": 4129000
			},
			"text": " Because it's very simple to for users to deal with one dimension basically, but the way it works is basically that it is basically that those two resources compete for use it in a block, right?"
		},
		{
			"timestamps": {
				"from": "01:08:49,000",
				"to": "01:08:56,000"
			},
			"offsets": {
				"from": 4129000,
				"to": 4136000
			},
			"text": " So you could imagine if you use, if a block is very full of compute, then there's very little room to put any data in it or the other way around."
		},
		{
			"timestamps": {
				"from": "01:08:56,000",
				"to": "01:09:05,000"
			},
			"offsets": {
				"from": 4136000,
				"to": 4145000
			},
			"text": " And actually if you if you want to like open Ethos, Ethos can, for example, they for every block for the detailed detailed page, they actually give you the size of the block."
		},
		{
			"timestamps": {
				"from": "01:09:05,000",
				"to": "01:09:10,000"
			},
			"offsets": {
				"from": 4145000,
				"to": 4150000
			},
			"text": " And usually it's something like 50 kilobytes, 100 kilobytes, but like rarely more than that."
		},
		{
			"timestamps": {
				"from": "01:09:10,000",
				"to": "01:09:24,000"
			},
			"offsets": {
				"from": 4150000,
				"to": 4164000
			},
			"text": " But if you look at what would a look, look, look like if it was like just full of call data, which is where all the data comes from, if you were all the way like say on the lower part of the diagram with a free source B was for data, it could actually be up to one or two megabytes,"
		},
		{
			"timestamps": {
				"from": "01:09:24,000",
				"to": "01:09:28,000"
			},
			"offsets": {
				"from": 4164000,
				"to": 4168000
			},
			"text": " like well, two megabytes basically of size, right?"
		},
		{
			"timestamps": {
				"from": "01:09:28,000",
				"to": "01:09:33,000"
			},
			"offsets": {
				"from": 4168000,
				"to": 4173000
			},
			"text": " So what that means is we basically determined in the past that two megabytes per block are kind of safe."
		},
		{
			"timestamps": {
				"from": "01:09:33,000",
				"to": "01:09:36,000"
			},
			"offsets": {
				"from": 4173000,
				"to": 4176000
			},
			"text": " And the reasons would be there, right?"
		},
		{
			"timestamps": {
				"from": "01:09:36,000",
				"to": "01:09:43,000"
			},
			"offsets": {
				"from": 4176000,
				"to": 4183000
			},
			"text": " It's basically sitting there, but an average block basically completely underutilizes data."
		},
		{
			"timestamps": {
				"from": "01:09:43,000",
				"to": "01:09:48,000"
			},
			"offsets": {
				"from": 4183000,
				"to": 4188000
			},
			"text": " And that is again just because it's just simpler for us conceptually to price these things."
		},
		{
			"timestamps": {
				"from": "01:09:48,000",
				"to": "01:09:53,000"
			},
			"offsets": {
				"from": 4188000,
				"to": 4193000
			},
			"text": " So most of the time we are like very far up the slope there."
		},
		{
			"timestamps": {
				"from": "01:09:53,000",
				"to": "01:09:54,000"
			},
			"offsets": {
				"from": 4193000,
				"to": 4194000
			},
			"text": " And where do you want to be?"
		},
		{
			"timestamps": {
				"from": "01:09:54,000",
				"to": "01:09:57,000"
			},
			"offsets": {
				"from": 4194000,
				"to": 4197000
			},
			"text": " Like what would be like the most efficient way of handling root resources?"
		},
		{
			"timestamps": {
				"from": "01:09:57,000",
				"to": "01:09:59,000"
			},
			"offsets": {
				"from": 4197000,
				"to": 4199000
			},
			"text": " Well, that would basically be this picture."
		},
		{
			"timestamps": {
				"from": "01:09:59,000",
				"to": "01:10:11,000"
			},
			"offsets": {
				"from": 4199000,
				"to": 4211000
			},
			"text": " So ideally you'd want to basically make these things independently consumable where you can basically consume the most amount of the most, the highest safe amount of data that we think you should be like the chain can manage."
		},
		{
			"timestamps": {
				"from": "01:10:11,000",
				"to": "01:10:19,000"
			},
			"offsets": {
				"from": 4211000,
				"to": 4219000
			},
			"text": " But then at the same time you should also be able to do, you know, do still do state access to the biggest, highest amount possible or memory or whatever, right?"
		},
		{
			"timestamps": {
				"from": "01:10:19,000",
				"to": "01:10:23,000"
			},
			"offsets": {
				"from": 4219000,
				"to": 4223000
			},
			"text": " There should not be this, this kind of competitive nature to it."
		},
		{
			"timestamps": {
				"from": "01:10:23,000",
				"to": "01:10:31,000"
			},
			"offsets": {
				"from": 4223000,
				"to": 4231000
			},
			"text": " And this is basically where 48 for four on the burst limit side and gets it's efficiency, right?"
		},
		{
			"timestamps": {
				"from": "01:10:31,000",
				"to": "01:10:36,000"
			},
			"offsets": {
				"from": 4231000,
				"to": 4236000
			},
			"text": " Because full charting, full-dung charting, we'll hear about it a bit more after this."
		},
		{
			"timestamps": {
				"from": "01:10:36,000",
				"to": "01:10:44,000"
			},
			"offsets": {
				"from": 4236000,
				"to": 4244000
			},
			"text": " Actually that's really clever things where people only sample the data so bandwidth constraints goes quite down but for 48 for four, there is no fancy trick, right?"
		},
		{
			"timestamps": {
				"from": "01:10:44,000",
				"to": "01:10:48,000"
			},
			"offsets": {
				"from": 4244000,
				"to": 4248000
			},
			"text": " Everyone still downloads all the data so it's very real bandwidth strain."
		},
		{
			"timestamps": {
				"from": "01:10:48,000",
				"to": "01:11:00,000"
			},
			"offsets": {
				"from": 4248000,
				"to": 4260000
			},
			"text": " So the innovation on the burst limit side is purely trying to get to this upper right point, trying to actually basically make it so that the existing resource we already have today is just more efficiently utilized."
		},
		{
			"timestamps": {
				"from": "01:11:00,000",
				"to": "01:11:11,000"
			},
			"offsets": {
				"from": 4260000,
				"to": 4271000
			},
			"text": " And the way we do this is by going from, as I was saying, like right now today pricing is one dimensional and so what we introduced with 444 is basically we go 2D."
		},
		{
			"timestamps": {
				"from": "01:11:11,000",
				"to": "01:11:15,000"
			},
			"offsets": {
				"from": 4271000,
				"to": 4275000
			},
			"text": " And this is how that looks like."
		},
		{
			"timestamps": {
				"from": "01:11:15,000",
				"to": "01:11:25,000"
			},
			"offsets": {
				"from": 4275000,
				"to": 4285000
			},
			"text": " So this is an open PR right now, it's not yet quite much but you can have a look so like small details might still change but I think the general direction is pretty set."
		},
		{
			"timestamps": {
				"from": "01:11:25,000",
				"to": "01:11:41,000"
			},
			"offsets": {
				"from": 4285000,
				"to": 4301000
			},
			"text": " And so the idea is we introduce what we call data gas and as you can kind of figure from the name, it's not blob gas, the aspiration would be that like maybe in the future we can expand this to cover the entire data dimension but for now it's only used for blobs and we set it in a way where basically"
		},
		{
			"timestamps": {
				"from": "01:11:41,000",
				"to": "01:11:45,000"
			},
			"offsets": {
				"from": 4301000,
				"to": 4305000
			},
			"text": " one byte of blob will cost one data gas."
		},
		{
			"timestamps": {
				"from": "01:11:45,000",
				"to": "01:12:01,000"
			},
			"offsets": {
				"from": 4305000,
				"to": 4321000
			},
			"text": " And this data gas, importantly basically is completely independently priced from normal gas, so it has its own 5059 style mechanism where, and that's where basically we're the use and I see Marius not very happy about this because you know like he has to implement it and get at the end of the day."
		},
		{
			"timestamps": {
				"from": "01:12:01,000",
				"to": "01:12:08,000"
			},
			"offsets": {
				"from": 4321000,
				"to": 4328000
			},
			"text": " But this is really important for the AP because other than that basically you wouldn't be able to get to this more efficient bandwidth usage."
		},
		{
			"timestamps": {
				"from": "01:12:08,000",
				"to": "01:12:18,000"
			},
			"offsets": {
				"from": 4328000,
				"to": 4338000
			},
			"text": " So what does it look like and how does it, how can you think about it, well it's just you know, similar to how 5059 already looks, so the way that this is courtesy of a prototype all the side."
		},
		{
			"timestamps": {
				"from": "01:12:18,000",
				"to": "01:12:29,000"
			},
			"offsets": {
				"from": 4338000,
				"to": 4349000
			},
			"text": " And so every column here would be a separate slot so the first slot, and in this case basically the target amount of blobs would be to the maximum allowed would be four on a block."
		},
		{
			"timestamps": {
				"from": "01:12:29,000",
				"to": "01:12:37,000"
			},
			"offsets": {
				"from": 4349000,
				"to": 4357000
			},
			"text": " So the first block comes in it has exactly two, so nothing happens, the next one has three right the red one it's basically one to many."
		},
		{
			"timestamps": {
				"from": "01:12:37,000",
				"to": "01:12:49,000"
			},
			"offsets": {
				"from": 4357000,
				"to": 4369000
			},
			"text": " So the price would go up, and then the next two kind of like a stable again and then one misses a blob so the price goes back down so it's like a very you know like just like 5059 like you know you know and love it basically."
		},
		{
			"timestamps": {
				"from": "01:12:49,000",
				"to": "01:13:08,000"
			},
			"offsets": {
				"from": 4369000,
				"to": 4388000
			},
			"text": " It is a bit different or like basically it's under the hood, it works a bit differently so here's kind of a bit more more look at the details here so first of all of course just we have a max data gas per block right just similar to 2059 and the target that is half of that."
		},
		{
			"timestamps": {
				"from": "01:13:08,000",
				"to": "01:13:22,000"
			},
			"offsets": {
				"from": 4388000,
				"to": 4402000
			},
			"text": " So these block transactions they specify an additional max fee per data gas field, so like how much are they willing max per data gas to have that transaction included."
		},
		{
			"timestamps": {
				"from": "01:13:22,000",
				"to": "01:13:32,000"
			},
			"offsets": {
				"from": 4402000,
				"to": 4412000
			},
			"text": " Importantly you know the disuses it is a little bit extra complexity for users, but users in this case are not actually users those are like big roll ups right so basically them having to specify one more value."
		},
		{
			"timestamps": {
				"from": "01:13:32,000",
				"to": "01:13:38,000"
			},
			"offsets": {
				"from": 4412000,
				"to": 4418000
			},
			"text": " You know fine that shouldn't like if you can't do that maybe you shouldn't be in the world game basically."
		},
		{
			"timestamps": {
				"from": "01:13:38,000",
				"to": "01:13:49,000"
			},
			"offsets": {
				"from": 4418000,
				"to": 4429000
			},
			"text": " And, and so we to keep the complexity this your minimum though we did not opt for having a separate tip for this dimension so we just reuse the existing existing tip."
		},
		{
			"timestamps": {
				"from": "01:13:49,000",
				"to": "01:14:04,000"
			},
			"offsets": {
				"from": 4429000,
				"to": 4444000
			},
			"text": " And, and then we one thing that we were deviate from 59 a little bit in 1559 basically if the demand were to completely crash theoretically like one gas could could be could be I think valued as little as seven way, which is just the minimum after which basically updates"
		},
		{
			"timestamps": {
				"from": "01:14:04,000",
				"to": "01:14:08,000"
			},
			"offsets": {
				"from": 4444000,
				"to": 4448000
			},
			"text": " don't don't go lower anymore but so transaction would basically be free."
		},
		{
			"timestamps": {
				"from": "01:14:08,000",
				"to": "01:14:21,000"
			},
			"offsets": {
				"from": 4448000,
				"to": 4461000
			},
			"text": " So we don't quite want basically to make the the lowest demand case of transaction see completely free so we said a minimum data gas price that's that's kind of at least somewhat meaningful so that's like 10 to the minus five eat per blob."
		},
		{
			"timestamps": {
				"from": "01:14:21,000",
				"to": "01:14:28,000"
			},
			"offsets": {
				"from": 4461000,
				"to": 4468000
			},
			"text": " So of course it's priced in data gas but it comes down if you if you compute the cost of a full block the book to to their value."
		},
		{
			"timestamps": {
				"from": "01:14:28,000",
				"to": "01:14:41,000"
			},
			"offsets": {
				"from": 4468000,
				"to": 4481000
			},
			"text": " And the last thing and again this is very technical so like if if you just want to stand to concept share this works don't don't care about this but, but if you ever pull up the IP and you might stumble across this and you might be confused so so actually the way we"
		},
		{
			"timestamps": {
				"from": "01:14:41,000",
				"to": "01:14:48,000"
			},
			"offsets": {
				"from": 4481000,
				"to": 4488000
			},
			"text": " we track this in 1559 right now we usually track the base fee directly and then we updated every every block."
		},
		{
			"timestamps": {
				"from": "01:14:48,000",
				"to": "01:15:02,000"
			},
			"offsets": {
				"from": 4488000,
				"to": 4502000
			},
			"text": " And actually turned out after we introduced it like looking at it it's slightly conceptually ugly because we always do these these kind of services basically there are some some properties in the operating we don't quite love it's a little path dependent and these kind of things."
		},
		{
			"timestamps": {
				"from": "01:15:02,000",
				"to": "01:15:17,000"
			},
			"offsets": {
				"from": 4502000,
				"to": 4517000
			},
			"text": " So, so we moved to to just a conceptually simple way of tracking for this dimension where we track the excess excess data gas that has been basically been used over the existence of the IP right so basically we just we have some sort of target that we want to be used"
		},
		{
			"timestamps": {
				"from": "01:15:17,000",
				"to": "01:15:27,000"
			},
			"offsets": {
				"from": 4517000,
				"to": 4527000
			},
			"text": " basically every block if it basically uses more than that we just add to this those is counter and then every block is basically use less than that but we're still above zero in this counter we just reduce the counter."
		},
		{
			"timestamps": {
				"from": "01:15:27,000",
				"to": "01:15:33,000"
			},
			"offsets": {
				"from": 4527000,
				"to": 4533000
			},
			"text": " Yeah, sure if you want to."
		},
		{
			"timestamps": {
				"from": "01:15:33,000",
				"to": "01:15:36,000"
			},
			"offsets": {
				"from": 4533000,
				"to": 4536000
			},
			"text": " This is basically a header field just like the base fee."
		},
		{
			"timestamps": {
				"from": "01:15:36,000",
				"to": "01:15:41,000"
			},
			"offsets": {
				"from": 4536000,
				"to": 4541000
			},
			"text": " Yeah, so yeah, yeah, so this is one additional header fields."
		},
		{
			"timestamps": {
				"from": "01:15:41,000",
				"to": "01:15:58,000"
			},
			"offsets": {
				"from": 4541000,
				"to": 4558000
			},
			"text": " Which good question actually also I you can you can see that because I just wanted to give you a good impression of what the kind of calculating the cost looks like with this header field so as you can see basically we have these kind of mock functions."
		},
		{
			"timestamps": {
				"from": "01:15:58,000",
				"to": "01:16:06,000"
			},
			"offsets": {
				"from": 4558000,
				"to": 4566000
			},
			"text": " If you want to get the fee that intersection actually has to pay it depends on the header of the previous blocks in my 2.59."
		},
		{
			"timestamps": {
				"from": "01:16:06,000",
				"to": "01:16:20,000"
			},
			"offsets": {
				"from": 4566000,
				"to": 4580000
			},
			"text": " So you first get the total data gas that the transaction consumes which is just you know data gas per blob times the number of blocks and then you calculate the basically the base fee but we don't call it basically because again there's no tip so it's kind of unnecessary to have the base fee tip"
		},
		{
			"timestamps": {
				"from": "01:16:20,000",
				"to": "01:16:39,000"
			},
			"offsets": {
				"from": 4580000,
				"to": 4599000
			},
			"text": " so we just call it data gas price and so how do we basically for each block you once basically calculate its its data gas price and you do that by by basically taking in this excess data gas and then we use this take exponential function it's a little nice little tip bit"
		},
		{
			"timestamps": {
				"from": "01:16:39,000",
				"to": "01:17:04,000"
			},
			"offsets": {
				"from": 4599000,
				"to": 4624000
			},
			"text": " maybe it's irrelevant but it's it's so fun to talk about briefly so like just because we want we want so so maybe I can already go to the next side to explain so basically this is kind of how the pricing develops it's it's like 1559 right so basically if you were to continue to just keep keep basically using up all the data space in a block not just a target it would basically be on an exponential curve and would be more and more and more expensive and you can see basically"
		},
		{
			"timestamps": {
				"from": "01:17:04,000",
				"to": "01:17:17,000"
			},
			"offsets": {
				"from": 4624000,
				"to": 4637000
			},
			"text": " like a thousand thousand excess blobs that that's roughly I don't know some like 10 minutes or so so within 10 minutes you'd really like we have like super expensive blobs if it were to keep to keep being fully used"
		},
		{
			"timestamps": {
				"from": "01:17:17,000",
				"to": "01:17:40,000"
			},
			"offsets": {
				"from": 4637000,
				"to": 4660000
			},
			"text": " yeah I'll be honest again"
		},
		{
			"timestamps": {
				"from": "01:17:40,000",
				"to": "01:18:00,000"
			},
			"offsets": {
				"from": 4660000,
				"to": 4680000
			},
			"text": " so this the nice thing about this is that's basically pure function in in excess data gas right so it doesn't really matter if it was related to beginning at the end and there will probably be a different like in the beginning it will probably be relatively cheap to use data gas because the rollups are still kind of"
		},
		{
			"timestamps": {
				"from": "01:18:00,000",
				"to": "01:18:29,000"
			},
			"offsets": {
				"from": 4680000,
				"to": 4709000
			},
			"text": " of dropped in the pros of adopting it so there's not that much that much demand so basically probably for the first first month or so we'd be like in the very zoomed in left part of this picture and then later on once once it's all basically fully adopted and people use it we'll be like a bit more towards the right in this picture but it's not like this is not a basically because it's so reactive it's it's so much of it's unites basically every block and at most 12.5% update so the difference here like basically you can come you can go from one of these paradigms to the other one of these paradigms to the other one of these paradigms to the other one of these different things"
		},
		{
			"timestamps": {
				"from": "01:18:29,000",
				"to": "01:18:47,000"
			},
			"offsets": {
				"from": 4709000,
				"to": 4727000
			},
			"text": " so it's not like it's not basically something where it matters immensely what and what what was done in the in the past basically a lot a high consumption in the past only means that like basically you have like five five minutes of of reduced"
		},
		{
			"timestamps": {
				"from": "01:18:47,000",
				"to": "01:19:07,000"
			},
			"offsets": {
				"from": 4727000,
				"to": 4747000
			},
			"text": " Bob usage before you're back to your normal price level so it's not yeah so basically there's no significant kind of accumulation effect or anything right"
		},
		{
			"timestamps": {
				"from": "01:19:07,000",
				"to": "01:19:25,000"
			},
			"offsets": {
				"from": 4747000,
				"to": 4765000
			},
			"text": " should know but the thing is because so so to think about it it's like because the the price is a pure function of the excess data gas so at any excess data gas I mean of course I put down excess blobs just think about it more easily but it's tracked in excess data gas but once you reach something like say"
		},
		{
			"timestamps": {
				"from": "01:19:25,000",
				"to": "01:19:41,000"
			},
			"offsets": {
				"from": 4765000,
				"to": 4781000
			},
			"text": " I don't know a thousand excess excess blobs that would mean that sending one block already costs 30 and that doesn't matter whether the excess blobs were accumulated over one day of the accumulated over a year so basically once they excess"
		},
		{
			"timestamps": {
				"from": "01:19:41,000",
				"to": "01:20:10,000"
			},
			"offsets": {
				"from": 4781000,
				"to": 4810000
			},
			"text": " the data gas field reaches that value it would cost 30th per blob to send blobs so we would expect of course it robs are not willing to pay that much for blobs right so if for some weird reason there was some spike in demand and the excess would shoot up to that level it would quickly come back down and stay at some some kind of permanent level so the excess is not something that will continue to grow over time it'll just similar to the base fee the base fee doesn't grow over time it just finds its equilibrium value and of course sometimes goes up and sometimes goes down temperature"
		},
		{
			"timestamps": {
				"from": "01:20:10,000",
				"to": "01:20:37,000"
			},
			"offsets": {
				"from": 4810000,
				"to": 4837000
			},
			"text": " sometimes goes down temporarily but it has around some sort of you know 10 to 100g level rich and similarly the excess blobs because that can go back down right if a block uses less than a target the number goes back down so it will just find some sort of equilibrium value that corresponds to some sort of equilibrium price and it'll just have around that"
		},
		{
			"timestamps": {
				"from": "01:20:37,000",
				"to": "01:21:05,000"
			},
			"offsets": {
				"from": 4837000,
				"to": 4865000
			},
			"text": " I'll just keep keep I feel like we are out of time unfortunately for the for the sorry P market section and yeah come at me after for continued questions so anyway so basically this is how we how we make for work history girls not a big deal bandwidth we really need to put in work to make this work and the business kind of where the core innovation of the year P for now lies other than that it's what's compatible for full bank shouting"
		},
		{
			"timestamps": {
				"from": "01:21:05,000",
				"to": "01:21:22,000"
			},
			"offsets": {
				"from": 4865000,
				"to": 4882000
			},
			"text": " but for now this 2D a fee market is really why why we can do this and why we basically just utilize existing Ethereum resources more efficiently and with that I think we have done with the kind of the for for part of today and we can move to full down"
		},
		{
			"timestamps": {
				"from": "01:21:22,000",
				"to": "01:21:37,000"
			},
			"offsets": {
				"from": 4882000,
				"to": 4897000
			},
			"text": " Okay I want to introduce now to the two dimensional case g scheme which we will need for full shouting sorry this is a big jump okay so when we do full shouting"
		},
		{
			"timestamps": {
				"from": "01:21:37,000",
				"to": "01:22:00,000"
			},
			"offsets": {
				"from": 4897000,
				"to": 4920000
			},
			"text": " why do we not take all the data that we want to encode and put it into one big case g commitment and the reason for that is that that is going to require a super note like some powerful note that you probably can't easily run at home unless you like have a very good connection and want to invest some money into it"
		},
		{
			"timestamps": {
				"from": "01:22:00,000",
				"to": "01:22:12,000"
			},
			"offsets": {
				"from": 4920000,
				"to": 4932000
			},
			"text": " so you will need this both to construct blocks where we're probably like kind of okay with it but we will also need it to reconstruct the data in case there is a failure"
		},
		{
			"timestamps": {
				"from": "01:22:12,000",
				"to": "01:22:29,000"
			},
			"offsets": {
				"from": 4932000,
				"to": 4949000
			},
			"text": " and this is an assumption that we want to avoid for validity so like it's kind of more acceptable if a failure leads to just not being able to construct blocks or maybe we have to make smaller blocks or we have to make blocks without"
		},
		{
			"timestamps": {
				"from": "01:22:29,000",
				"to": "01:22:41,000"
			},
			"offsets": {
				"from": 4949000,
				"to": 4961000
			},
			"text": " charted data but it would be really bad if the absence of the super note could lead to the to a network split where some people think data is available and some people think it's not available"
		},
		{
			"timestamps": {
				"from": "01:22:41,000",
				"to": "01:22:54,000"
			},
			"offsets": {
				"from": 4961000,
				"to": 4974000
			},
			"text": " this is what we want to avoid so what we want is a construction where yes like there will be a lot of data in the network and maybe like someone needs to be there specialized into distributing that data but once they've done their job"
		},
		{
			"timestamps": {
				"from": "01:22:54,000",
				"to": "01:23:04,000"
			},
			"offsets": {
				"from": 4974000,
				"to": 4984000
			},
			"text": " the very decentralized network of maybe you Raspberry Pi is at home can guarantee that it will always converge it will always be safe and so on"
		},
		{
			"timestamps": {
				"from": "01:23:04,000",
				"to": "01:23:21,000"
			},
			"offsets": {
				"from": 4984000,
				"to": 5001000
			},
			"text": " okay so what if we just use many many different kzg commitments just a list of kzg commitments"
		},
		{
			"timestamps": {
				"from": "01:23:21,000",
				"to": "01:23:35,000"
			},
			"offsets": {
				"from": 5001000,
				"to": 5015000
			},
			"text": " so if we do this naively we just take many commitments and we sample from each then we'll need a lot of samples because we before had this number of samples for example say 30 samples now we need 30 samples per commitment"
		},
		{
			"timestamps": {
				"from": "01:23:35,000",
				"to": "01:23:52,000"
			},
			"offsets": {
				"from": 5015000,
				"to": 5032000
			},
			"text": " that would be a lot of samples but there's another much cooler way of doing this where we use read Solomon codes again and we extend m commitments for like m actual payload blobs and extend them to to m commitments"
		},
		{
			"timestamps": {
				"from": "01:23:52,000",
				"to": "01:24:08,000"
			},
			"offsets": {
				"from": 5032000,
				"to": 5048000
			},
			"text": " here's how this is going to work so we have our original data commitments in this case three commitments and what we'll do is we'll define another four commitments that are an extension of these commitments"
		},
		{
			"timestamps": {
				"from": "01:24:08,000",
				"to": "01:24:24,000"
			},
			"offsets": {
				"from": 5048000,
				"to": 5064000
			},
			"text": " so they will be completely determined by the actual data commitments and yes so here's the math of how this works so what we'll do is we'll define a two dimensional polynomial for the data"
		},
		{
			"timestamps": {
				"from": "01:24:24,000",
				"to": "01:24:40,000"
			},
			"offsets": {
				"from": 5064000,
				"to": 5080000
			},
			"text": " and it works the same way as before so basically we will interpolate this polynomial we will define it by this data region the original data that comes from many different transactions that included shotted data"
		},
		{
			"timestamps": {
				"from": "01:24:40,000",
				"to": "01:24:54,000"
			},
			"offsets": {
				"from": 5080000,
				"to": 5094000
			},
			"text": " and what we'll say is for simplicity others say the row K will just be the evaluation of this polynomial where we set y equals to the number of the row y equals K"
		},
		{
			"timestamps": {
				"from": "01:24:54,000",
				"to": "01:25:13,000"
			},
			"offsets": {
				"from": 5094000,
				"to": 5113000
			},
			"text": " so we evaluate the row the polynomial at K and then we get a one dimensional polynomial right so we get fk of X equals to this and like you can pull up all of this together and what do you get as again an expression just in this powers of X"
		},
		{
			"timestamps": {
				"from": "01:25:13,000",
				"to": "01:25:33,000"
			},
			"offsets": {
				"from": 5113000,
				"to": 5133000
			},
			"text": " and then we can commit to those polynomials in our normal KZG way okay so we have fk of S equals to this now we replace the X by S and some complicated sum in there but of all we have like one elliptical of element this black box evaluation"
		},
		{
			"timestamps": {
				"from": "01:25:33,000",
				"to": "01:25:50,000"
			},
			"offsets": {
				"from": 5133000,
				"to": 5150000
			},
			"text": " is a function of K okay now the cool thing is if you look at this expression as a function of K then this is also polynomial right it's just a sum of terms of powers of K"
		},
		{
			"timestamps": {
				"from": "01:25:50,000",
				"to": "01:26:07,000"
			},
			"offsets": {
				"from": 5150000,
				"to": 5167000
			},
			"text": " okay so this is very cool and what this means is that our commitments themselves will be on a polynomial so if we see the commitments which are now elliptical points as a function of K they are on a polynomial so what we have is before we started with having"
		},
		{
			"timestamps": {
				"from": "01:26:07,000",
				"to": "01:26:24,000"
			},
			"offsets": {
				"from": 5167000,
				"to": 5184000
			},
			"text": " each row being a polynomial that we commit to we also have that each row I mean this is a property of just a two dimensional binomial each column will be a polynomial but also the commitments themselves are a polynomial in this case of degree three"
		},
		{
			"timestamps": {
				"from": "01:26:24,000",
				"to": "01:26:53,000"
			},
			"offsets": {
				"from": 5184000,
				"to": 5213000
			},
			"text": " yeah because they're determined by these four commitments okay so what we have is how the 2D commitment scheme will work as we'll have two 2M row commitments and we can actually verify that this is the cool thing like any anyone who validates these commitments can easily verify"
		},
		{
			"timestamps": {
				"from": "01:26:53,000",
				"to": "01:27:11,000"
			},
			"offsets": {
				"from": 5213000,
				"to": 5231000
			},
			"text": " that they are on this polynomial using a random evaluation trick again which I introduced earlier so what we'll do is we'll take the first M commitments evaluate them at a random point and we'll do the same for the second M commitments"
		},
		{
			"timestamps": {
				"from": "01:27:11,000",
				"to": "01:27:23,000"
			},
			"offsets": {
				"from": 5231000,
				"to": 5243000
			},
			"text": " and if these two result in the same point actually the point will be in this case an elliptical point then they are actually on a polynomial of degree M minus 1"
		},
		{
			"timestamps": {
				"from": "01:27:23,000",
				"to": "01:27:40,000"
			},
			"offsets": {
				"from": 5243000,
				"to": 5260000
			},
			"text": " for those who are interested there is a way to do something very similar using 2D commitment so you can do one commitment to the whole thing but I won't go into details here but they're basically some downsides but which is why we're not choosing"
		},
		{
			"timestamps": {
				"from": "01:27:40,000",
				"to": "01:28:05,000"
			},
			"offsets": {
				"from": 5260000,
				"to": 5285000
			},
			"text": " that way cool and so what what's what why are we doing this okay so we have properties that we already know we can verify all samples directly against commitments there are no fraud proofs required but now we need a constant number of samples for all these commitments in order to get probabilistic data availability"
		},
		{
			"timestamps": {
				"from": "01:28:05,000",
				"to": "01:28:24,000"
			},
			"offsets": {
				"from": 5285000,
				"to": 5304000
			},
			"text": " and basically we get the property that if at least 75% of those samples are available then all the data is available and it can be reconstructed and that's the cool thing from validators or other nodes so only observe rows and columns so there's nobody"
		},
		{
			"timestamps": {
				"from": "01:28:24,000",
				"to": "01:28:37,000"
			},
			"offsets": {
				"from": 5304000,
				"to": 5317000
			},
			"text": " in the system will ever need or will be necessary I'm sure they will exist but it's not necessary that anyone watches the full square of samples in order to get these convergence properties"
		},
		{
			"timestamps": {
				"from": "01:28:37,000",
				"to": "01:28:49,000"
			},
			"offsets": {
				"from": 5317000,
				"to": 5329000
			},
			"text": " so what you'll notice is that this number is a bit higher than before so like if we only have one commitment then we only 50% of the samples to be available for the square we need 75%"
		},
		{
			"timestamps": {
				"from": "01:28:49,000",
				"to": "01:29:09,000"
			},
			"offsets": {
				"from": 5329000,
				"to": 5349000
			},
			"text": " so the number of samples you need to get this will be a bit higher cool and so what we get with this is that I made a proposal I mean this is also in discussions but like one of the ideas how we could extend this to a full charting construction is that"
		},
		{
			"timestamps": {
				"from": "01:29:09,000",
				"to": "01:29:30,000"
			},
			"offsets": {
				"from": 5349000,
				"to": 5370000
			},
			"text": " basically the way validators use this construction is that they will download rows and columns they will each choose to randomly of each and then what we get is that if a block is unavailable it can't get more than one sixteenths of"
		},
		{
			"timestamps": {
				"from": "01:29:30,000",
				"to": "01:29:50,000"
			},
			"offsets": {
				"from": 5370000,
				"to": 5390000
			},
			"text": " the data stations so automatically the consensus will never vote to unavailable blocks and at the same time they can use these full rows and columns that they download to reconstruct any incomplete rows or columns so if any samples are missing they"
		},
		{
			"timestamps": {
				"from": "01:29:50,000",
				"to": "01:30:05,000"
			},
			"offsets": {
				"from": 5390000,
				"to": 5405000
			},
			"text": " can't reconstruct this and because there will be some intersections like for each validator they will be if they do too like there will be these four intersections and they can see the orthogonal rows and columns with the samples that may be missing"
		},
		{
			"timestamps": {
				"from": "01:30:05,000",
				"to": "01:30:22,000"
			},
			"offsets": {
				"from": 5405000,
				"to": 5422000
			},
			"text": " and so like as an example I made a computation that basically with about 55,000 online validators you get a guaranteed reconstruction where basically every sample will always be reconstructed if like we initially had enough data available to do this"
		},
		{
			"timestamps": {
				"from": "01:30:22,000",
				"to": "01:30:41,000"
			},
			"offsets": {
				"from": 5422000,
				"to": 5441000
			},
			"text": " in practice this number will be much smaller because most notes don't run one by the data but tens and some even hundreds and data availability sampling is basically just checking like random samples on a square and what we want"
		},
		{
			"timestamps": {
				"from": "01:30:41,000",
				"to": "01:30:57,000"
			},
			"offsets": {
				"from": 5441000,
				"to": 5457000
			},
			"text": " as again we want to get that the probability that unavailable block passes is less than two to the minus 30 and if you do the math you find that you need about 75 random samples to do that and so the bandwidth to do that in this example if we do 512"
		},
		{
			"timestamps": {
				"from": "01:30:57,000",
				"to": "01:31:10,000"
			},
			"offsets": {
				"from": 5457000,
				"to": 5470000
			},
			"text": " and byte samples would be 2.5 kilobytes per second which is really nice low number cool okay handing it to Danny"
		},
		{
			"timestamps": {
				"from": "01:31:10,000",
				"to": "01:31:28,000"
			},
			"offsets": {
				"from": 5470000,
				"to": 5488000
			},
			"text": " okay so there's a lot of math and there's an elegant construction assuming that we can do a constant amount of work for a large amount of data to kind of layer it into it as similar to like a validity condition on our on our"
		},
		{
			"timestamps": {
				"from": "01:31:28,000",
				"to": "01:31:40,000"
			},
			"offsets": {
				"from": 5488000,
				"to": 5500000
			},
			"text": " we don't consider envelop blocks in our block tree we don't consider unavailable blocks in our block tree and so the math and the construction very elegant but in the upper reads the road"
		},
		{
			"timestamps": {
				"from": "01:31:40,000",
				"to": "01:31:53,000"
			},
			"offsets": {
				"from": 5500000,
				"to": 5513000
			},
			"text": " and the networking layer is actually a non-true problem that's the wrong way the arrow that goes right is so worn it doesn't look like an arrow anymore"
		},
		{
			"timestamps": {
				"from": "01:31:53,000",
				"to": "01:32:08,000"
			},
			"offsets": {
				"from": 5513000,
				"to": 5528000
			},
			"text": " okay so kind of stepping back why is this why we make this problem hard for ourselves everyone's seen this these things are not fundamental that they cannot come together scalability security and decentralization all in one system but it is hard"
		},
		{
			"timestamps": {
				"from": "01:32:08,000",
				"to": "01:32:22,000"
			},
			"offsets": {
				"from": 5528000,
				"to": 5542000
			},
			"text": " and it's hard primarily because we want home nodes to run we want standard computers to be able to validate the system to kind of have security and aggregate even against a malicious majority of our consensus participants"
		},
		{
			"timestamps": {
				"from": "01:32:22,000",
				"to": "01:32:34,000"
			},
			"offsets": {
				"from": 5542000,
				"to": 5554000
			},
			"text": " again kind of in that validity condition of if there's an invalid block and all the validators or miners are saying that's a that's what is that's what the head is you say well that's not even literally real because it is invalid"
		},
		{
			"timestamps": {
				"from": "01:32:34,000",
				"to": "01:32:50,000"
			},
			"offsets": {
				"from": 5554000,
				"to": 5570000
			},
			"text": " and so users in power kind of define what the network is similarly we want to do that with our bandwidth consideration with respect to data availability so thus we need to focus on the bandwidth here"
		},
		{
			"timestamps": {
				"from": "01:32:50,000",
				"to": "01:33:06,000"
			},
			"offsets": {
				"from": 5570000,
				"to": 5586000
			},
			"text": " a lot of this is a quick recap we've been talking about this all day but we need to scale execution we need to scale a data availability essentially rollups give us some sort of like compression algorithm for the execution of transactions whether it be from fraud proofs or"
		},
		{
			"timestamps": {
				"from": "01:33:06,000",
				"to": "01:33:36,000"
			},
			"offsets": {
				"from": 5586000,
				"to": 5616000
			},
			"text": " or we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to do that we want to"
		},
		{
			"timestamps": {
				"from": "01:33:36,000",
				"to": "01:33:50,000"
			},
			"offsets": {
				"from": 5616000,
				"to": 5630000
			},
			"text": " and we want those home notes it becomes very hard right so again we want the amount of work to not really scale as those blocks become very large to scale the network"
		},
		{
			"timestamps": {
				"from": "01:33:50,000",
				"to": "01:34:01,000"
			},
			"offsets": {
				"from": 5630000,
				"to": 5641000
			},
			"text": " so data availability share and sedate is not withheld also share and sedate is was published real quick shout out docrad made most of these slides for another talk and I'm just reusing them"
		},
		{
			"timestamps": {
				"from": "01:34:01,000",
				"to": "01:34:16,000"
			},
			"offsets": {
				"from": 5641000,
				"to": 5656000
			},
			"text": " important to note it's not data storage it's not continued availability there's a debate as to how long the network needs to have the data available so that people can check that it was made available"
		},
		{
			"timestamps": {
				"from": "01:34:16,000",
				"to": "01:34:31,000"
			},
			"offsets": {
				"from": 5656000,
				"to": 5671000
			},
			"text": " some people say on the order of where we at like 100 seconds some people say two weeks you know it kind of depends on the use case and it's a bit of a more of a UX debate it's kind of the online in this requirement of people to be able to get this security"
		},
		{
			"timestamps": {
				"from": "01:34:31,000",
				"to": "01:34:45,000"
			},
			"offsets": {
				"from": 5671000,
				"to": 5685000
			},
			"text": " without trusting you know someone else so is it important I don't think we need to get into this too much optimistic rollups and ZK rollups it's critically important and"
		},
		{
			"timestamps": {
				"from": "01:34:45,000",
				"to": "01:34:55,000"
			},
			"offsets": {
				"from": 5685000,
				"to": 5695000
			},
			"text": " you know who knows the utility of solving this problem might extend beyond these two types of systems so networking"
		},
		{
			"timestamps": {
				"from": "01:34:55,000",
				"to": "01:35:08,000"
			},
			"offsets": {
				"from": 5695000,
				"to": 5708000
			},
			"text": " and we probably are making it even harder on ourselves by some of our assumptions here so we could say okay we want we certainly want to make sure that block producer and consensus nodes we want to be able to not be fooled by a malicious"
		},
		{
			"timestamps": {
				"from": "01:35:08,000",
				"to": "01:35:15,000"
			},
			"offsets": {
				"from": 5708000,
				"to": 5715000
			},
			"text": " majority but maybe we have a neutral PDP network and we can just assume that PDP network is like healthy and gives us what we want."
		},
		{
			"timestamps": {
				"from": "01:35:15,000",
				"to": "01:35:31,000"
			},
			"offsets": {
				"from": 5715000,
				"to": 5731000
			},
			"text": " This is certainly attractive. It ensures that each node really can see that they get the statistical security but if we're assuming that the validators can be malicious it's very high amount of them, at least you know that two thirds"
		},
		{
			"timestamps": {
				"from": "01:35:31,000",
				"to": "01:35:55,000"
			},
			"offsets": {
				"from": 5731000,
				"to": 5755000
			},
			"text": " some people like to say 99% depends on probably the construction on what the real one is then the assumption then that the network network is neutral is probably not a realistic assumption so well maybe it's realistic in most scenarios but if we want to really be able to harden against that majority adversary we need to be thinking about"
		},
		{
			"timestamps": {
				"from": "01:35:55,000",
				"to": "01:36:11,000"
			},
			"offsets": {
				"from": 5755000,
				"to": 5771000
			},
			"text": " an attacker controlled PDP network by some threshold defining whatever that is. Again this is a lot of kind of exposition of the problem rather than total solutions of the problem so you know if I'm thinking about designing data availability sampling"
		},
		{
			"timestamps": {
				"from": "01:36:11,000",
				"to": "01:36:28,000"
			},
			"offsets": {
				"from": 5771000,
				"to": 5788000
			},
			"text": " I'm probably it's probably interesting to think about what's a good neutral network solution but then I think when the rubber meets the road we need to think about what thresholds can we actually harden against a very attacker controlled PDP network"
		},
		{
			"timestamps": {
				"from": "01:36:28,000",
				"to": "01:36:46,000"
			},
			"offsets": {
				"from": 5788000,
				"to": 5806000
			},
			"text": " So in this model certainly some notes can be fooled and so it ends up being a collective guarantee again depending on the thresholds and how the system is tuned but rather than no note can be fooled it's probably going to end up looking like no above certain threshold of node can be fooled"
		},
		{
			"timestamps": {
				"from": "01:36:46,000",
				"to": "01:36:56,000"
			},
			"offsets": {
				"from": 5806000,
				"to": 5816000
			},
			"text": " maybe for a certain period of time maybe until the network kind of resolves itself but so this is likely correct model but it does make the problem harder"
		},
		{
			"timestamps": {
				"from": "01:36:56,000",
				"to": "01:37:01,000"
			},
			"offsets": {
				"from": 5816000,
				"to": 5821000
			},
			"text": " So the PDP problem what are we trying to do here?"
		},
		{
			"timestamps": {
				"from": "01:37:01,000",
				"to": "01:37:10,000"
			},
			"offsets": {
				"from": 5821000,
				"to": 5830000
			},
			"text": " We want this like PDP distributed data structure that can reliably serve samples so that people can do their job of getting the samples."
		},
		{
			"timestamps": {
				"from": "01:37:10,000",
				"to": "01:37:25,000"
			},
			"offsets": {
				"from": 5830000,
				"to": 5845000
			},
			"text": " We want low overhead on notes from multiple perspectives one on notes that are participating in pulling down samples but also potentially we want to leverage nodes that are not just validators not just builders in this distributed PDP structure"
		},
		{
			"timestamps": {
				"from": "01:37:25,000",
				"to": "01:37:37,000"
			},
			"offsets": {
				"from": 5845000,
				"to": 5857000
			},
			"text": " So we want to also consider the overhead of these nodes that are participating in the serving of the samples as well or in the dissemination of the samples other things"
		},
		{
			"timestamps": {
				"from": "01:37:37,000",
				"to": "01:37:48,000"
			},
			"offsets": {
				"from": 5857000,
				"to": 5868000
			},
			"text": " One of your rusticants attacks I think one of the really really scary things here is liveness attacks, dosses, civil attacks etc. that happen on the network layer"
		},
		{
			"timestamps": {
				"from": "01:37:48,000",
				"to": "01:37:58,000"
			},
			"offsets": {
				"from": 5868000,
				"to": 5878000
			},
			"text": " because if a majority of nodes are seeing data as unavailable either temporarily or permanently then they cannot follow the chain at all"
		},
		{
			"timestamps": {
				"from": "01:37:58,000",
				"to": "01:38:05,000"
			},
			"offsets": {
				"from": 5878000,
				"to": 5885000
			},
			"text": " Again we want this to be essentially of validity condition you know if there's an invalid transaction in this branch I don't follow the branch"
		},
		{
			"timestamps": {
				"from": "01:38:05,000",
				"to": "01:38:15,000"
			},
			"offsets": {
				"from": 5885000,
				"to": 5895000
			},
			"text": " If that branch is unavailable I don't follow the branch so that is a very important critical requirement but a very terrifying requirement"
		},
		{
			"timestamps": {
				"from": "01:38:15,000",
				"to": "01:38:29,000"
			},
			"offsets": {
				"from": 5895000,
				"to": 5909000
			},
			"text": " Meaning that like it is very important that these PDP structures are hardened and we do understand their failure modes we understand where they operate and we do understand how they resolve maybe after an attack"
		},
		{
			"timestamps": {
				"from": "01:38:29,000",
				"to": "01:38:35,000"
			},
			"offsets": {
				"from": 5909000,
				"to": 5915000
			},
			"text": " And low latency on the order of seconds I have a page of some desiderato we can get into in a second"
		},
		{
			"timestamps": {
				"from": "01:38:35,000",
				"to": "01:38:44,000"
			},
			"offsets": {
				"from": 5915000,
				"to": 5924000
			},
			"text": " And there's some distinct challenges I think when you're kind of thinking about this problem dissemination into the PDP structure we have a lot of data"
		},
		{
			"timestamps": {
				"from": "01:38:44,000",
				"to": "01:38:52,000"
			},
			"offsets": {
				"from": 5924000,
				"to": 5932000
			},
			"text": " How do you efficiently get it into this PDP structure without causing high load on the individual nodes of the PDP structure"
		},
		{
			"timestamps": {
				"from": "01:38:52,000",
				"to": "01:39:00,000"
			},
			"offsets": {
				"from": 5932000,
				"to": 5940000
			},
			"text": " So if every node only needs you know 1/100 of the data but they had to touch 50% of the data to get it disseminated the structure"
		},
		{
			"timestamps": {
				"from": "01:39:00,000",
				"to": "01:39:11,000"
			},
			"offsets": {
				"from": 5940000,
				"to": 5951000
			},
			"text": " And they're kind of missing something there. Similarly we want to support queries and disseminated data sample for x amount of time which I can get into this desiderato again"
		},
		{
			"timestamps": {
				"from": "01:39:11,000",
				"to": "01:39:23,000"
			},
			"offsets": {
				"from": 5951000,
				"to": 5963000
			},
			"text": " And validators certainly with their row and column kind of cryptoeconomic duty can identify and reconstruct missing data but we also probably want to consider"
		},
		{
			"timestamps": {
				"from": "01:39:23,000",
				"to": "01:39:35,000"
			},
			"offsets": {
				"from": 5963000,
				"to": 5975000
			},
			"text": " Should this PDP structure be able to identify and reconstruct missing data. So there's two kinds of potential reconstruction that we might want. So validators are very incentivized out the gate"
		},
		{
			"timestamps": {
				"from": "01:39:35,000",
				"to": "01:39:49,000"
			},
			"offsets": {
				"from": 5975000,
				"to": 5989000
			},
			"text": " You know if things are missing from the rows and columns to incentivize to repair patch and make things whole but if say the PDP structure is supposed to serve data availability sampling for one week"
		},
		{
			"timestamps": {
				"from": "01:39:49,000",
				"to": "01:40:01,000"
			},
			"offsets": {
				"from": 5989000,
				"to": 6001000
			},
			"text": " Then are those validators the same people that will then identify and reconstruct missing data or is there some other more distributed and less timely required method to do so"
		},
		{
			"timestamps": {
				"from": "01:40:01,000",
				"to": "01:40:11,000"
			},
			"offsets": {
				"from": 6001000,
				"to": 6011000
			},
			"text": " There's a hand-billed actors involved in data availability sampling for investors is going to talk about builders and where they fit into the kind of the consensus protocol but they're kind of the original source of the data"
		},
		{
			"timestamps": {
				"from": "01:40:11,000",
				"to": "01:40:25,000"
			},
			"offsets": {
				"from": 6011000,
				"to": 6025000
			},
			"text": " They're highly incentivized to get it out but they're probably not one that you'd want to rely on in perpetuity validators highly these are you know, cryptoeconomically incentivized actors that we can try to leverage in this construction"
		},
		{
			"timestamps": {
				"from": "01:40:25,000",
				"to": "01:40:40,000"
			},
			"offsets": {
				"from": 6025000,
				"to": 6040000
			},
			"text": " They do have the rows and columns. They do also perform data availability sampling like a user node and then we have users. Users perform data availability sampling. Hopefully they can be leveraged in serving and making the whole PDP also more resilient itself"
		},
		{
			"timestamps": {
				"from": "01:40:40,000",
				"to": "01:40:54,000"
			},
			"offsets": {
				"from": 6040000,
				"to": 6054000
			},
			"text": " Some quick to sitarata right now, you know, if I were thinking about building data a little bit sampling if I'm researching and doing stuff. I'm these are kind of some target numbers but I would also be sweeping these numbers and understanding where they"
		},
		{
			"timestamps": {
				"from": "01:40:54,000",
				"to": "01:41:06,000"
			},
			"offsets": {
				"from": 6054000,
				"to": 6066000
			},
			"text": " Where they work and where they don't so data size 32 megabytes per block. That's per 12 seconds or if the slot time were adjusted might be per some other amount of seconds go at 16 or 20"
		},
		{
			"timestamps": {
				"from": "01:41:06,000",
				"to": "01:41:20,000"
			},
			"offsets": {
				"from": 6066000,
				"to": 6080000
			},
			"text": " But with the 2D ratio coding that ends up being 128 megabytes of data being disseminated into the network chunks. I think we there's chunks and we sampled the chunks or were there samples and we sampled the samples"
		},
		{
			"timestamps": {
				"from": "01:41:20,000",
				"to": "01:41:35,000"
			},
			"offsets": {
				"from": 6080000,
				"to": 6095000
			},
			"text": " But on the order of 250,000 you can make these larger but then you end up with you still need the same constant number of samples. So you end up with more overhead samples. He said 75 something on that order but essentially want to drive that probability down"
		},
		{
			"timestamps": {
				"from": "01:41:35,000",
				"to": "01:41:55,000"
			},
			"offsets": {
				"from": 6095000,
				"to": 6115000
			},
			"text": " As we're doing the sampling latency validators really right now need to make decisions about what they see is the valid and available head on the order of four seconds. That could be tuned depending on the construction available to us. But they if they could not regularly be able to do data"
		},
		{
			"timestamps": {
				"from": "01:41:55,000",
				"to": "01:42:17,000"
			},
			"offsets": {
				"from": 6115000,
				"to": 6137000
			},
			"text": " But the sampling then on the order of four seconds we have a problem users. You could have a potentially lack more lacks requirement on the order of 12 seconds on the order of a slot. Or you could even consider maybe they needed to be doing it on the order of epochs and optimistically following head as available and maybe there's some play in the constructions there"
		},
		{
			"timestamps": {
				"from": "01:42:17,000",
				"to": "01:42:37,000"
			},
			"offsets": {
				"from": 6137000,
				"to": 6157000
			},
			"text": " Validator nodes 100k is pretty optimistic but we probably have in the order of 4000 today. So something on that order is kind of the baseline. And then user nodes on the order of 10 years, especially we start adding light lighter weight nodes with statelessness and light clients that might want to participate in this data bill, but the sampling"
		},
		{
			"timestamps": {
				"from": "01:42:37,000",
				"to": "01:42:55,000"
			},
			"offsets": {
				"from": 6157000,
				"to": 6175000
			},
			"text": " You know 100k to a million user nodes, you know, so it's really if the user nodes cannot participate in the serving of samples, then the load on if we only relied on say it's set of eyes actors like validators, then the load would actually scale as the"
		},
		{
			"timestamps": {
				"from": "01:42:55,000",
				"to": "01:43:08,000"
			},
			"offsets": {
				"from": 6175000,
				"to": 6188000
			},
			"text": " The if to serve as the user node serve. So it's probably very important to tie them into the data structure itself. The amount of assumption. I don't know. It's probably worth discussing the"
		},
		{
			"timestamps": {
				"from": "01:43:08,000",
				"to": "01:43:37,000"
			},
			"offsets": {
				"from": 6188000,
				"to": 6217000
			},
			"text": " If that org website suggests a minimum of 10 megabytes per second around a full node, but but for good whatever 25 megabytes per second. I don't know who came up with that number. Maybe it's a good place to start the conversation. And then persistence. Obviously, like I said, data bill be sampling is not for persistence. It's to ensure that data is made available. Where how but you know data if data was made available for half a second like no one's necessarily be able to"
		},
		{
			"timestamps": {
				"from": "01:43:37,000",
				"to": "01:44:03,000"
			},
			"offsets": {
				"from": 6217000,
				"to": 6243000
			},
			"text": " To prove that to themselves that it was made available or very small subset. So is it two epochs is it two weeks? There's much debate here on scar. I think what was your recent numbers. You still here? Okay. Okay. Okay. 10 minutes an hour. Whereas I think some are more like a week two weeks. And those that actually changes the requirements on nodes, especially in terms of storage."
		},
		{
			"timestamps": {
				"from": "01:44:03,000",
				"to": "01:44:25,000"
			},
			"offsets": {
				"from": 6243000,
				"to": 6265000
			},
			"text": " My intuition here is that the online industry requirement for users that want to get their, you know, state transition changes from ZK roll ups or policemen users that want to submit fraud proofs for ZK for optimistic roll ups. You know, this dictates their on in line in this requirement. And so I'm."
		},
		{
			"timestamps": {
				"from": "01:44:25,000",
				"to": "01:44:31,000"
			},
			"offsets": {
				"from": 6265000,
				"to": 6271000
			},
			"text": " I feel like 10 minute. Oh, man, I gotta get out of here. Okay, cool. So debate."
		},
		{
			"timestamps": {
				"from": "01:44:31,000",
				"to": "01:44:46,000"
			},
			"offsets": {
				"from": 6271000,
				"to": 6286000
			},
			"text": " And our seems short. P2P designs. So one easy thing you could do is just say there's a bunch of super nodes on network. And if you connect to them, you do it DIS. And if they give you the samples that you want, then things are available."
		},
		{
			"timestamps": {
				"from": "01:44:46,000",
				"to": "01:44:55,000"
			},
			"offsets": {
				"from": 6286000,
				"to": 6295000
			},
			"text": " This is I believe Celeste is current design, although that statement I could claim is true a few months ago. I'm not sure today."
		},
		{
			"timestamps": {
				"from": "01:44:55,000",
				"to": "01:45:09,000"
			},
			"offsets": {
				"from": 6295000,
				"to": 6309000
			},
			"text": " And you could potentially do something in similar theorem, whereas maybe instead of a each node, meaning you have everything, you could leverage a theorem validators, the rows and columns that they custody."
		},
		{
			"timestamps": {
				"from": "01:45:09,000",
				"to": "01:45:12,000"
			},
			"offsets": {
				"from": 6309000,
				"to": 6312000
			},
			"text": " And it looks kind of similar."
		},
		{
			"timestamps": {
				"from": "01:45:12,000",
				"to": "01:45:15,000"
			},
			"offsets": {
				"from": 6312000,
				"to": 6315000
			},
			"text": " This is nice."
		},
		{
			"timestamps": {
				"from": "01:45:15,000",
				"to": "01:45:20,000"
			},
			"offsets": {
				"from": 6315000,
				"to": 6320000
			},
			"text": " You know, if you connect to one on a super node, then you get what you need."
		},
		{
			"timestamps": {
				"from": "01:45:20,000",
				"to": "01:45:35,000"
			},
			"offsets": {
				"from": 6320000,
				"to": 6335000
			},
			"text": " But this doesn't really fit well into the node model, especially if validators, you know, a node that's running on the order of one to maybe three validators should be able to run on the order of, you know, home resources, which is definitely not the case."
		},
		{
			"timestamps": {
				"from": "01:45:35,000",
				"to": "01:45:50,000"
			},
			"offsets": {
				"from": 6335000,
				"to": 6350000
			},
			"text": " They all of a sudden, DHT is nice way to distribute data and attributed data structure across the network. It's a nice way to find data and seems intuitively like a very good direction or very good start."
		},
		{
			"timestamps": {
				"from": "01:45:50,000",
				"to": "01:46:07,000"
			},
			"offsets": {
				"from": 6350000,
				"to": 6367000
			},
			"text": " If it's really well in because each of these notes can have very small amount of data and really nice scalability as you add more nodes to network, you can, depending on your redundancy factor, you can have, you know, similar or less data per node."
		},
		{
			"timestamps": {
				"from": "01:46:07,000",
				"to": "01:46:22,000"
			},
			"offsets": {
				"from": 6367000,
				"to": 6382000
			},
			"text": " Prone to lightness attacks, it's really easy to civil this thing, and naively, you just make note IDs, you fill the tables. And if you're a malicious node, you can just return entries from your table that are full of malicious nodes."
		},
		{
			"timestamps": {
				"from": "01:46:22,000",
				"to": "01:46:33,000"
			},
			"offsets": {
				"from": 6382000,
				"to": 6393000
			},
			"text": " And one thing that's I think very promising is looking at secure DHTs, Docker's been digging to us, and I believe there may be some others in this room that have looked at some other papers about hardened DHTs."
		},
		{
			"timestamps": {
				"from": "01:46:33,000",
				"to": "01:46:52,000"
			},
			"offsets": {
				"from": 6393000,
				"to": 6412000
			},
			"text": " And we do have, as long as you have a simple resistance set, then you all of a sudden can have certain guarantees in these constructions. So you can leverage the validator set, or maybe other types of crypto economic sets to have hardened DHTs."
		},
		{
			"timestamps": {
				"from": "01:46:52,000",
				"to": "01:47:14,000"
			},
			"offsets": {
				"from": 6412000,
				"to": 6434000
			},
			"text": " So, you could use standard open DHT for average case performance and maybe a secondary fallback DHT, leveraging the validator set for in case of attack. You could also, there's some weirdness because then all of a sudden you're assuming that you have a certain amount of honest validators for this."
		},
		{
			"timestamps": {
				"from": "01:47:14,000",
				"to": "01:47:30,000"
			},
			"offsets": {
				"from": 6434000,
				"to": 6450000
			},
			"text": " So, does that suffice under the malicious majority construction? Sure, you can probably tune the numbers, but you could also potentially layer other types of crypto economic sets, you know proof of humanity, spruce ID, whatever the hell, all sorts of stuff,"
		},
		{
			"timestamps": {
				"from": "01:47:30,000",
				"to": "01:47:38,000"
			},
			"offsets": {
				"from": 6450000,
				"to": 6458000
			},
			"text": " and could have layered DHTs where they're ultimately just kind of fallbacks in the event that the big main DHT starts failing."
		},
		{
			"timestamps": {
				"from": "01:47:38,000",
				"to": "01:47:50,000"
			},
			"offsets": {
				"from": 6458000,
				"to": 6470000
			},
			"text": " So, I'm about a variety of privacy and optionality and how they construction and note setups is probably very important. I'm definitely over time. Okay, cool, great."
		},
		{
			"timestamps": {
				"from": "01:47:50,000",
				"to": "01:48:08,000"
			},
			"offsets": {
				"from": 6470000,
				"to": 6488000
			},
			"text": " I'm Francesco and I'll cover the last bit of this very large topic that we've kind of gone over today. It's proposal separation. I expect probably most people will be somewhat familiar with the concept, but this will be kind of a light introduction like it's not going to be"
		},
		{
			"timestamps": {
				"from": "01:48:08,000",
				"to": "01:48:21,000"
			},
			"offsets": {
				"from": 6488000,
				"to": 6501000
			},
			"text": " to advance it's going to be just for you to get a picture of how does it fit with done charting and what does it have to do with it in general and also like kind of how does the roadmap of that fit in the protocol look like."
		},
		{
			"timestamps": {
				"from": "01:48:21,000",
				"to": "01:48:25,000"
			},
			"offsets": {
				"from": 6501000,
				"to": 6505000
			},
			"text": " So, first of all, what is PBS? It's."
		},
		{
			"timestamps": {
				"from": "01:48:25,000",
				"to": "01:48:43,000"
			},
			"offsets": {
				"from": 6505000,
				"to": 6523000
			},
			"text": " Oh, sorry. Yeah, so there's a let's start from the pieces. We have DB and so first of all, block building the B is essentially this task of actually creating and distributing execution payloads mainly so we have become blocks, but then inside them there is execution payload,"
		},
		{
			"timestamps": {
				"from": "01:48:43,000",
				"to": "01:48:58,000"
			},
			"offsets": {
				"from": 6523000,
				"to": 6538000
			},
			"text": " which is the kind of the valuable part in some sense the part that actually changes the state in of the execution layer and this is the part that is kind of requires some special specialization to deal with, whereas the become block parts more of a consensus part."
		},
		{
			"timestamps": {
				"from": "01:48:58,000",
				"to": "01:49:14,000"
			},
			"offsets": {
				"from": 6538000,
				"to": 6554000
			},
			"text": " And yeah, so this is the normally today we only think about the creating part like only basically putting together a new execution payload, but the distribution part will also become critical, especially in the, well, in the context of done charting."
		},
		{
			"timestamps": {
				"from": "01:49:14,000",
				"to": "01:49:27,000"
			},
			"offsets": {
				"from": 6554000,
				"to": 6567000
			},
			"text": " And, and also this distribute so the distribution will involve the data that is committed to, which is going to be eventually very large. So that's why it's kind of important task eventually."
		},
		{
			"timestamps": {
				"from": "01:49:27,000",
				"to": "01:49:40,000"
			},
			"offsets": {
				"from": 6567000,
				"to": 6580000
			},
			"text": " And so for these reasons and, well, later, it will get a bit more into them. It's quite specialized activity that we don't really want normal validators to do because it would kind of increase their requirements too much for us to be comfortable with."
		},
		{
			"timestamps": {
				"from": "01:49:40,000",
				"to": "01:49:58,000"
			},
			"offsets": {
				"from": 6580000,
				"to": 6598000
			},
			"text": " And then there's proposing. So this is just, you could think of today proposing includes both things, both this kind of consensus part of making a become block and including all the consensus messages in it, attestations and other things like slash messages or anything that's kind of critical to the good function of the"
		},
		{
			"timestamps": {
				"from": "01:49:58,000",
				"to": "01:50:08,000"
			},
			"offsets": {
				"from": 6598000,
				"to": 6608000
			},
			"text": " But then also the putting an execution payload in it so today it's still possible for anyone to do this by themselves and kind of have both the roles together."
		},
		{
			"timestamps": {
				"from": "01:50:08,000",
				"to": "01:50:21,000"
			},
			"offsets": {
				"from": 6608000,
				"to": 6621000
			},
			"text": " But if we kind of ignore this execution payload part, this is really not a particular specialized role and be thing that it's always going to be possible to really want this to always be possible with it lower requirements."
		},
		{
			"timestamps": {
				"from": "01:50:21,000",
				"to": "01:50:25,000"
			},
			"offsets": {
				"from": 6621000,
				"to": 6625000
			},
			"text": " Basically what we expect today, a value to have."
		},
		{
			"timestamps": {
				"from": "01:50:25,000",
				"to": "01:50:29,000"
			},
			"offsets": {
				"from": 6625000,
				"to": 6629000
			},
			"text": " And yeah, the separation is just that these two things are split up like we don't."
		},
		{
			"timestamps": {
				"from": "01:50:29,000",
				"to": "01:50:41,000"
			},
			"offsets": {
				"from": 6629000,
				"to": 6641000
			},
			"text": " The default would not be any more that a validator does both things or the proposal, which is a validator does both things but that the proposal does the become block relevant part of the consensus messages."
		},
		{
			"timestamps": {
				"from": "01:50:41,000",
				"to": "01:50:50,000"
			},
			"offsets": {
				"from": 6641000,
				"to": 6650000
			},
			"text": " Part and some other kind of specialized actor comes in with the execution payload and distribution of the data eventually."
		},
		{
			"timestamps": {
				"from": "01:50:50,000",
				"to": "01:51:00,000"
			},
			"offsets": {
				"from": 6650000,
				"to": 6660000
			},
			"text": " And yeah, so why do you want to do this? I've kind of already hinted at it, but it's simply that if we outsource the specialized stuff, we can keep the simple stuff."
		},
		{
			"timestamps": {
				"from": "01:51:00,000",
				"to": "01:51:05,000"
			},
			"offsets": {
				"from": 6660000,
				"to": 6665000
			},
			"text": " Basically decentralized, we can keep the really consensus critical things."
		},
		{
			"timestamps": {
				"from": "01:51:05,000",
				"to": "01:51:13,000"
			},
			"offsets": {
				"from": 6665000,
				"to": 6673000
			},
			"text": " Essentially done by a very decentralized value they're set, which is a really important goal in Ethereum in general."
		},
		{
			"timestamps": {
				"from": "01:51:13,000",
				"to": "01:51:19,000"
			},
			"offsets": {
				"from": 6673000,
				"to": 6679000
			},
			"text": " So, and I mean practically why, you know, what are these things that want outsource."
		},
		{
			"timestamps": {
				"from": "01:51:19,000",
				"to": "01:51:30,000"
			},
			"offsets": {
				"from": 6679000,
				"to": 6690000
			},
			"text": " So there we've for all the whole day, we've been talking about done sharding and it's not there's nothing really, I guess, fundamental about sharding that requires this outsourcing."
		},
		{
			"timestamps": {
				"from": "01:51:30,000",
				"to": "01:51:38,000"
			},
			"offsets": {
				"from": 6690000,
				"to": 6698000
			},
			"text": " You could imagine other models. I mean, the I guess original sharding model before the tank part didn't require this outsourcing."
		},
		{
			"timestamps": {
				"from": "01:51:38,000",
				"to": "01:51:56,000"
			},
			"offsets": {
				"from": 6698000,
				"to": 6716000
			},
			"text": " But it's really like a major simplification. And so I'm not just simplification also as I think like consequences for latency, like it just makes the gives us this really tight coupling between the execution payload, the blobs and kind of"
		},
		{
			"timestamps": {
				"from": "01:51:56,000",
				"to": "01:52:14,000"
			},
			"offsets": {
				"from": 6716000,
				"to": 6734000
			},
			"text": " just streamlines the whole process. And so we then sharding, if we do want these simplifications, we kind of have to, we start having something to outsource because the proposal has to compute these commitments really quickly, which is not easy to do for like normal hardware."
		},
		{
			"timestamps": {
				"from": "01:52:14,000",
				"to": "01:52:31,000"
			},
			"offsets": {
				"from": 6734000,
				"to": 6751000
			},
			"text": " And also like probably the most prohibited part is the basically distribution of the data to the network so that would require like really kind of not acceptable upstream requirements for validators like more than probably multiple gigabits."
		},
		{
			"timestamps": {
				"from": "01:52:31,000",
				"to": "01:52:46,000"
			},
			"offsets": {
				"from": 6751000,
				"to": 6766000
			},
			"text": " And so yeah, we don't want to require this. It's like, or there's a magnitude more than what someone would need today, because basically the most you might need to distribute is 128 megabytes per block."
		},
		{
			"timestamps": {
				"from": "01:52:46,000",
				"to": "01:53:00,000"
			},
			"offsets": {
				"from": 6766000,
				"to": 6780000
			},
			"text": " And yeah, this is not a kind of fundamental reason if there was no other reason that we needed the separation for, we might be a bit more skeptical about non charity we might think well you know, we don't need these other actors."
		},
		{
			"timestamps": {
				"from": "01:53:00,000",
				"to": "01:53:09,000"
			},
			"offsets": {
				"from": 6780000,
				"to": 6789000
			},
			"text": " Why are we introducing the system just to get the simplification that's not kind of the ethos of Ethereum like we really want everything to be as decentralized as possible as like resilient as possible."
		},
		{
			"timestamps": {
				"from": "01:53:09,000",
				"to": "01:53:14,000"
			},
			"offsets": {
				"from": 6789000,
				"to": 6794000
			},
			"text": " These actors probably, you know, do introduce some complexities in this vision."
		},
		{
			"timestamps": {
				"from": "01:53:14,000",
				"to": "01:53:21,000"
			},
			"offsets": {
				"from": 6794000,
				"to": 6801000
			},
			"text": " But the issue is done charting isn't the reason why we introduce the actors. The reason is M.E.D. and this kind of fundamental reason."
		},
		{
			"timestamps": {
				"from": "01:53:21,000",
				"to": "01:53:29,000"
			},
			"offsets": {
				"from": 6801000,
				"to": 6809000
			},
			"text": " There's, I don't think anyone that has looked into me enough things that there is any other way essentially to go."
		},
		{
			"timestamps": {
				"from": "01:53:29,000",
				"to": "01:53:35,000"
			},
			"offsets": {
				"from": 6809000,
				"to": 6815000
			},
			"text": " And the issue is simply that, as I said, these execution payloads are really valuable."
		},
		{
			"timestamps": {
				"from": "01:53:35,000",
				"to": "01:53:49,000"
			},
			"offsets": {
				"from": 6815000,
				"to": 6829000
			},
			"text": " And extracting value from them is a really sophisticated activity from many points of view algorithmic infrastructure like it requires potentially very good hardware, very good connection like latency is really important."
		},
		{
			"timestamps": {
				"from": "01:53:49,000",
				"to": "01:53:53,000"
			},
			"offsets": {
				"from": 6829000,
				"to": 6833000
			},
			"text": " So there's like all kinds of reasons out and also like access to order flow."
		},
		{
			"timestamps": {
				"from": "01:53:53,000",
				"to": "01:54:06,000"
			},
			"offsets": {
				"from": 6833000,
				"to": 6846000
			},
			"text": " You know, today we can think that order flow is more or less so you know, essentially access to memple transactions is more or less available to everyone publicly, but that's, it seems very naive to assume that that's going to be the case in the future."
		},
		{
			"timestamps": {
				"from": "01:54:06,000",
				"to": "01:54:09,000"
			},
			"offsets": {
				"from": 6846000,
				"to": 6849000
			},
			"text": " And already it's not quite true that that's the case."
		},
		{
			"timestamps": {
				"from": "01:54:09,000",
				"to": "01:54:21,000"
			},
			"offsets": {
				"from": 6849000,
				"to": 6861000
			},
			"text": " So maybe they're always going to be a public memple for sensory resistance reasons for, I mean, other reasons, but it's really naive to think that everyone is going to have access to the same kind of raw material to build blocks like the transactions."
		},
		{
			"timestamps": {
				"from": "01:54:21,000",
				"to": "01:54:26,000"
			},
			"offsets": {
				"from": 6861000,
				"to": 6866000
			},
			"text": " And this access to order flow is a huge part of being able to create valuable payloads."
		},
		{
			"timestamps": {
				"from": "01:54:26,000",
				"to": "01:54:34,000"
			},
			"offsets": {
				"from": 6866000,
				"to": 6874000
			},
			"text": " So there's like all kinds of reasons why it's just not realistic to think that validators will be able to profitably make their own blocks."
		},
		{
			"timestamps": {
				"from": "01:54:34,000",
				"to": "01:54:37,000"
			},
			"offsets": {
				"from": 6874000,
				"to": 6877000
			},
			"text": " And so there's this really like strong centralization pressures."
		},
		{
			"timestamps": {
				"from": "01:54:37,000",
				"to": "01:54:42,000"
			},
			"offsets": {
				"from": 6877000,
				"to": 6882000
			},
			"text": " If we essentially don't provide them a way to do it, you just go and you don't have someone else to do it."
		},
		{
			"timestamps": {
				"from": "01:54:42,000",
				"to": "01:54:46,000"
			},
			"offsets": {
				"from": 6882000,
				"to": 6886000
			},
			"text": " Well, which is the whole point of separation, but there's different ways in which it could happen."
		},
		{
			"timestamps": {
				"from": "01:54:46,000",
				"to": "01:54:54,000"
			},
			"offsets": {
				"from": 6886000,
				"to": 6894000
			},
			"text": " Some ways in which it could happen are, for example, just everyone's taking with pools because that's the only way that they can extract value."
		},
		{
			"timestamps": {
				"from": "01:54:54,000",
				"to": "01:55:00,000"
			},
			"offsets": {
				"from": 6894000,
				"to": 6900000
			},
			"text": " Although that's actually kind of not already, it seems like a scenario that in some sense, maybe we can avoid."
		},
		{
			"timestamps": {
				"from": "01:55:00,000",
				"to": "01:55:12,000"
			},
			"offsets": {
				"from": 6900000,
				"to": 6912000
			},
			"text": " We already have PBS today, like we usually say PBS and we mean basically in protocol PBS, where the protocol kind of knows about this separation, like has a concept of builder."
		},
		{
			"timestamps": {
				"from": "01:55:12,000",
				"to": "01:55:17,000"
			},
			"offsets": {
				"from": 6912000,
				"to": 6917000
			},
			"text": " And in some sense, like negotiates this outsourcing."
		},
		{
			"timestamps": {
				"from": "01:55:17,000",
				"to": "01:55:20,000"
			},
			"offsets": {
				"from": 6917000,
				"to": 6920000
			},
			"text": " But today we basically have PBS is just not in protocol."
		},
		{
			"timestamps": {
				"from": "01:55:20,000",
				"to": "01:55:21,000"
			},
			"offsets": {
				"from": 6920000,
				"to": 6921000
			},
			"text": " It's called a Mav Boost."
		},
		{
			"timestamps": {
				"from": "01:55:21,000",
				"to": "01:55:24,000"
			},
			"offsets": {
				"from": 6921000,
				"to": 6924000
			},
			"text": " Maybe probably a lot of you know it."
		},
		{
			"timestamps": {
				"from": "01:55:24,000",
				"to": "01:55:31,000"
			},
			"offsets": {
				"from": 6924000,
				"to": 6931000
			},
			"text": " And essentially what it does is it introduces a trusted third party in between a builder and proposal, which are these relayers."
		},
		{
			"timestamps": {
				"from": "01:55:31,000",
				"to": "01:55:38,000"
			},
			"offsets": {
				"from": 6931000,
				"to": 6938000
			},
			"text": " I don't think I have time to like go into the details of it, but essentially, you know, we don't, we want builders to not trust proposal."
		},
		{
			"timestamps": {
				"from": "01:55:38,000",
				"to": "01:55:40,000"
			},
			"offsets": {
				"from": 6938000,
				"to": 6940000
			},
			"text": " We want proposal to not trust builders."
		},
		{
			"timestamps": {
				"from": "01:55:40,000",
				"to": "01:55:42,000"
			},
			"offsets": {
				"from": 6940000,
				"to": 6942000
			},
			"text": " There's reasons for that."
		},
		{
			"timestamps": {
				"from": "01:55:42,000",
				"to": "01:55:48,000"
			},
			"offsets": {
				"from": 6942000,
				"to": 6948000
			},
			"text": " And yeah, we just basically put like a trust to the third party in the middle, which kind of negotiates the exchange."
		},
		{
			"timestamps": {
				"from": "01:55:48,000",
				"to": "01:55:51,000"
			},
			"offsets": {
				"from": 6948000,
				"to": 6951000
			},
			"text": " So, you know, the proposal wants something they're built from the builder."
		},
		{
			"timestamps": {
				"from": "01:55:51,000",
				"to": "01:55:53,000"
			},
			"offsets": {
				"from": 6951000,
				"to": 6953000
			},
			"text": " The builder wants to get something to the proposal."
		},
		{
			"timestamps": {
				"from": "01:55:53,000",
				"to": "01:55:59,000"
			},
			"offsets": {
				"from": 6953000,
				"to": 6959000
			},
			"text": " The trust third party makes sure that the exchange happens in a way where none of the two parties can cheat each other, essentially."
		},
		{
			"timestamps": {
				"from": "01:55:59,000",
				"to": "01:56:00,000"
			},
			"offsets": {
				"from": 6959000,
				"to": 6960000
			},
			"text": " And so this already exists today."
		},
		{
			"timestamps": {
				"from": "01:56:00,000",
				"to": "01:56:03,000"
			},
			"offsets": {
				"from": 6960000,
				"to": 6963000
			},
			"text": " A lot of Ethereum blocks are built in this way."
		},
		{
			"timestamps": {
				"from": "01:56:03,000",
				"to": "01:56:09,000"
			},
			"offsets": {
				"from": 6963000,
				"to": 6969000
			},
			"text": " So it's a reality that it's not something that, you know, the Ethereum community,"
		},
		{
			"timestamps": {
				"from": "01:56:09,000",
				"to": "01:56:16,000"
			},
			"offsets": {
				"from": 6969000,
				"to": 6976000
			},
			"text": " I'm kind of made well, it is something that the Ethereum community made happen, but it's in some sense inevitable."
		},
		{
			"timestamps": {
				"from": "01:56:16,000",
				"to": "01:56:22,000"
			},
			"offsets": {
				"from": 6976000,
				"to": 6982000
			},
			"text": " Like anyone could always build some infrastructure of this kind and people could use it if it's more profitable for them."
		},
		{
			"timestamps": {
				"from": "01:56:22,000",
				"to": "01:56:25,000"
			},
			"offsets": {
				"from": 6982000,
				"to": 6985000
			},
			"text": " So, yeah, so, you know, we already have this."
		},
		{
			"timestamps": {
				"from": "01:56:25,000",
				"to": "01:56:29,000"
			},
			"offsets": {
				"from": 6985000,
				"to": 6989000
			},
			"text": " Why do we care about potentially putting this separation protocol?"
		},
		{
			"timestamps": {
				"from": "01:56:29,000",
				"to": "01:56:32,000"
			},
			"offsets": {
				"from": 6989000,
				"to": 6992000
			},
			"text": " So, as I said, really is a trusted third parties."
		},
		{
			"timestamps": {
				"from": "01:56:32,000",
				"to": "01:56:36,000"
			},
			"offsets": {
				"from": 6992000,
				"to": 6996000
			},
			"text": " We don't usually like to have these sort of entities in the protocol."
		},
		{
			"timestamps": {
				"from": "01:56:36,000",
				"to": "01:56:39,000"
			},
			"offsets": {
				"from": 6996000,
				"to": 6999000
			},
			"text": " They're not critical in some sense."
		},
		{
			"timestamps": {
				"from": "01:56:39,000",
				"to": "01:56:46,000"
			},
			"offsets": {
				"from": 6999000,
				"to": 7006000
			},
			"text": " Well, if things are set up properly, which I mean, I think there's a lot of improvements to be done on the infrastructure that exists today."
		},
		{
			"timestamps": {
				"from": "01:56:46,000",
				"to": "01:56:49,000"
			},
			"offsets": {
				"from": 7006000,
				"to": 7009000
			},
			"text": " It's very, you know, young infrastructure."
		},
		{
			"timestamps": {
				"from": "01:56:49,000",
				"to": "01:56:59,000"
			},
			"offsets": {
				"from": 7009000,
				"to": 7019000
			},
			"text": " But either way, there's always going to be some kind of failure modes that we don't really like or some, some requirements that we don't really like from having these parties."
		},
		{
			"timestamps": {
				"from": "01:56:59,000",
				"to": "01:57:03,000"
			},
			"offsets": {
				"from": 7019000,
				"to": 7023000
			},
			"text": " So one is that you have to basically whitelist them because they're trusted."
		},
		{
			"timestamps": {
				"from": "01:57:03,000",
				"to": "01:57:08,000"
			},
			"offsets": {
				"from": 7023000,
				"to": 7028000
			},
			"text": " So everyone has to kind of go and configure some list of these entities that they're fine with essentially the distrust."
		},
		{
			"timestamps": {
				"from": "01:57:08,000",
				"to": "01:57:10,000"
			},
			"offsets": {
				"from": 7028000,
				"to": 7030000
			},
			"text": " And we don't care."
		},
		{
			"timestamps": {
				"from": "01:57:10,000",
				"to": "01:57:13,000"
			},
			"offsets": {
				"from": 7030000,
				"to": 7033000
			},
			"text": " Builders do that, but we don't really like validators to do that."
		},
		{
			"timestamps": {
				"from": "01:57:13,000",
				"to": "01:57:15,000"
			},
			"offsets": {
				"from": 7033000,
				"to": 7035000
			},
			"text": " Or, well, I don't know, that's debatable."
		},
		{
			"timestamps": {
				"from": "01:57:15,000",
				"to": "01:57:21,000"
			},
			"offsets": {
				"from": 7035000,
				"to": 7041000
			},
			"text": " But anyway, there's, I think there's a future for relays to still exist and just have a full fallback in protocol."
		},
		{
			"timestamps": {
				"from": "01:57:21,000",
				"to": "01:57:24,000"
			},
			"offsets": {
				"from": 7041000,
				"to": 7044000
			},
			"text": " That is not the default, but that's a conversation for the right time."
		},
		{
			"timestamps": {
				"from": "01:57:24,000",
				"to": "01:57:30,000"
			},
			"offsets": {
				"from": 7044000,
				"to": 7050000
			},
			"text": " But yeah, another thing is that today we don't really have a kind of live monitoring for relays."
		},
		{
			"timestamps": {
				"from": "01:57:30,000",
				"to": "01:57:43,000"
			},
			"offsets": {
				"from": 7050000,
				"to": 7063000
			},
			"text": " Like locally people don't have a chance to observe interactions that relays have had with other proposers and then disconnect for them if these interactions look suspicious essentially."
		},
		{
			"timestamps": {
				"from": "01:57:43,000",
				"to": "01:57:44,000"
			},
			"offsets": {
				"from": 7063000,
				"to": 7064000
			},
			"text": " So that's something that we can include."
		},
		{
			"timestamps": {
				"from": "01:57:44,000",
				"to": "01:57:54,000"
			},
			"offsets": {
				"from": 7064000,
				"to": 7074000
			},
			"text": " We can basically really improve the resilience of this whole system because we can make it so that people don't need to go on Twitter and find out, oh, this relay is malicious."
		},
		{
			"timestamps": {
				"from": "01:57:54,000",
				"to": "01:57:58,000"
			},
			"offsets": {
				"from": 7074000,
				"to": 7078000
			},
			"text": " I'm going to disconnect for them, but just maybe this can happen locally, essentially."
		},
		{
			"timestamps": {
				"from": "01:57:58,000",
				"to": "01:58:00,000"
			},
			"offsets": {
				"from": 7078000,
				"to": 7080000
			},
			"text": " So there's a lot of improvement there."
		},
		{
			"timestamps": {
				"from": "01:58:00,000",
				"to": "01:58:17,000"
			},
			"offsets": {
				"from": 7080000,
				"to": 7097000
			},
			"text": " But still, there's some kind of, I guess, really fundamental catastrophic scenario that seems unavoidable to me if we keep having, or rather, if we only rely on these entities for this outsourcing, if we have kind of no fallback."
		},
		{
			"timestamps": {
				"from": "01:58:17,000",
				"to": "01:58:22,000"
			},
			"offsets": {
				"from": 7097000,
				"to": 7102000
			},
			"text": " So, especially with Deng Xia, so today you can always have a fallback."
		},
		{
			"timestamps": {
				"from": "01:58:22,000",
				"to": "01:58:35,000"
			},
			"offsets": {
				"from": 7102000,
				"to": 7115000
			},
			"text": " Actually, it's not so fundamental to just the state of things today. You could always have this fallback, which is essentially the catastrophic scenario is like all relays that most people are connected to fail for some whatever reason they're malicious or they're attacked."
		},
		{
			"timestamps": {
				"from": "01:58:35,000",
				"to": "01:58:36,000"
			},
			"offsets": {
				"from": 7115000,
				"to": 7116000
			},
			"text": " Anything could happen."
		},
		{
			"timestamps": {
				"from": "01:58:36,000",
				"to": "01:58:40,000"
			},
			"offsets": {
				"from": 7116000,
				"to": 7120000
			},
			"text": " They fail and now all of a sudden today it's fine."
		},
		{
			"timestamps": {
				"from": "01:58:40,000",
				"to": "01:58:48,000"
			},
			"offsets": {
				"from": 7120000,
				"to": 7128000
			},
			"text": " You could, you know, once you manage to disconnect because you realize, okay, these people haven't given me blocks for, you know, however many times I've tried, or if you have this monitoring system."
		},
		{
			"timestamps": {
				"from": "01:58:48,000",
				"to": "01:58:54,000"
			},
			"offsets": {
				"from": 7128000,
				"to": 7134000
			},
			"text": " Then it's fine, you just fall back to you building your own, you know, gat or whatever, like other execution clients are running, building your own blocks."
		},
		{
			"timestamps": {
				"from": "01:58:54,000",
				"to": "01:58:59,000"
			},
			"offsets": {
				"from": 7134000,
				"to": 7139000
			},
			"text": " So now what likeness is not really threatened. Maybe it's like a temporary thing."
		},
		{
			"timestamps": {
				"from": "01:58:59,000",
				"to": "01:59:07,000"
			},
			"offsets": {
				"from": 7139000,
				"to": 7147000
			},
			"text": " But with Deng Xiaodin and also statelessness in some sense, if, you know, let's say all the validators are stateless, they cannot build their own blocks."
		},
		{
			"timestamps": {
				"from": "01:59:07,000",
				"to": "01:59:13,000"
			},
			"offsets": {
				"from": 7147000,
				"to": 7153000
			},
			"text": " Or with Deng Xiaodin, like they cannot distribute data, then this becomes like a threat to no lightness."
		},
		{
			"timestamps": {
				"from": "01:59:13,000",
				"to": "01:59:21,000"
			},
			"offsets": {
				"from": 7153000,
				"to": 7161000
			},
			"text": " With Deng Xiaodin, not exactly, it's like you could make blocks, you just cannot put a lot of data into them, but you could argue, well, is that really lightness?"
		},
		{
			"timestamps": {
				"from": "01:59:21,000",
				"to": "01:59:27,000"
			},
			"offsets": {
				"from": 7161000,
				"to": 7167000
			},
			"text": " Like if all the rollups can stop because they don't have access to data anymore, that is not really what we want."
		},
		{
			"timestamps": {
				"from": "01:59:27,000",
				"to": "01:59:36,000"
			},
			"offsets": {
				"from": 7167000,
				"to": 7176000
			},
			"text": " Yes, so this is maybe what it will, well, this is like the one of the current ideas of what it could look like to put it in protocol."
		},
		{
			"timestamps": {
				"from": "01:59:36,000",
				"to": "01:59:42,000"
			},
			"offsets": {
				"from": 7176000,
				"to": 7182000
			},
			"text": " I think, well, I think probably can't really go into it."
		},
		{
			"timestamps": {
				"from": "01:59:42,000",
				"to": "01:59:49,000"
			},
			"offsets": {
				"from": 7182000,
				"to": 7189000
			},
			"text": " I don't think we have time to go into it in much detail, but basically it looks like, you know, as I said before, what are the relays?"
		},
		{
			"timestamps": {
				"from": "01:59:49,000",
				"to": "01:59:53,000"
			},
			"offsets": {
				"from": 7189000,
				"to": 7193000
			},
			"text": " They're just these kind of actors that negotiate the exchange."
		},
		{
			"timestamps": {
				"from": "01:59:53,000",
				"to": "01:59:56,000"
			},
			"offsets": {
				"from": 7193000,
				"to": 7196000
			},
			"text": " You know, what do we do if we want to remove these actors?"
		},
		{
			"timestamps": {
				"from": "01:59:56,000",
				"to": "01:59:58,000"
			},
			"offsets": {
				"from": 7196000,
				"to": 7198000
			},
			"text": " We basically have the protocol, negotiate exchange."
		},
		{
			"timestamps": {
				"from": "01:59:58,000",
				"to": "02:00:01,000"
			},
			"offsets": {
				"from": 7198000,
				"to": 7201000
			},
			"text": " And the protocol in this case is basically other validators."
		},
		{
			"timestamps": {
				"from": "02:00:01,000",
				"to": "02:00:22,000"
			},
			"offsets": {
				"from": 7201000,
				"to": 7222000
			},
			"text": " So there's a proposal, there's a builder, and we have the whole rest of the validator set or some committee, more likely, that basically kind of makes sure with their, well, they observed the exchange and with their attestations, they sort of make sure that if the proposal tries to shoot the builder, they fail and vice versa, essentially."
		},
		{
			"timestamps": {
				"from": "02:00:22,000",
				"to": "02:00:39,000"
			},
			"offsets": {
				"from": 7222000,
				"to": 7239000
			},
			"text": " So it essentially gives us the property, for example, that if the proposal accepts some block and some, some, like a bid, you could say from a builder, and we have good latency, like, things are fine from a fortress perspective from a network perspective, then the"
		},
		{
			"timestamps": {
				"from": "02:00:39,000",
				"to": "02:00:47,000"
			},
			"offsets": {
				"from": 7239000,
				"to": 7247000
			},
			"text": " proposal will get paid. It doesn't matter what the builder does. If they reveal their block, good, kind of this is a good case. If they don't reveal their block, they're really late."
		},
		{
			"timestamps": {
				"from": "02:00:47,000",
				"to": "02:00:54,000"
			},
			"offsets": {
				"from": 7247000,
				"to": 7254000
			},
			"text": " You know, tough luck for them. They're going to pay the validator and not even get their building opportunity. And so this is one design."
		},
		{
			"timestamps": {
				"from": "02:00:54,000",
				"to": "02:01:04,000"
			},
			"offsets": {
				"from": 7254000,
				"to": 7264000
			},
			"text": " There's a design which is kind of interesting. Oh, yeah. Also, thanks to Vitalik for all of the things that just took him from many of his research posts."
		},
		{
			"timestamps": {
				"from": "02:01:04,000",
				"to": "02:01:18,000"
			},
			"offsets": {
				"from": 7264000,
				"to": 7278000
			},
			"text": " But yeah, like, so this is basically, you could say in protocol map boost because it's really like designed to look like map boost. Again, we have basically this like party in the middle, this time more clearly than before, which is in this case, a committee."
		},
		{
			"timestamps": {
				"from": "02:01:18,000",
				"to": "02:01:32,000"
			},
			"offsets": {
				"from": 7278000,
				"to": 7292000
			},
			"text": " It also was before, but anyway, and this kind of party again, like negotiate, negotiate the exchange. We could think of the party as basically an availability or a call. So it's basically its job to ensure it's to give guarantees to the"
		},
		{
			"timestamps": {
				"from": "02:01:32,000",
				"to": "02:01:44,000"
			},
			"offsets": {
				"from": 7292000,
				"to": 7304000
			},
			"text": " composer that what the builder sent is available. So the composer will accept a header like a basically offer of, you know, I want to give you this block pay you this much."
		},
		{
			"timestamps": {
				"from": "02:01:44,000",
				"to": "02:01:52,000"
			},
			"offsets": {
				"from": 7304000,
				"to": 7312000
			},
			"text": " And the builder will essentially erase your code. So, you know, hopefully if you follow the discussion, you know, what is your coding is by now."
		},
		{
			"timestamps": {
				"from": "02:01:52,000",
				"to": "02:02:09,000"
			},
			"offsets": {
				"from": 7312000,
				"to": 7329000
			},
			"text": " And essentially the execution payload to the committee, like essentially encrypt, well, there's your code, then encrypt, and then basically split the parts to the committee so that if some threshold of the committee is honest and online, they will be able to decrypt, even if not all of the committee is."
		},
		{
			"timestamps": {
				"from": "02:02:09,000",
				"to": "02:02:27,000"
			},
			"offsets": {
				"from": 7329000,
				"to": 7347000
			},
			"text": " And basically the committee signs, you know, essentially individual members of the committee will attest to the fact that they have their part so that if you see enough attestations and the committee is visually honest, then you know that as a composer that this thing will be able to be decrypted and the data will be there essentially."
		},
		{
			"timestamps": {
				"from": "02:02:27,000",
				"to": "02:02:38,000"
			},
			"offsets": {
				"from": 7347000,
				"to": 7358000
			},
			"text": " So, actually, yeah, this kind of fits in quite nicely with these data availability discussions like that is really the problem here that the proposers accepting a bit, but the builder doesn't want to say what the bit is because that's their kind of private."
		},
		{
			"timestamps": {
				"from": "02:02:38,000",
				"to": "02:02:50,000"
			},
			"offsets": {
				"from": 7358000,
				"to": 7370000
			},
			"text": " Like, secret information. And we want basically some guarantee that even if you don't know what it is, it is going to be there once the time comes essentially like once you've accepted the bid and it's ready to go in the chain."
		},
		{
			"timestamps": {
				"from": "02:02:50,000",
				"to": "02:02:59,000"
			},
			"offsets": {
				"from": 7370000,
				"to": 7379000
			},
			"text": " So that's what it looks like. And just last quickly, I want to comment on basically."
		},
		{
			"timestamps": {
				"from": "02:02:59,000",
				"to": "02:03:14,000"
			},
			"offsets": {
				"from": 7379000,
				"to": 7394000
			},
			"text": " Well, so there's like sensory resistance questions about PBS. And I think they're not, you know, they're like fairly well understood. There's a, there's clearly like a way that PBS in order to call so it doesn't really pan."
		},
		{
			"timestamps": {
				"from": "02:03:14,000",
				"to": "02:03:16,000"
			},
			"offsets": {
				"from": 7394000,
				"to": 7396000
			},
			"text": " This is like, you know, our questions also today."
		},
		{
			"timestamps": {
				"from": "02:03:16,000",
				"to": "02:03:18,000"
			},
			"offsets": {
				"from": 7396000,
				"to": 7398000
			},
			"text": " It does the great sensory resistance."
		},
		{
			"timestamps": {
				"from": "02:03:18,000",
				"to": "02:03:27,000"
			},
			"offsets": {
				"from": 7398000,
				"to": 7407000
			},
			"text": " But we already know kind of how to deal with that. There's this concept inclusion list. There's like slight tweaks to that. There's, I mean, there's like basically really wide design space of."
		},
		{
			"timestamps": {
				"from": "02:03:27,000",
				"to": "02:03:40,000"
			},
			"offsets": {
				"from": 7407000,
				"to": 7420000
			},
			"text": " Very like roughly said ways for validators or proposals, but you could just say validators to basically make sure that transaction that should go in the chain eventually getting the chain, even if builders don't want that."
		},
		{
			"timestamps": {
				"from": "02:03:40,000",
				"to": "02:03:48,000"
			},
			"offsets": {
				"from": 7420000,
				"to": 7428000
			},
			"text": " And this also, by the way, like a really important reason why we want decentralization of the validator set because if you don't have that, then you just don't have this option."
		},
		{
			"timestamps": {
				"from": "02:03:48,000",
				"to": "02:03:59,000"
			},
			"offsets": {
				"from": 7428000,
				"to": 7439000
			},
			"text": " Like if you have 100 validators and they don't want something to do on the chain, that's it. There's no way. Well, I mean, there's ways like software can or, you know, other reasons, other ways, but there's no kind of automatic way to do that."
		},
		{
			"timestamps": {
				"from": "02:03:59,000",
				"to": "02:04:02,000"
			},
			"offsets": {
				"from": 7439000,
				"to": 7442000
			},
			"text": " Whereas with a decentralized by little set, we can always do that."
		},
		{
			"timestamps": {
				"from": "02:04:02,000",
				"to": "02:04:12,000"
			},
			"offsets": {
				"from": 7442000,
				"to": 7452000
			},
			"text": " And yeah, so inclusion list are quite simple in some sense. And there's like disagreements about how exactly they should work, but they're sort of simple today."
		},
		{
			"timestamps": {
				"from": "02:04:12,000",
				"to": "02:04:19,000"
			},
			"offsets": {
				"from": 7452000,
				"to": 7459000
			},
			"text": " So if we have like the property that it is easy for a validator to say this transaction is available and this transaction is valid."
		},
		{
			"timestamps": {
				"from": "02:04:19,000",
				"to": "02:04:25,000"
			},
			"offsets": {
				"from": 7459000,
				"to": 7465000
			},
			"text": " So the validity part becomes a bit harder with account instructions. So there's some questions there, but won't go into that."
		},
		{
			"timestamps": {
				"from": "02:04:25,000",
				"to": "02:04:33,000"
			},
			"offsets": {
				"from": 7465000,
				"to": 7473000
			},
			"text": " That's not really relevant here. The availability part that becomes a bit harder with done charting, because now all of a sudden, you know, there's all these like all these blobs floating around the network."
		},
		{
			"timestamps": {
				"from": "02:04:33,000",
				"to": "02:04:41,000"
			},
			"offsets": {
				"from": 7473000,
				"to": 7481000
			},
			"text": " There's all this data that you're not supposed to know. You're not supposed to essentially download all of your only supposed to sample what actually ends up."
		},
		{
			"timestamps": {
				"from": "02:04:41,000",
				"to": "02:04:52,000"
			},
			"offsets": {
				"from": 7481000,
				"to": 7492000
			},
			"text": " Okay. Yeah, well, I'll just finish this phrase and then I guess that's it. But yeah, basically with, yeah, so we done charting the term availability becomes a bit harder."
		},
		{
			"timestamps": {
				"from": "02:04:52,000",
				"to": "02:05:01,000"
			},
			"offsets": {
				"from": 7492000,
				"to": 7501000
			},
			"text": " So we would like to have some kind of shardin mem pool construction so that you can even for things that have not been included in a block yet."
		},
		{
			"timestamps": {
				"from": "02:05:01,000",
				"to": "02:05:07,000"
			},
			"offsets": {
				"from": 7501000,
				"to": 7507000
			},
			"text": " You can still in some way determine that they're available without everyone have to download everything essentially."
		},
		{
			"timestamps": {
				"from": "02:05:07,000",
				"to": "02:05:19,000"
			},
			"offsets": {
				"from": 7507000,
				"to": 7519000
			},
			"text": " And at this point, in this might not need to be the default route that all transaction goes through and probably won't for kind of the some other reasons that I've already hinted at before."
		},
		{
			"timestamps": {
				"from": "02:05:19,000",
				"to": "02:05:25,000"
			},
			"offsets": {
				"from": 7519000,
				"to": 7525000
			},
			"text": " Like it's some reason we'll expect that everything will go through a public mem pool, but this is kind of the fallback for a sense of resistance always."
		},
		{
			"timestamps": {
				"from": "02:05:25,000",
				"to": "02:05:29,000"
			},
			"offsets": {
				"from": 7525000,
				"to": 7529000
			},
			"text": " And so we want to basically have some kind of construction like this."
		},
		{
			"timestamps": {
				"from": "02:05:29,000",
				"to": "02:05:36,000"
			},
			"offsets": {
				"from": 7529000,
				"to": 7536000
			},
			"text": " And I think that's it throughout time, hopefully, maybe he's still have time for some questions for everyone."
		},
		{
			"timestamps": {
				"from": "02:05:36,000",
				"to": "02:05:39,000"
			},
			"offsets": {
				"from": 7536000,
				"to": 7539000
			},
			"text": " But otherwise that's it."
		},
		{
			"timestamps": {
				"from": "02:05:39,000",
				"to": "02:05:52,000"
			},
			"offsets": {
				"from": 7539000,
				"to": 7552000
			},
			"text": " Okay, so we are obviously out of time, but we can still use this room for another 20 minutes. We have a special guest here, Vitalik here to answer some questions around it."
		},
		{
			"timestamps": {
				"from": "02:05:52,000",
				"to": "02:06:02,000"
			},
			"offsets": {
				"from": 7552000,
				"to": 7562000
			},
			"text": " Okay, any questions?"
		},
		{
			"timestamps": {
				"from": "02:06:02,000",
				"to": "02:06:06,000"
			},
			"offsets": {
				"from": 7562000,
				"to": 7566000
			},
			"text": " Oh, okay, there's a question."
		},
		{
			"timestamps": {
				"from": "02:06:06,000",
				"to": "02:06:08,000"
			},
			"offsets": {
				"from": 7566000,
				"to": 7568000
			},
			"text": " Hello, thank you."
		},
		{
			"timestamps": {
				"from": "02:06:08,000",
				"to": "02:06:21,000"
			},
			"offsets": {
				"from": 7568000,
				"to": 7581000
			},
			"text": " How do you approach the topic of multi-relie in this time sharing it like ecosystem because there are penny solutions."
		},
		{
			"timestamps": {
				"from": "02:06:21,000",
				"to": "02:06:32,000"
			},
			"offsets": {
				"from": 7581000,
				"to": 7592000
			},
			"text": " Because I heard in PBS, but that it weakens the topic of censorship."
		},
		{
			"timestamps": {
				"from": "02:06:32,000",
				"to": "02:06:44,000"
			},
			"offsets": {
				"from": 7592000,
				"to": 7604000
			},
			"text": " But how do you approach them to improve the mem pool with multiple relays?"
		},
		{
			"timestamps": {
				"from": "02:06:44,000",
				"to": "02:06:54,000"
			},
			"offsets": {
				"from": 7604000,
				"to": 7614000
			},
			"text": " So relays are concepts that exist in like Mev boost kind of out of protocol, PBS, right? Like it's not a concept that exists in protocol, PBS."
		},
		{
			"timestamps": {
				"from": "02:06:54,000",
				"to": "02:07:04,000"
			},
			"offsets": {
				"from": 7614000,
				"to": 7624000
			},
			"text": " So the long, right, so the long term solution is I think they're not needs to rely on them."
		},
		{
			"timestamps": {
				"from": "02:07:04,000",
				"to": "02:07:12,000"
			},
			"offsets": {
				"from": 7624000,
				"to": 7632000
			},
			"text": " Hi, the erasure coding and Shamia's secret sharing scheme seem very related."
		},
		{
			"timestamps": {
				"from": "02:07:12,000",
				"to": "02:07:18,000"
			},
			"offsets": {
				"from": 7632000,
				"to": 7638000
			},
			"text": " Stay are there the exact same math. Okay."
		},
		{
			"timestamps": {
				"from": "02:07:18,000",
				"to": "02:07:25,000"
			},
			"offsets": {
				"from": 7638000,
				"to": 7645000
			},
			"text": " Is network persistent for the blob is going to be dependent on finality?"
		},
		{
			"timestamps": {
				"from": "02:07:25,000",
				"to": "02:07:32,000"
			},
			"offsets": {
				"from": 7645000,
				"to": 7652000
			},
			"text": " Because I would have expected this to be the case and therefore rule out completely this notions of having them for only five minutes."
		},
		{
			"timestamps": {
				"from": "02:07:32,000",
				"to": "02:07:37,000"
			},
			"offsets": {
				"from": 7652000,
				"to": 7657000
			},
			"text": " What do you mean by dependent on finality like that if we're not finalizing them, we need to keep the blobs for longer."
		},
		{
			"timestamps": {
				"from": "02:07:37,000",
				"to": "02:07:44,000"
			},
			"offsets": {
				"from": 7657000,
				"to": 7664000
			},
			"text": " No, well, oh, I see it. Well, so like if you're in the middle of an inactivity league, that probably makes sense."
		},
		{
			"timestamps": {
				"from": "02:07:44,000",
				"to": "02:07:52,000"
			},
			"offsets": {
				"from": 7664000,
				"to": 7672000
			},
			"text": " I mean, I think like I personally favor blobs being around for long enough like, you know, like at least a month or so."
		},
		{
			"timestamps": {
				"from": "02:07:52,000",
				"to": "02:07:57,000"
			},
			"offsets": {
				"from": 7672000,
				"to": 7677000
			},
			"text": " So that's, you know, any like it's longer than any realistic in activity league that would happen."
		},
		{
			"timestamps": {
				"from": "02:07:57,000",
				"to": "02:08:01,000"
			},
			"offsets": {
				"from": 7677000,
				"to": 7681000
			},
			"text": " But there's different approaches."
		},
		{
			"timestamps": {
				"from": "02:08:01,000",
				"to": "02:08:09,000"
			},
			"offsets": {
				"from": 7681000,
				"to": 7689000
			},
			"text": " Just on the setup, was there like a consideration or is it even possible for what's the problem with actually making that like a separate system?"
		},
		{
			"timestamps": {
				"from": "02:08:09,000",
				"to": "02:08:13,000"
			},
			"offsets": {
				"from": 7689000,
				"to": 7693000
			},
			"text": " It reminds me a bit too like swarm as it was integrated into Ethereum nodes."
		},
		{
			"timestamps": {
				"from": "02:08:13,000",
				"to": "02:08:21,000"
			},
			"offsets": {
				"from": 7693000,
				"to": 7701000
			},
			"text": " Like, wouldn't it be possible with precompiled and like the right new EVM opcodes actually make that an independent system or."
		},
		{
			"timestamps": {
				"from": "02:08:21,000",
				"to": "02:08:30,000"
			},
			"offsets": {
				"from": 7701000,
				"to": 7710000
			},
			"text": " So the problem, the reason why we need like data availability sampling in consensus and why it's like so different from, you know, IPFS and everything out there is because we want to act like actually."
		},
		{
			"timestamps": {
				"from": "02:08:30,000",
				"to": "02:08:37,000"
			},
			"offsets": {
				"from": 7710000,
				"to": 7717000
			},
			"text": " We have consensus on the fact that the data is available, right? Like the IPFS does not provide that, right?"
		},
		{
			"timestamps": {
				"from": "02:08:37,000",
				"to": "02:08:42,000"
			},
			"offsets": {
				"from": 7717000,
				"to": 7722000
			},
			"text": " And there's ways to like upload files so that some people think it's available and other people think it's not available."
		},
		{
			"timestamps": {
				"from": "02:08:42,000",
				"to": "02:08:48,000"
			},
			"offsets": {
				"from": 7722000,
				"to": 7728000
			},
			"text": " And for like regular file publishing, that's fine because if it's if a file is half available, you just publish it again."
		},
		{
			"timestamps": {
				"from": "02:08:48,000",
				"to": "02:08:59,000"
			},
			"offsets": {
				"from": 7728000,
				"to": 7739000
			},
			"text": " But for rollups, like you need like exact console, like global agreement on which data was published on time in which data was not published on time because the rollups."
		},
		{
			"timestamps": {
				"from": "02:08:59,000",
				"to": "02:09:08,000"
			},
			"offsets": {
				"from": 7739000,
				"to": 7748000
			},
			"text": " Like in order to figure out the current state of our rollup, you need to figure out like which data blobs to include and which ones to skip over."
		},
		{
			"timestamps": {
				"from": "02:09:08,000",
				"to": "02:09:12,000"
			},
			"offsets": {
				"from": 7748000,
				"to": 7752000
			},
			"text": " Yes."
		},
		{
			"timestamps": {
				"from": "02:09:12,000",
				"to": "02:09:16,000"
			},
			"offsets": {
				"from": 7752000,
				"to": 7756000
			},
			"text": " And tightly coupled with the chain."
		},
		{
			"timestamps": {
				"from": "02:09:16,000",
				"to": "02:09:18,000"
			},
			"offsets": {
				"from": 7756000,
				"to": 7758000
			},
			"text": " Okay, next session."
		},
		{
			"timestamps": {
				"from": "02:09:18,000",
				"to": "02:09:28,000"
			},
			"offsets": {
				"from": 7758000,
				"to": 7768000
			},
			"text": " Hi, for the sort of data blob storing period, are there any thoughts about like challenges for validators that kind of keep the data or is that purely altruistic behavior?"
		},
		{
			"timestamps": {
				"from": "02:09:28,000",
				"to": "02:09:41,000"
			},
			"offsets": {
				"from": 7768000,
				"to": 7781000
			},
			"text": " There I mean, there have been proof of custody designs that we've worked on over the years. I think it's kind of it's on the sort of rhetorical back burner because we just like know that these techniques exist and like we know that when the time comes, you know,"
		},
		{
			"timestamps": {
				"from": "02:09:41,000",
				"to": "02:09:47,000"
			},
			"offsets": {
				"from": 7781000,
				"to": 7787000
			},
			"text": " we can probably just stick them in to get extra security."
		},
		{
			"timestamps": {
				"from": "02:09:47,000",
				"to": "02:10:01,000"
			},
			"offsets": {
				"from": 7787000,
				"to": 7801000
			},
			"text": " Yeah, so I wanted to ask about the multi dimensional fee market quickly. I will talk about this excess data gas field. I was just wondering if the same would like in an ideal world, if we hadn't already done."
		},
		{
			"timestamps": {
				"from": "02:10:01,000",
				"to": "02:10:12,000"
			},
			"offsets": {
				"from": 7801000,
				"to": 7812000
			},
			"text": " Yeah, I P 1 5 5 9 with the same construction be wanted for the like original kind of gas. Yes. Yeah. And but this can happen simply just could this happen."
		},
		{
			"timestamps": {
				"from": "02:10:12,000",
				"to": "02:10:22,000"
			},
			"offsets": {
				"from": 7812000,
				"to": 7822000
			},
			"text": " If you own 5 5 9 could be upgraded to that over time. Okay, cool. I think they even already thoughts in that direction. So I think over the medium term we would want to."
		},
		{
			"timestamps": {
				"from": "02:10:22,000",
				"to": "02:10:35,000"
			},
			"offsets": {
				"from": 7822000,
				"to": 7835000
			},
			"text": " One of the nice side benefits it would get us that it also makes other improvements to 1559 like mechanism easier like, for example, like a time based instead of a block based kind of throughput targeting."
		},
		{
			"timestamps": {
				"from": "02:10:35,000",
				"to": "02:10:41,000"
			},
			"offsets": {
				"from": 7835000,
				"to": 7841000
			},
			"text": " So yeah, we would probably want to homogenize this over time."
		},
		{
			"timestamps": {
				"from": "02:10:41,000",
				"to": "02:10:57,000"
			},
			"offsets": {
				"from": 7841000,
				"to": 7857000
			},
			"text": " Yeah, so I could be wrong in one of these assumptions, but my understanding is that proposal to separation was motivated largely by the centralizing effects of me and us wanting to keep the proposal set centralized."
		},
		{
			"timestamps": {
				"from": "02:10:57,000",
				"to": "02:11:17,000"
			},
			"offsets": {
				"from": 7857000,
				"to": 7877000
			},
			"text": " But then kind of later with these designs like full sharding. We realized that we could utilize the builders as kind of with extra hardware requirements because they'd be incentivized with the M e V that they're extracting to have these nodes, but then there's also research into completely mitigating"
		},
		{
			"timestamps": {
				"from": "02:11:17,000",
				"to": "02:11:19,000"
			},
			"offsets": {
				"from": 7877000,
				"to": 7879000
			},
			"text": " the"
		},
		{
			"timestamps": {
				"from": "02:11:19,000",
				"to": "02:11:30,000"
			},
			"offsets": {
				"from": 7879000,
				"to": 7890000
			},
			"text": " Okay, so maybe that's wrong. Well, yeah, there's multiple strands of research and some of them are definitely sort of covering for each other in case the other fails, but and some of them are complimentary right so there's a"
		},
		{
			"timestamps": {
				"from": "02:11:30,000",
				"to": "02:11:45,000"
			},
			"offsets": {
				"from": 7890000,
				"to": 7905000
			},
			"text": " BBS which allows proposers and validators to be more decentralized, but at the cost of kind of shifting that centralization to builders. There's a separate strand of research on the topic of trying to make build like builders themselves decentralized internally."
		},
		{
			"timestamps": {
				"from": "02:11:45,000",
				"to": "02:11:56,000"
			},
			"offsets": {
				"from": 7905000,
				"to": 7916000
			},
			"text": " So like a some kind of protocol would plug into the market and make bids instead of being a single actor. And then there's also research on making applications that are M e V minimized. So like all three of those exist."
		},
		{
			"timestamps": {
				"from": "02:11:56,000",
				"to": "02:12:04,000"
			},
			"offsets": {
				"from": 7916000,
				"to": 7924000
			},
			"text": " Okay, I guess the question is just simply like if we mitigate or very minimize M e V then how do we incentivize it?"
		},
		{
			"timestamps": {
				"from": "02:12:04,000",
				"to": "02:12:11,000"
			},
			"offsets": {
				"from": 7924000,
				"to": 7931000
			},
			"text": " It's just like minimizing M e V doesn't mean M e V goes away. It just means we do as much as possible to reduce it, but there's no."
		},
		{
			"timestamps": {
				"from": "02:12:11,000",
				"to": "02:12:24,000"
			},
			"offsets": {
				"from": 7931000,
				"to": 7944000
			},
			"text": " Sure. Again, like I think anyone that has thought about me for some time will come to the conclusion that is just not possible to assume that there's not going to be an incentive to be a specialized actor like there's even if all transactions are encrypted."
		},
		{
			"timestamps": {
				"from": "02:12:24,000",
				"to": "02:12:29,000"
			},
			"offsets": {
				"from": 7944000,
				"to": 7949000
			},
			"text": " There's always going to be some reason to be the first to touch this state like."
		},
		{
			"timestamps": {
				"from": "02:12:29,000",
				"to": "02:12:37,000"
			},
			"offsets": {
				"from": 7949000,
				"to": 7957000
			},
			"text": " So the ideas are still being incentivized to run these notes. Yes, there's always going to be I think a lot of money to be made by controlling a block."
		},
		{
			"timestamps": {
				"from": "02:12:37,000",
				"to": "02:12:42,000"
			},
			"offsets": {
				"from": 7957000,
				"to": 7962000
			},
			"text": " I don't think that if Ethereum is a platform with value flowing essentially."
		},
		{
			"timestamps": {
				"from": "02:12:42,000",
				"to": "02:12:48,000"
			},
			"offsets": {
				"from": 7962000,
				"to": 7968000
			},
			"text": " Right. And just to do with mention, it's not just done shouting. We're basically a PBS like architecture would help."
		},
		{
			"timestamps": {
				"from": "02:12:48,000",
				"to": "02:13:05,000"
			},
			"offsets": {
				"from": 7968000,
				"to": 7985000
			},
			"text": " So once we move with worker trees to some to a world where it's easier to run stateless notes and then also with what PBS would get us would be that like normal validators would basically all of a sudden have like way way lower and storage requirements."
		},
		{
			"timestamps": {
				"from": "02:13:05,000",
				"to": "02:13:10,000"
			},
			"offsets": {
				"from": 7985000,
				"to": 7990000
			},
			"text": " And we only we don't don't get that if they still have to create blocks because then they need to stay."
		},
		{
			"timestamps": {
				"from": "02:13:10,000",
				"to": "02:13:19,000"
			},
			"offsets": {
				"from": 7990000,
				"to": 7999000
			},
			"text": " But if you only actually validate, but you don't create your own box, you get you leave that to a specialized entity, then you can as a validator turn stateless."
		},
		{
			"timestamps": {
				"from": "02:13:19,000",
				"to": "02:13:25,000"
			},
			"offsets": {
				"from": 7999000,
				"to": 8005000
			},
			"text": " And that's kind of the one more of these benefits we would get out of this."
		},
		{
			"timestamps": {
				"from": "02:13:25,000",
				"to": "02:13:39,000"
			},
			"offsets": {
				"from": 8005000,
				"to": 8019000
			},
			"text": " Would it make sense to like charge for some bring or something like that to like motivate people to have the data and be able to collect the charges."
		},
		{
			"timestamps": {
				"from": "02:13:39,000",
				"to": "02:13:45,000"
			},
			"offsets": {
				"from": 8019000,
				"to": 8025000
			},
			"text": " And I think that's definitely a possible construction. You could like see a way like where you."
		},
		{
			"timestamps": {
				"from": "02:13:45,000",
				"to": "02:13:49,000"
			},
			"offsets": {
				"from": 8025000,
				"to": 8029000
			},
			"text": " Oops, a specialized sample provider and you pay them."
		},
		{
			"timestamps": {
				"from": "02:13:49,000",
				"to": "02:14:07,000"
			},
			"offsets": {
				"from": 8029000,
				"to": 8047000
			},
			"text": " I think like the downside is that it makes it much harder to run a node because now you need to somehow set up this payment infrastructure. So I think it's not ideal. That's possible."
		},
		{
			"timestamps": {
				"from": "02:14:07,000",
				"to": "02:14:24,000"
			},
			"offsets": {
				"from": 8047000,
				"to": 8064000
			},
			"text": " Will there be ways to for for somebody who wants to make data available to provide a proof of through a smart contract as well, which is independent of this call data layer to send zone, which is smart contract specific, like as a generic infrastructure for proving that the data was available."
		},
		{
			"timestamps": {
				"from": "02:14:24,000",
				"to": "02:14:33,000"
			},
			"offsets": {
				"from": 8064000,
				"to": 8073000
			},
			"text": " I mean, that's that's what this construction does. Okay, like this is part of like you there will be a type of transaction that is called with the KZG commitment."
		},
		{
			"timestamps": {
				"from": "02:14:33,000",
				"to": "02:14:38,000"
			},
			"offsets": {
				"from": 8073000,
				"to": 8078000
			},
			"text": " And with the guarantee that this data is available and then also being and do that."
		},
		{
			"timestamps": {
				"from": "02:14:38,000",
				"to": "02:14:50,000"
			},
			"offsets": {
				"from": 8078000,
				"to": 8090000
			},
			"text": " Does there need to be like a special op-code in so that it to run the proof then to will to check that it was actually provided because the data was like if you get that commitment, it was provided."
		},
		{
			"timestamps": {
				"from": "02:14:50,000",
				"to": "02:14:54,000"
			},
			"offsets": {
				"from": 8090000,
				"to": 8094000
			},
			"text": " Yes, full stop. But if it has no extra check necessary. Yeah, that it was provided."
		},
		{
			"timestamps": {
				"from": "02:14:54,000",
				"to": "02:15:05,000"
			},
			"offsets": {
				"from": 8094000,
				"to": 8105000
			},
			"text": " But then if let's say somebody wants to have a check inside to Liddity, if the data was available, but it's a parameter. It's just like you get this commitment and you know it's there. It's like extra data."
		},
		{
			"timestamps": {
				"from": "02:15:05,000",
				"to": "02:15:11,000"
			},
			"offsets": {
				"from": 8105000,
				"to": 8111000
			},
			"text": " If you want to check that the data is available behind the commitments, you use a history proof of the document with any transaction."
		},
		{
			"timestamps": {
				"from": "02:15:11,000",
				"to": "02:15:12,000"
			},
			"offsets": {
				"from": 8111000,
				"to": 8112000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "02:15:12,000",
				"to": "02:15:14,000"
			},
			"offsets": {
				"from": 8112000,
				"to": 8114000
			},
			"text": " Okay."
		},
		{
			"timestamps": {
				"from": "02:15:14,000",
				"to": "02:15:16,000"
			},
			"offsets": {
				"from": 8114000,
				"to": 8116000
			},
			"text": " Any other questions?"
		},
		{
			"timestamps": {
				"from": "02:15:16,000",
				"to": "02:15:36,000"
			},
			"offsets": {
				"from": 8116000,
				"to": 8136000
			},
			"text": " So one way to understand the data, but I believe sampling network can be basically, basically interpret it as kind of like a dedicated IPFS, but with the samples that distribute in the network and a validator sample aids."
		},
		{
			"timestamps": {
				"from": "02:15:36,000",
				"to": "02:15:48,000"
			},
			"offsets": {
				"from": 8136000,
				"to": 8148000
			},
			"text": " But definitely IPFS has IPFS is a very bad way to think about it because what we're doing is not storage. It's proof that data was not withheld."
		},
		{
			"timestamps": {
				"from": "02:15:48,000",
				"to": "02:15:58,000"
			},
			"offsets": {
				"from": 8148000,
				"to": 8158000
			},
			"text": " Yes, but I just like kind of like for the network perspective, like."
		},
		{
			"timestamps": {
				"from": "02:15:58,000",
				"to": "02:16:02,000"
			},
			"offsets": {
				"from": 8158000,
				"to": 8162000
			},
			"text": " Yeah, and I know IPFS is one one blow to civil attack."
		},
		{
			"timestamps": {
				"from": "02:16:02,000",
				"to": "02:16:05,000"
			},
			"offsets": {
				"from": 8162000,
				"to": 8165000
			},
			"text": " There's something that is going to be a chess."
		},
		{
			"timestamps": {
				"from": "02:16:05,000",
				"to": "02:16:21,000"
			},
			"offsets": {
				"from": 8165000,
				"to": 8181000
			},
			"text": " I think, yeah, from the kind of network perspective of how the thing should be implemented, like this thing has much higher requirements in terms of visibility and fault tolerance and in terms of real time access and like real time, real time being able to change what you're accessing."
		},
		{
			"timestamps": {
				"from": "02:16:21,000",
				"to": "02:16:24,000"
			},
			"offsets": {
				"from": 8181000,
				"to": 8184000
			},
			"text": " Yeah, there's probably the biggest differences in requirements."
		},
		{
			"timestamps": {
				"from": "02:16:24,000",
				"to": "02:16:26,000"
			},
			"offsets": {
				"from": 8184000,
				"to": 8186000
			},
			"text": " Okay, great."
		},
		{
			"timestamps": {
				"from": "02:16:26,000",
				"to": "02:16:32,000"
			},
			"offsets": {
				"from": 8186000,
				"to": 8192000
			},
			"text": " There."
		},
		{
			"timestamps": {
				"from": "02:16:32,000",
				"to": "02:16:36,000"
			},
			"offsets": {
				"from": 8192000,
				"to": 8196000
			},
			"text": " Tell like if you don't mind. Have you enjoyed your stay in Colombia so far? Sorry."
		},
		{
			"timestamps": {
				"from": "02:16:36,000",
				"to": "02:16:41,000"
			},
			"offsets": {
				"from": 8196000,
				"to": 8201000
			},
			"text": " Yeah, no, no, I have. It's been very fun. Thank you."
		},
		{
			"timestamps": {
				"from": "02:16:41,000",
				"to": "02:16:49,000"
			},
			"offsets": {
				"from": 8201000,
				"to": 8209000
			},
			"text": " Thanks."
		},
		{
			"timestamps": {
				"from": "02:16:49,000",
				"to": "02:17:00,000"
			},
			"offsets": {
				"from": 8209000,
				"to": 8220000
			},
			"text": " Something else thinking about is that if so we assume that the movie exists and the people want to, you know, sandwich other people's transactions and stuff like this. So we, we have this proposal to separation, which makes a lot of sense."
		},
		{
			"timestamps": {
				"from": "02:17:00,000",
				"to": "02:17:18,000"
			},
			"offsets": {
				"from": 8220000,
				"to": 8238000
			},
			"text": " And then we once we have this, we kind of start to utilize it to do heavier work like thanks, and things like this. One of the things that be concerned about is that presently we have the ability for users to just simply not run a movie and they just let the transactions come in as they will and they, you know, lose a bit of money, but they're kind of genuine people."
		},
		{
			"timestamps": {
				"from": "02:17:18,000",
				"to": "02:17:30,000"
			},
			"offsets": {
				"from": 8238000,
				"to": 8250000
			},
			"text": " I just be interested to make sure that we don't rule out this person and we don't kind of glue together the role of like making really specialized fancy sandwiching blocks and then also doing all of the tank sharding stuff."
		},
		{
			"timestamps": {
				"from": "02:17:30,000",
				"to": "02:17:37,000"
			},
			"offsets": {
				"from": 8250000,
				"to": 8257000
			},
			"text": " I think it'd be nice if we can make sure that we keep a space for the home home user to continue to pack their own transactions and just be like, you know, an ask guy."
		},
		{
			"timestamps": {
				"from": "02:17:37,000",
				"to": "02:17:38,000"
			},
			"offsets": {
				"from": 8257000,
				"to": 8258000
			},
			"text": " Oh, go."
		},
		{
			"timestamps": {
				"from": "02:17:38,000",
				"to": "02:17:39,000"
			},
			"offsets": {
				"from": 8258000,
				"to": 8259000
			},
			"text": " Yeah."
		},
		{
			"timestamps": {
				"from": "02:17:39,000",
				"to": "02:17:47,000"
			},
			"offsets": {
				"from": 8259000,
				"to": 8267000
			},
			"text": " No, no, this is actually one of those things that I think I wrote a need to research post about last week right like basically, yeah."
		},
		{
			"timestamps": {
				"from": "02:17:47,000",
				"to": "02:17:58,000"
			},
			"offsets": {
				"from": 8267000,
				"to": 8278000
			},
			"text": " Like, can we push the autonomy and choosing a block content back to the proposal and like that's a spectrum that potentially could go all the way up to the proposal having an option to make everything."
		},
		{
			"timestamps": {
				"from": "02:17:58,000",
				"to": "02:18:10,000"
			},
			"offsets": {
				"from": 8278000,
				"to": 8290000
			},
			"text": " One of the conclusions there was that if we want to have that kind of proposal autonomy property, but also have the, the property of like potential low proposal requirements."
		},
		{
			"timestamps": {
				"from": "02:18:10,000",
				"to": "02:18:27,000"
			},
			"offsets": {
				"from": 8290000,
				"to": 8307000
			},
			"text": " We might need to have a third category of actor that does are kind of not an item of the extraction that in basically the entire bundle of computationally expensive stuff so like a witness edition state root calculation in the future as he case snarking"
		},
		{
			"timestamps": {
				"from": "02:18:27,000",
				"to": "02:18:33,000"
			},
			"offsets": {
				"from": 8307000,
				"to": 8313000
			},
			"text": " and figuring out polynomial commitments and proofs and broadcasting and so forth."
		},
		{
			"timestamps": {
				"from": "02:18:33,000",
				"to": "02:18:47,000"
			},
			"offsets": {
				"from": 8313000,
				"to": 8327000
			},
			"text": " I mean, I just want to comment. I think like people need to stop thinking of me only as a bad thing because you think of sandwiching and even without any front running any sandwiching we still have lots of me and it's actually a necessary part of the system."
		},
		{
			"timestamps": {
				"from": "02:18:47,000",
				"to": "02:18:50,000"
			},
			"offsets": {
				"from": 8327000,
				"to": 8330000
			},
			"text": " Like someone needs to do the arbitrage on the exchanges."
		},
		{
			"timestamps": {
				"from": "02:18:50,000",
				"to": "02:18:52,000"
			},
			"offsets": {
				"from": 8330000,
				"to": 8332000
			},
			"text": " Someone needs to do the liquidation."
		},
		{
			"timestamps": {
				"from": "02:18:52,000",
				"to": "02:18:54,000"
			},
			"offsets": {
				"from": 8332000,
				"to": 8334000
			},
			"text": " Someone needs to submit the fraud proofs."
		},
		{
			"timestamps": {
				"from": "02:18:54,000",
				"to": "02:18:56,000"
			},
			"offsets": {
				"from": 8334000,
				"to": 8336000
			},
			"text": " All of these are maybe."
		},
		{
			"timestamps": {
				"from": "02:18:56,000",
				"to": "02:19:02,000"
			},
			"offsets": {
				"from": 8336000,
				"to": 8342000
			},
			"text": " So don't think like this is a real incentivizing a bad thing here. So it's a part of the systems we're building."
		},
		{
			"timestamps": {
				"from": "02:19:02,000",
				"to": "02:19:18,000"
			},
			"offsets": {
				"from": 8342000,
				"to": 8358000
			},
			"text": " I mean, you so I would say for example, you can eliminate the bad me be using transaction encryption, for example, but you'll still have loads of me be left."
		},
		{
			"timestamps": {
				"from": "02:19:18,000",
				"to": "02:19:26,000"
			},
			"offsets": {
				"from": 8358000,
				"to": 8366000
			},
			"text": " Like, yeah."
		},
		{
			"timestamps": {
				"from": "02:19:26,000",
				"to": "02:19:38,000"
			},
			"offsets": {
				"from": 8366000,
				"to": 8378000
			},
			"text": " Sorry, I forgot what I was going to say. I think well, but basically, like we're relying on ethical builders. I think there's like all kinds of techniques that were layering on top to learn or that builders have to do."
		},
		{
			"timestamps": {
				"from": "02:19:38,000",
				"to": "02:19:41,000"
			},
			"offsets": {
				"from": 8378000,
				"to": 8381000
			},
			"text": " Yeah, I really nasty things."
		},
		{
			"timestamps": {
				"from": "02:19:41,000",
				"to": "02:19:45,000"
			},
			"offsets": {
				"from": 8381000,
				"to": 8385000
			},
			"text": " A question on the PPS. So here."
		},
		{
			"timestamps": {
				"from": "02:19:45,000",
				"to": "02:19:53,000"
			},
			"offsets": {
				"from": 8385000,
				"to": 8393000
			},
			"text": " So we like sort of apply some slashing mechanism once we have a separation between the builders and the."
		},
		{
			"timestamps": {
				"from": "02:19:53,000",
				"to": "02:19:58,000"
			},
			"offsets": {
				"from": 8393000,
				"to": 8398000
			},
			"text": " Proposers, I sort of different actions. The probably have."
		},
		{
			"timestamps": {
				"from": "02:19:58,000",
				"to": "02:20:07,000"
			},
			"offsets": {
				"from": 8398000,
				"to": 8407000
			},
			"text": " So there are slashing mechanism like slashing mechanisms at play, right? So like there's a slashing mechanism that slashes the proposal."
		},
		{
			"timestamps": {
				"from": "02:20:07,000",
				"to": "02:20:18,000"
			},
			"offsets": {
				"from": 8407000,
				"to": 8418000
			},
			"text": " So if they make two conflicting blocks in some of these partial block auction protocols, we use I can layer in that's like basically exposes the proposal to kind of extra slashing if they get."
		},
		{
			"timestamps": {
				"from": "02:20:18,000",
				"to": "02:20:25,000"
			},
			"offsets": {
				"from": 8418000,
				"to": 8425000
			},
			"text": " So if they violate the rules of the partial block auction protocol, builders can get."
		},
		{
			"timestamps": {
				"from": "02:20:25,000",
				"to": "02:20:36,000"
			},
			"offsets": {
				"from": 8425000,
				"to": 8436000
			},
			"text": " So there's definitely a lot of people who have been slashed in some like some contacts for get exactly which ones, but there's definitely a few cases."
		},
		{
			"timestamps": {
				"from": "02:20:36,000",
				"to": "02:20:44,000"
			},
			"offsets": {
				"from": 8436000,
				"to": 8444000
			},
			"text": " So, yeah, there's definitely like different forms of slashing to make sure the different participants follow the rules of the protocol."
		},
		{
			"timestamps": {
				"from": "02:20:44,000",
				"to": "02:20:49,000"
			},
			"offsets": {
				"from": 8444000,
				"to": 8449000
			},
			"text": " Thank you."
		},
		{
			"timestamps": {
				"from": "02:20:49,000",
				"to": "02:20:56,000"
			},
			"offsets": {
				"from": 8449000,
				"to": 8456000
			},
			"text": " Thank you."
		}
	]
}
